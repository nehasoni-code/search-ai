

==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\agentic_retrieval.md ====

# RAG chat: Using agentic retrieval

This repository includes an optional feature that uses [agentic retrieval from Azure AI Search](https://learn.microsoft.com/azure/search/search-agentic-retrieval-concept) to find the most relevant content given a user's conversation history. The agentic retrieval feature uses a LLM to analyze the conversation and generate multiple search queries to find relevant content. This can improve the quality of the responses, especially for complex or multi-faceted questions.

## Deployment

1. **Enable agentic retrieval:**

   Set the azd environment variable to enable the agentic retrieval feature:

   ```shell
   azd env set USE_AGENTIC_RETRIEVAL true
   ```

2. **(Optional) Customize the agentic retrieval model**

   You can configure which model agentic retrieval uses. By default, gpt-4.1-mini is used.

   To change the model, set the following environment variables appropriately:

   ```shell
   azd env set AZURE_OPENAI_SEARCHAGENT_DEPLOYMENT searchagent
   azd env set AZURE_OPENAI_SEARCHAGENT_MODEL gpt-4.1-mini
   azd env set AZURE_OPENAI_SEARCHAGENT_MODEL_VERSION 2025-04-14
   ```

   You can only change it to one of the [supported models](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-create#supported-models).

3. **Update the infrastructure and application:**

   Execute `azd up` to provision the infrastructure changes (only the new model, if you ran `up` previously) and deploy the application code with the updated environment variables. The post-provision script will configure Azure AI Search with a Knowledge agent pointing at the search index.

4. **Try out the feature:**

   Open the web app and start a new chat. Agentic retrieval will be used to find all sources.

5. **Review the query plan**

   Agentic retrieval uses additional billed tokens behind the scenes for the planning process.
   To see the token usage, select the lightbulb icon on a chat answer. This will open the "Thought process" tab, which shows the amount of tokens used by and the queries produced by the planning process

   ![Thought process token usage](./images/query-plan.png)


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\appservice.md ====

# RAG chat: Debugging the app on App Service

When you run `azd up` or `azd deploy`, it deploys your application to App Service,
and displays the deployed endpoint in the console.

If you encounter an error with that deployed app, you can debug the deployment using the tips below.

- [Debugging failed Azure App Service deployments](#debugging-failed-azure-app-service-deployments)
- [Checking the deployment logs for errors](#checking-the-deployment-logs-for-errors)
- [Checking the app logs for errors](#checking-the-app-logs-for-errors)
- [Checking Azure Monitor for errors](#checking-azure-monitor-for-errors)
- [Configuring log levels](#configuring-log-levels)

## Debugging failed Azure App Service deployments

If you see a 500 error upon visiting your app after deployment, something went wrong
during either the deployment or the server start script.

We recommend always waiting 10 minutes, to give the server time to properly startup.

If you still see a 500 error after 10 minutes:

1. [Check the deployment logs](#checking-the-deployment-logs-for-errors)
2. [Look for errors in the app logs](#checking-the-app-logs-for-errors)
3. [Look for errors in Azure Monitor](#checking-azure-monitor-for-errors)

## Checking the deployment logs for errors

In the Azure portal, navigate to your App Service.

Select _Deployment Center_ from the side navigation menu, then select _Logs_.
You should see a timestamped list of recent deploys:

![Screenshot of deployment logs tab from Azure App Service](images/screenshot_appservice_deploymentcenter.png)

Check whether the status of the most recent deploy is "Success (Active)" or "Failed". If it's success, the deployment logs might still reveal issues, and if it's failed, the logs should certainly reveal the issue.

Click the commit ID to open the logs for the most recent deploy. First scroll down to see if any errors or warnings are reported at the end. This is what you'll hopefully see if all went well:

![Screenshot of successful deployment logs from Azure App Service](images/screenshot_appservice_deployment_log.png)

Now scroll back up to find the timestamp with the label "Running oryx build".
[Oryx](https://github.com/microsoft/Oryx) is the open source tool that builds apps for App Service, Functions, and other platforms, across all the supported MS languages. Click the _Show logs_ link next to that label. That will pop open detailed logs at the bottom. Scroll down.

<details>
<summary>Expand to see the logs for a successful Oryx build for the application.</summary>

```plaintext
Command: oryx build /tmp/zipdeploy/extracted -o /home/site/wwwroot --platform python --platform-version 3.11 -p virtualenv_name=antenv --log-file /tmp/build-debug.log  -i /tmp/8dc28dad0e10acb --compress-destination-dir | tee /tmp/oryx-build.log
Operation performed by Microsoft Oryx, https://github.com/Microsoft/Oryx
You can report issues at https://github.com/Microsoft/Oryx/issues

Oryx Version: 0.2.20230508.1, Commit: 7fe2bf39b357dd68572b438a85ca50b5ecfb4592, ReleaseTagName: 20230508.1

Build Operation ID: 7440a33100749a32
Repository Commit : b09bff8b-da36-4d70-9e2f-c7b9131d85bc
OS Type           : bullseye
Image Type        : githubactions

Detecting platforms...
Detected following platforms:
  python: 3.11.7
Version '3.11.7' of platform 'python' is not installed. Generating script to install it...

Using intermediate directory '/tmp/8dc28dad0e10acb'.

Copying files to the intermediate directory...
Done in 27 sec(s).

Source directory     : /tmp/8dc28dad0e10acb
Destination directory: /home/site/wwwroot


Downloading and extracting 'python' version '3.11.7' to '/tmp/oryx/platforms/python/3.11.7'...
Detected image debian flavor: bullseye.
Downloaded in 5 sec(s).
Verifying checksum...
Extracting contents...
performing sha512 checksum for: python...
Done in 48 sec(s).

image detector file exists, platform is python..
OS detector file exists, OS is bullseye..
Python Version: /tmp/oryx/platforms/python/3.11.7/bin/python3.11
Creating directory for command manifest file if it does not exist
Removing existing manifest file
Python Virtual Environment: antenv
Creating virtual environment...
Activating virtual environment...
Running pip install...
[19:21:31+0000] Collecting aiofiles==23.2.1 (from -r requirements.txt (line 7))
[19:21:31+0000]   Obtaining dependency information for aiofiles==23.2.1 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata
[19:21:31+0000]   Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)
[19:21:35+0000] Collecting aiohttp==3.9.3 (from -r requirements.txt (line 9))
[19:21:35+0000]   Obtaining dependency information for aiohttp==3.9.3 from https://files.pythonhosted.org/packages/84/bb/74c9f32e1a76fab04b54ed6cd4b0dc4a07bd9dc6f3bb37f630149a9c3068/aiohttp-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:21:35+0000]   Downloading aiohttp-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)
[19:21:35+0000] Collecting aiosignal==1.3.1 (from -r requirements.txt (line 11))
[19:21:35+0000]   Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
[19:21:36+0000] Collecting annotated-types==0.6.0 (from -r requirements.txt (line 13))
[19:21:36+0000]   Obtaining dependency information for annotated-types==0.6.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata
[19:21:36+0000]   Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)
[19:21:36+0000] Collecting anyio==4.2.0 (from -r requirements.txt (line 15))
[19:21:36+0000]   Obtaining dependency information for anyio==4.2.0 from https://files.pythonhosted.org/packages/bf/cd/d6d9bb1dadf73e7af02d18225cbd2c93f8552e13130484f1c8dcfece292b/anyio-4.2.0-py3-none-any.whl.metadata
[19:21:36+0000]   Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)
[19:21:36+0000] Collecting asgiref==3.7.2 (from -r requirements.txt (line 19))
[19:21:36+0000]   Obtaining dependency information for asgiref==3.7.2 from https://files.pythonhosted.org/packages/9b/80/b9051a4a07ad231558fcd8ffc89232711b4e618c15cb7a392a17384bbeef/asgiref-3.7.2-py3-none-any.whl.metadata
[19:21:36+0000]   Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)
[19:21:36+0000] Collecting attrs==23.2.0 (from -r requirements.txt (line 21))
[19:21:36+0000]   Obtaining dependency information for attrs==23.2.0 from https://files.pythonhosted.org/packages/e0/44/827b2a91a5816512fcaf3cc4ebc465ccd5d598c45cefa6703fcf4a79018f/attrs-23.2.0-py3-none-any.whl.metadata
[19:21:36+0000]   Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)
[19:21:36+0000] Collecting azure-common==1.1.28 (from -r requirements.txt (line 23))
[19:21:36+0000]   Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)
[19:21:36+0000] Collecting azure-core==1.29.7 (from -r requirements.txt (line 27))
[19:21:36+0000]   Obtaining dependency information for azure-core==1.29.7 from https://files.pythonhosted.org/packages/ff/29/dbc7182bc207530c7b5858d59f429158465f878845d64a038afc1aa61e35/azure_core-1.29.7-py3-none-any.whl.metadata
[19:21:36+0000]   Downloading azure_core-1.29.7-py3-none-any.whl.metadata (36 kB)
[19:21:36+0000] Collecting azure-core-tracing-opentelemetry==1.0.0b11 (from -r requirements.txt (line 37))
[19:21:36+0000]   Obtaining dependency information for azure-core-tracing-opentelemetry==1.0.0b11 from https://files.pythonhosted.org/packages/e6/6e/3ef6dfba8e0faa4692caa6d103c721ccba6ac37a24744848a3a10bb3fe89/azure_core_tracing_opentelemetry-1.0.0b11-py3-none-any.whl.metadata
[19:21:36+0000]   Downloading azure_core_tracing_opentelemetry-1.0.0b11-py3-none-any.whl.metadata (8.5 kB)
[19:21:37+0000] Collecting azure-identity==1.15.0 (from -r requirements.txt (line 39))
[19:21:37+0000]   Obtaining dependency information for azure-identity==1.15.0 from https://files.pythonhosted.org/packages/30/10/5dbf755b368d10a28d55b06ac1f12512a13e88874a23db82defdea9a8cd9/azure_identity-1.15.0-py3-none-any.whl.metadata
[19:21:37+0000]   Downloading azure_identity-1.15.0-py3-none-any.whl.metadata (75 kB)
[19:21:37+0000]      ━━━━━━━━━━━━━━━━��━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 6.2 MB/s eta 0:00:00
[19:21:37+0000] Collecting azure-keyvault-secrets==4.7.0 (from -r requirements.txt (line 41))
[19:21:37+0000]   Downloading azure_keyvault_secrets-4.7.0-py3-none-any.whl (348 kB)
[19:21:37+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 348.6/348.6 kB 19.6 MB/s eta 0:00:00
[19:21:37+0000] Collecting azure-monitor-opentelemetry==1.2.0 (from -r requirements.txt (line 43))
[19:21:37+0000]   Obtaining dependency information for azure-monitor-opentelemetry==1.2.0 from https://files.pythonhosted.org/packages/66/72/5a6bac11b8f3bd60825f19c144c4c770c46951165f8ee5fc10ab3eaadf59/azure_monitor_opentelemetry-1.2.0-py3-none-any.whl.metadata
[19:21:37+0000]   Downloading azure_monitor_opentelemetry-1.2.0-py3-none-any.whl.metadata (19 kB)
[19:21:37+0000] Collecting azure-monitor-opentelemetry-exporter==1.0.0b21 (from -r requirements.txt (line 45))
[19:21:37+0000]   Obtaining dependency information for azure-monitor-opentelemetry-exporter==1.0.0b21 from https://files.pythonhosted.org/packages/4a/0d/18cb0da98b49c9a6724f6cae46a7e59b8325cda476bde13b64404a428ae8/azure_monitor_opentelemetry_exporter-1.0.0b21-py2.py3-none-any.whl.metadata
[19:21:37+0000]   Downloading azure_monitor_opentelemetry_exporter-1.0.0b21-py2.py3-none-any.whl.metadata (31 kB)
[19:21:37+0000] Collecting azure-search-documents==11.6.0b1 (from -r requirements.txt (line 47))
[19:21:37+0000]   Obtaining dependency information for azure-search-documents==11.6.0b1 from https://files.pythonhosted.org/packages/7c/f6/b138d9a252f80db69c052c65410bc972dca375e29c71c472e27d0bae327d/azure_search_documents-11.6.0b1-py3-none-any.whl.metadata
[19:21:37+0000]   Downloading azure_search_documents-11.6.0b1-py3-none-any.whl.metadata (23 kB)
[19:21:37+0000] Collecting azure-storage-blob==12.19.0 (from -r requirements.txt (line 49))
[19:21:37+0000]   Obtaining dependency information for azure-storage-blob==12.19.0 from https://files.pythonhosted.org/packages/f6/82/24b0d7cf67ea63af86f11092756b8fe2adc1d55323241dc4107f5f5748e2/azure_storage_blob-12.19.0-py3-none-any.whl.metadata
[19:21:37+0000]   Downloading azure_storage_blob-12.19.0-py3-none-any.whl.metadata (26 kB)
[19:21:37+0000] Collecting blinker==1.7.0 (from -r requirements.txt (line 51))
[19:21:37+0000]   Obtaining dependency information for blinker==1.7.0 from https://files.pythonhosted.org/packages/fa/2a/7f3714cbc6356a0efec525ce7a0613d581072ed6eb53eb7b9754f33db807/blinker-1.7.0-py3-none-any.whl.metadata
[19:21:37+0000]   Downloading blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)
[19:21:37+0000] Collecting certifi==2023.11.17 (from -r requirements.txt (line 55))
[19:21:37+0000]   Obtaining dependency information for certifi==2023.11.17 from https://files.pythonhosted.org/packages/64/62/428ef076be88fa93716b576e4a01f919d25968913e817077a386fcbe4f42/certifi-2023.11.17-py3-none-any.whl.metadata
[19:21:37+0000]   Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)
[19:21:39+0000] Collecting cffi==1.16.0 (from -r requirements.txt (line 61))
[19:21:39+0000]   Obtaining dependency information for cffi==1.16.0 from https://files.pythonhosted.org/packages/9b/89/a31c81e36bbb793581d8bba4406a8aac4ba84b2559301c44eef81f4cf5df/cffi-1.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:21:39+0000]   Downloading cffi-1.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)
[19:21:40+0000] Collecting charset-normalizer==3.3.2 (from -r requirements.txt (line 63))
[19:21:40+0000]   Obtaining dependency information for charset-normalizer==3.3.2 from https://files.pythonhosted.org/packages/40/26/f35951c45070edc957ba40a5b1db3cf60a9dbb1b350c2d5bef03e01e61de/charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:21:40+0000]   Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)
[19:21:40+0000] Collecting click==8.1.7 (from -r requirements.txt (line 65))
[19:21:40+0000]   Obtaining dependency information for click==8.1.7 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata
[19:21:40+0000]   Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)
[19:21:43+0000] Collecting cryptography==42.0.1 (from -r requirements.txt (line 70))
[19:21:43+0000]   Obtaining dependency information for cryptography==42.0.1 from https://files.pythonhosted.org/packages/f8/46/2776ca9b602f79633fdf69824b5e18c94f2e0c5f09a94fc69e5b0887c14d/cryptography-42.0.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata
[19:21:43+0000]   Downloading cryptography-42.0.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)
[19:21:43+0000] Collecting deprecated==1.2.14 (from -r requirements.txt (line 78))
[19:21:43+0000]   Obtaining dependency information for deprecated==1.2.14 from https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl.metadata
[19:21:43+0000]   Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)
[19:21:43+0000] Collecting distro==1.9.0 (from -r requirements.txt (line 80))
[19:21:43+0000]   Obtaining dependency information for distro==1.9.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata
[19:21:44+0000]   Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
[19:21:44+0000] Collecting ecdsa==0.18.0 (from -r requirements.txt (line 82))
[19:21:44+0000]   Downloading ecdsa-0.18.0-py2.py3-none-any.whl (142 kB)
[19:21:44+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.9/142.9 kB 3.7 MB/s eta 0:00:00
[19:21:44+0000] Collecting fixedint==0.1.6 (from -r requirements.txt (line 84))
[19:21:44+0000]   Downloading fixedint-0.1.6-py3-none-any.whl (12 kB)
[19:21:45+0000] Collecting flask==3.0.1 (from -r requirements.txt (line 86))
[19:21:45+0000]   Obtaining dependency information for flask==3.0.1 from https://files.pythonhosted.org/packages/bd/0e/63738e88e981ae57c23bad6c499898314a1110a4141f77d7bd929b552fb4/flask-3.0.1-py3-none-any.whl.metadata
[19:21:45+0000]   Downloading flask-3.0.1-py3-none-any.whl.metadata (3.6 kB)
[19:21:47+0000] Collecting frozenlist==1.4.1 (from -r requirements.txt (line 88))
[19:21:47+0000]   Obtaining dependency information for frozenlist==1.4.1 from https://files.pythonhosted.org/packages/b3/c9/0bc5ee7e1f5cc7358ab67da0b7dfe60fbd05c254cea5c6108e7d1ae28c63/frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:21:47+0000]   Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
[19:21:47+0000] Collecting h11==0.14.0 (from -r requirements.txt (line 92))
[19:21:47+0000]   Downloading h11-0.14.0-py3-none-any.whl (58 kB)
[19:21:47+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 2.5 MB/s eta 0:00:00
[19:21:48+0000] Collecting h2==4.1.0 (from -r requirements.txt (line 98))
[19:21:48+0000]   Downloading h2-4.1.0-py3-none-any.whl (57 kB)
[19:21:48+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 627.9 kB/s eta 0:00:00
[19:21:48+0000] Collecting hpack==4.0.0 (from -r requirements.txt (line 100))
[19:21:48+0000]   Downloading hpack-4.0.0-py3-none-any.whl (32 kB)
[19:21:49+0000] Collecting httpcore==1.0.2 (from -r requirements.txt (line 102))
[19:21:49+0000]   Obtaining dependency information for httpcore==1.0.2 from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata
[19:21:49+0000]   Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)
[19:21:50+0000] Collecting httpx==0.26.0 (from -r requirements.txt (line 104))
[19:21:50+0000]   Obtaining dependency information for httpx==0.26.0 from https://files.pythonhosted.org/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl.metadata
[19:21:50+0000]   Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)
[19:21:50+0000] Collecting hypercorn==0.16.0 (from -r requirements.txt (line 106))
[19:21:50+0000]   Obtaining dependency information for hypercorn==0.16.0 from https://files.pythonhosted.org/packages/17/9e/700d764316399c20fbe8e98c6fff903b5d3f950043cc2fcbd0831a42c953/hypercorn-0.16.0-py3-none-any.whl.metadata
[19:21:50+0000]   Downloading hypercorn-0.16.0-py3-none-any.whl.metadata (5.4 kB)
[19:21:50+0000] Collecting hyperframe==6.0.1 (from -r requirements.txt (line 108))
[19:21:50+0000]   Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)
[19:21:51+0000] Collecting idna==3.6 (from -r requirements.txt (line 110))
[19:21:51+0000]   Obtaining dependency information for idna==3.6 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata
[19:21:51+0000]   Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)
[19:21:51+0000] Collecting importlib-metadata==6.11.0 (from -r requirements.txt (line 116))
[19:21:51+0000]   Obtaining dependency information for importlib-metadata==6.11.0 from https://files.pythonhosted.org/packages/59/9b/ecce94952ab5ea74c31dcf9ccf78ccd484eebebef06019bf8cb579ab4519/importlib_metadata-6.11.0-py3-none-any.whl.metadata
[19:21:51+0000]   Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)
[19:21:52+0000] Collecting isodate==0.6.1 (from -r requirements.txt (line 118))
[19:21:52+0000]   Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)
[19:21:52+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 1.8 MB/s eta 0:00:00
[19:21:52+0000] Collecting itsdangerous==2.1.2 (from -r requirements.txt (line 124))
[19:21:52+0000]   Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)
[19:21:52+0000] Collecting jinja2==3.1.3 (from -r requirements.txt (line 128))
[19:21:52+0000]   Obtaining dependency information for jinja2==3.1.3 from https://files.pythonhosted.org/packages/30/6d/6de6be2d02603ab56e72997708809e8a5b0fbfee080735109b40a3564843/Jinja2-3.1.3-py3-none-any.whl.metadata
[19:21:52+0000]   Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)
[19:21:53+0000] Collecting markupsafe==2.1.4 (from -r requirements.txt (line 132))
[19:21:53+0000]   Obtaining dependency information for markupsafe==2.1.4 from https://files.pythonhosted.org/packages/d3/0a/c6dfffacc5a9a17c97019cb7cbec67e5abfb65c59a58ecba270fa224f88d/MarkupSafe-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:21:53+0000]   Downloading MarkupSafe-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
[19:21:53+0000] Collecting msal==1.26.0 (from -r requirements.txt (line 137))
[19:21:53+0000]   Obtaining dependency information for msal==1.26.0 from https://files.pythonhosted.org/packages/b7/61/2756b963e84db6946e4b93a8e288595106286fc11c7129fcb869267ead67/msal-1.26.0-py2.py3-none-any.whl.metadata
[19:21:53+0000]   Downloading msal-1.26.0-py2.py3-none-any.whl.metadata (11 kB)
[19:21:54+0000] Collecting msal-extensions==1.1.0 (from -r requirements.txt (line 142))
[19:21:54+0000]   Obtaining dependency information for msal-extensions==1.1.0 from https://files.pythonhosted.org/packages/78/8d/ecd0eb93196f25c722ba1b923fd54d190366feccfa5b159d48dacf2b1fee/msal_extensions-1.1.0-py3-none-any.whl.metadata
[19:21:54+0000]   Downloading msal_extensions-1.1.0-py3-none-any.whl.metadata (7.7 kB)
[19:21:54+0000] Collecting msrest==0.7.1 (from -r requirements.txt (line 144))
[19:21:54+0000]   Downloading msrest-0.7.1-py3-none-any.whl (85 kB)
[19:21:54+0000]      ━━━━━━━━━━━━━━━━━━━��━━━━━━━━━━━━━━━━━━━━ 85.4/85.4 kB 6.2 MB/s eta 0:00:00
[19:21:59+0000] Collecting multidict==6.0.4 (from -r requirements.txt (line 146))
[19:21:59+0000]   Downloading multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)
[19:21:59+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.4/117.4 kB 2.2 MB/s eta 0:00:00
[19:22:05+0000] Collecting numpy==1.26.3 (from -r requirements.txt (line 150))
[19:22:05+0000]   Obtaining dependency information for numpy==1.26.3 from https://files.pythonhosted.org/packages/5a/62/007b63f916aca1d27f5fede933fda3315d931ff9b2c28b9c2cf388cd8edb/numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:22:05+0000]   Downloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
[19:22:05+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━���━━━━━━━━━━━━━━ 61.2/61.2 kB 5.6 MB/s eta 0:00:00
[19:22:05+0000] Collecting oauthlib==3.2.2 (from -r requirements.txt (line 155))
[19:22:05+0000]   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)
[19:22:05+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 7.9 MB/s eta 0:00:00
[19:22:06+0000] Collecting openai[datalib]==1.10.0 (from -r requirements.txt (line 157))
[19:22:06+0000]   Obtaining dependency information for openai[datalib]==1.10.0 from https://files.pythonhosted.org/packages/46/85/8681046cd9cc13a36ac76e4a1b047338c90dbeab2e9b14fb36de7f314c93/openai-1.10.0-py3-none-any.whl.metadata
[19:22:06+0000]   Downloading openai-1.10.0-py3-none-any.whl.metadata (18 kB)
[19:22:06+0000] Collecting opentelemetry-api==1.22.0 (from -r requirements.txt (line 159))
[19:22:06+0000]   Obtaining dependency information for opentelemetry-api==1.22.0 from https://files.pythonhosted.org/packages/fc/2e/a8509051aa446783e24ee03d74bd268c07d5d25a8d48686cfcf3429d5d32/opentelemetry_api-1.22.0-py3-none-any.whl.metadata
[19:22:06+0000]   Downloading opentelemetry_api-1.22.0-py3-none-any.whl.metadata (1.4 kB)
[19:22:07+0000] Collecting opentelemetry-instrumentation==0.43b0 (from -r requirements.txt (line 177))
[19:22:07+0000]   Obtaining dependency information for opentelemetry-instrumentation==0.43b0 from https://files.pythonhosted.org/packages/91/f0/4a9f7cbcc697273d847040a9e4f98ceb07b642e1fe5fed56a0fb6b567665/opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata
[19:22:07+0000]   Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)
[19:22:08+0000] Collecting opentelemetry-instrumentation-aiohttp-client==0.43b0 (from -r requirements.txt (line 191))
[19:22:08+0000]   Obtaining dependency information for opentelemetry-instrumentation-aiohttp-client==0.43b0 from https://files.pythonhosted.org/packages/23/75/ce33cd15bc706b1e170e5ce65235a8418e3332ad543419b902a9d24f079f/opentelemetry_instrumentation_aiohttp_client-0.43b0-py3-none-any.whl.metadata
[19:22:08+0000]   Downloading opentelemetry_instrumentation_aiohttp_client-0.43b0-py3-none-any.whl.metadata (2.2 kB)
[19:22:08+0000] Collecting opentelemetry-instrumentation-asgi==0.43b0 (from -r requirements.txt (line 193))
[19:22:08+0000]   Obtaining dependency information for opentelemetry-instrumentation-asgi==0.43b0 from https://files.pythonhosted.org/packages/71/cd/a0456c8e4441d9ef5b412a3ffdf97629a81adeb331f8bb645df4f9153dd8/opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata
[19:22:08+0000]   Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)
[19:22:08+0000] Collecting opentelemetry-instrumentation-dbapi==0.43b0 (from -r requirements.txt (line 197))
[19:22:08+0000]   Obtaining dependency information for opentelemetry-instrumentation-dbapi==0.43b0 from https://files.pythonhosted.org/packages/6d/96/f46bffb40e71f0abd82ad24ecfa7a8e29b6abca631f7d049d80afee83ff9/opentelemetry_instrumentation_dbapi-0.43b0-py3-none-any.whl.metadata
[19:22:08+0000]   Downloading opentelemetry_instrumentation_dbapi-0.43b0-py3-none-any.whl.metadata (1.9 kB)
[19:22:09+0000] Collecting opentelemetry-instrumentation-django==0.43b0 (from -r requirements.txt (line 199))
[19:22:09+0000]   Obtaining dependency information for opentelemetry-instrumentation-django==0.43b0 from https://files.pythonhosted.org/packages/11/66/a6b5aadb04b5daf002fcbe97bb6bc83416c53b81a608de0e9ad886c59643/opentelemetry_instrumentation_django-0.43b0-py3-none-any.whl.metadata
[19:22:09+0000]   Downloading opentelemetry_instrumentation_django-0.43b0-py3-none-any.whl.metadata (2.3 kB)
[19:22:09+0000] Collecting opentelemetry-instrumentation-fastapi==0.43b0 (from -r requirements.txt (line 201))
[19:22:09+0000]   Obtaining dependency information for opentelemetry-instrumentation-fastapi==0.43b0 from https://files.pythonhosted.org/packages/1d/51/429d04b8694fec2f87184ced4beeab1dd6db194a9444b0a6fca1675338b2/opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata
[19:22:09+0000]   Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)
[19:22:10+0000] Collecting opentelemetry-instrumentation-flask==0.43b0 (from -r requirements.txt (line 203))
[19:22:10+0000]   Obtaining dependency information for opentelemetry-instrumentation-flask==0.43b0 from https://files.pythonhosted.org/packages/21/eb/4b0d6f98d2767c7117ebe497bcc58f00e70cc6b4ce97b99bd3eccf3d6644/opentelemetry_instrumentation_flask-0.43b0-py3-none-any.whl.metadata
[19:22:10+0000]   Downloading opentelemetry_instrumentation_flask-0.43b0-py3-none-any.whl.metadata (2.4 kB)
[19:22:10+0000] Collecting opentelemetry-instrumentation-httpx==0.43b0 (from -r requirements.txt (line 205))
[19:22:10+0000]   Obtaining dependency information for opentelemetry-instrumentation-httpx==0.43b0 from https://files.pythonhosted.org/packages/7e/ed/a8d3951650145d7d7997c83e35c59c02c8bf632c24ff2e07ab065ad7dd48/opentelemetry_instrumentation_httpx-0.43b0-py3-none-any.whl.metadata
[19:22:10+0000]   Downloading opentelemetry_instrumentation_httpx-0.43b0-py3-none-any.whl.metadata (7.1 kB)
[19:22:11+0000] Collecting opentelemetry-instrumentation-psycopg2==0.43b0 (from -r requirements.txt (line 207))
[19:22:11+0000]   Obtaining dependency information for opentelemetry-instrumentation-psycopg2==0.43b0 from https://files.pythonhosted.org/packages/0a/4e/f2085da8254b0f019a5dd57f737395c39274a23c25bf3dfe4030a4169325/opentelemetry_instrumentation_psycopg2-0.43b0-py3-none-any.whl.metadata
[19:22:11+0000]   Downloading opentelemetry_instrumentation_psycopg2-0.43b0-py3-none-any.whl.metadata (2.1 kB)
[19:22:11+0000] Collecting opentelemetry-instrumentation-requests==0.43b0 (from -r requirements.txt (line 209))
[19:22:11+0000]   Obtaining dependency information for opentelemetry-instrumentation-requests==0.43b0 from https://files.pythonhosted.org/packages/3b/a9/98618c6383cad51313f448412cadd0bed43634f0287eaf67a3e71a536f9c/opentelemetry_instrumentation_requests-0.43b0-py3-none-any.whl.metadata
[19:22:11+0000]   Downloading opentelemetry_instrumentation_requests-0.43b0-py3-none-any.whl.metadata (2.7 kB)
[19:22:12+0000] Collecting opentelemetry-instrumentation-urllib==0.43b0 (from -r requirements.txt (line 213))
[19:22:12+0000]   Obtaining dependency information for opentelemetry-instrumentation-urllib==0.43b0 from https://files.pythonhosted.org/packages/29/8a/c184945b2628ed44b9357e0df84dfc0974efd4e1360b3d89d2180ebfb3c0/opentelemetry_instrumentation_urllib-0.43b0-py3-none-any.whl.metadata
[19:22:12+0000]   Downloading opentelemetry_instrumentation_urllib-0.43b0-py3-none-any.whl.metadata (3.4 kB)
[19:22:12+0000] Collecting opentelemetry-instrumentation-urllib3==0.43b0 (from -r requirements.txt (line 215))
[19:22:12+0000]   Obtaining dependency information for opentelemetry-instrumentation-urllib3==0.43b0 from https://files.pythonhosted.org/packages/a0/54/3e6fc502e06d6c4cba23f314426951225f950b1af3c2e6decb780cd64ff1/opentelemetry_instrumentation_urllib3-0.43b0-py3-none-any.whl.metadata
[19:22:12+0000]   Downloading opentelemetry_instrumentation_urllib3-0.43b0-py3-none-any.whl.metadata (3.6 kB)
[19:22:13+0000] Collecting opentelemetry-instrumentation-wsgi==0.43b0 (from -r requirements.txt (line 217))
[19:22:13+0000]   Obtaining dependency information for opentelemetry-instrumentation-wsgi==0.43b0 from https://files.pythonhosted.org/packages/4a/37/6315abd394778d76b9bf206980436a8539cc13ddcd0bced709f4d9c3d1e8/opentelemetry_instrumentation_wsgi-0.43b0-py3-none-any.whl.metadata
[19:22:13+0000]   Downloading opentelemetry_instrumentation_wsgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)
[19:22:13+0000] Collecting opentelemetry-resource-detector-azure==0.1.3 (from -r requirements.txt (line 221))
[19:22:13+0000]   Obtaining dependency information for opentelemetry-resource-detector-azure==0.1.3 from https://files.pythonhosted.org/packages/99/c4/6790b15d360d0a14c5fb3a754d713470758da8a3635d90502aabb52febe2/opentelemetry_resource_detector_azure-0.1.3-py3-none-any.whl.metadata
[19:22:13+0000]   Downloading opentelemetry_resource_detector_azure-0.1.3-py3-none-any.whl.metadata (4.6 kB)
[19:22:14+0000] Collecting opentelemetry-sdk==1.22.0 (from -r requirements.txt (line 223))
[19:22:14+0000]   Obtaining dependency information for opentelemetry-sdk==1.22.0 from https://files.pythonhosted.org/packages/ff/94/588f49e0dd9a62ec46102736d2378330032a55e19c79ff7e4febea7ebed1/opentelemetry_sdk-1.22.0-py3-none-any.whl.metadata
[19:22:14+0000]   Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl.metadata (1.5 kB)
[19:22:14+0000] Collecting opentelemetry-semantic-conventions==0.43b0 (from -r requirements.txt (line 227))
[19:22:14+0000]   Obtaining dependency information for opentelemetry-semantic-conventions==0.43b0 from https://files.pythonhosted.org/packages/e0/26/69be0f1a56a362c68fa0c7632d841b1b8f29d809bc6b1b897387c9f46973/opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl.metadata
[19:22:14+0000]   Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl.metadata (2.3 kB)
[19:22:15+0000] Collecting opentelemetry-util-http==0.43b0 (from -r requirements.txt (line 241))
[19:22:15+0000]   Obtaining dependency information for opentelemetry-util-http==0.43b0 from https://files.pythonhosted.org/packages/74/91/a87a59baeeb917a93f2cc86fa670cf533328d18a2d09b0cef4f65e8b83e9/opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata
[19:22:15+0000]   Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)
[19:22:15+0000] Collecting packaging==23.2 (from -r requirements.txt (line 252))
[19:22:15+0000]   Obtaining dependency information for packaging==23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata
[19:22:15+0000]   Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
[19:22:20+0000] Collecting pandas==2.2.0 (from -r requirements.txt (line 256))
[19:22:20+0000]   Obtaining dependency information for pandas==2.2.0 from https://files.pythonhosted.org/packages/5b/7e/9fd11ba8e86a8add8f2ff4e11c7111f65ec6fd1b547222160bb969e2bf5e/pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:22:20+0000]   Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)
[19:22:21+0000] Collecting pandas-stubs==2.1.4.231227 (from -r requirements.txt (line 258))
[19:22:21+0000]   Obtaining dependency information for pandas-stubs==2.1.4.231227 from https://files.pythonhosted.org/packages/c0/6d/c5c23926fcc7526a5df32a8f3b3540948be8dd4c25f4a097f9091d40535c/pandas_stubs-2.1.4.231227-py3-none-any.whl.metadata
[19:22:21+0000]   Downloading pandas_stubs-2.1.4.231227-py3-none-any.whl.metadata (9.6 kB)
[19:22:26+0000] Collecting pillow==10.2.0 (from -r requirements.txt (line 260))
[19:22:26+0000]   Obtaining dependency information for pillow==10.2.0 from https://files.pythonhosted.org/packages/66/9c/2e1877630eb298bbfd23f90deeec0a3f682a4163d5ca9f178937de57346c/pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata
[19:22:26+0000]   Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)
[19:22:27+0000] Collecting portalocker==2.8.2 (from -r requirements.txt (line 262))
[19:22:27+0000]   Obtaining dependency information for portalocker==2.8.2 from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata
[19:22:27+0000]   Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)
[19:22:28+0000] Collecting priority==2.0.0 (from -r requirements.txt (line 264))
[19:22:28+0000]   Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)
[19:22:28+0000] Collecting pyasn1==0.5.1 (from -r requirements.txt (line 266))
[19:22:28+0000]   Obtaining dependency information for pyasn1==0.5.1 from https://files.pythonhosted.org/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl.metadata
[19:22:29+0000]   Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)
[19:22:29+0000] Collecting pycparser==2.21 (from -r requirements.txt (line 270))
[19:22:29+0000]   Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)
[19:22:29+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.7/118.7 kB 3.6 MB/s eta 0:00:00
[19:22:31+0000] Collecting pydantic==2.6.0 (from -r requirements.txt (line 272))
[19:22:31+0000]   Obtaining dependency information for pydantic==2.6.0 from https://files.pythonhosted.org/packages/e4/37/3ffe6e7daa1ea1b4bf5228807a92ccbae538cf57c0c50b93564c310c11a8/pydantic-2.6.0-py3-none-any.whl.metadata
[19:22:31+0000]   Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)
[19:22:31+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.8/81.8 kB 3.6 MB/s eta 0:00:00
[19:22:39+0000] Collecting pydantic-core==2.16.1 (from -r requirements.txt (line 274))
[19:22:39+0000]   Obtaining dependency information for pydantic-core==2.16.1 from https://files.pythonhosted.org/packages/98/19/955b83b6e33b7ac27914860069a918fe49b29c13bc149dc7bb7c60954812/pydantic_core-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:22:39+0000]   Downloading pydantic_core-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)
[19:22:39+0000] Collecting pyjwt[crypto]==2.8.0 (from -r requirements.txt (line 276))
[19:22:39+0000]   Obtaining dependency information for pyjwt[crypto]==2.8.0 from https://files.pythonhosted.org/packages/2b/4f/e04a8067c7c96c364cef7ef73906504e2f40d690811c021e1a1901473a19/PyJWT-2.8.0-py3-none-any.whl.metadata
[19:22:39+0000]   Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)
[19:22:39+0000] Collecting python-dateutil==2.8.2 (from -r requirements.txt (line 278))
[19:22:39+0000]   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
[19:22:39+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 19.2 MB/s eta 0:00:00
[19:22:40+0000] Collecting python-jose[cryptography]==3.3.0 (from -r requirements.txt (line 280))
[19:22:40+0000]   Downloading python_jose-3.3.0-py2.py3-none-any.whl (33 kB)
[19:22:40+0000] Collecting pytz==2023.4 (from -r requirements.txt (line 282))
[19:22:40+0000]   Obtaining dependency information for pytz==2023.4 from https://files.pythonhosted.org/packages/3b/dd/9b84302ba85ac6d3d3042d3e8698374838bde1c386b4adb1223d7a0efd4e/pytz-2023.4-py2.py3-none-any.whl.metadata
[19:22:40+0000]   Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)
[19:22:41+0000] Collecting quart==0.19.4 (from -r requirements.txt (line 284))
[19:22:41+0000]   Obtaining dependency information for quart==0.19.4 from https://files.pythonhosted.org/packages/9a/2c/681b4fcecefd98627a90dd5aecdc6b57ba18c9ce07e173d86a0b1274f20b/quart-0.19.4-py3-none-any.whl.metadata
[19:22:41+0000]   Downloading quart-0.19.4-py3-none-any.whl.metadata (5.7 kB)
[19:22:42+0000] Collecting quart-cors==0.7.0 (from -r requirements.txt (line 288))
[19:22:42+0000]   Obtaining dependency information for quart-cors==0.7.0 from https://files.pythonhosted.org/packages/60/fc/1ffe9042df05d48f5eaac4116708fee3f7bb18b696380cc4e3797c8fd510/quart_cors-0.7.0-py3-none-any.whl.metadata
[19:22:42+0000]   Downloading quart_cors-0.7.0-py3-none-any.whl.metadata (9.4 kB)
[19:22:50+0000] Collecting regex==2023.12.25 (from -r requirements.txt (line 290))
[19:22:50+0000]   Obtaining dependency information for regex==2023.12.25 from https://files.pythonhosted.org/packages/8d/6b/2f6478814954c07c04ba60b78d688d3d7bab10d786e0b6c1db607e4f6673/regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:22:50+0000]   Downloading regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
[19:22:50+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 504.8 kB/s eta 0:00:00
[19:22:50+0000] Collecting requests==2.31.0 (from -r requirements.txt (line 292))
[19:22:50+0000]   Obtaining dependency information for requests==2.31.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata
[19:22:50+0000]   Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
[19:22:51+0000] Collecting requests-oauthlib==1.3.1 (from -r requirements.txt (line 299))
[19:22:51+0000]   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
[19:22:51+0000] Collecting rsa==4.9 (from -r requirements.txt (line 301))
[19:22:51+0000]   Downloading rsa-4.9-py3-none-any.whl (34 kB)
[19:22:51+0000] Collecting six==1.16.0 (from -r requirements.txt (line 303))
[19:22:52+0000]   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
[19:22:53+0000] Collecting sniffio==1.3.0 (from -r requirements.txt (line 309))
[19:22:53+0000]   Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)
[19:22:53+0000] Collecting tenacity==8.2.3 (from -r requirements.txt (line 314))
[19:22:53+0000]   Obtaining dependency information for tenacity==8.2.3 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata
[19:22:53+0000]   Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)
[19:22:54+0000] Collecting tiktoken==0.5.2 (from -r requirements.txt (line 316))
[19:22:54+0000]   Obtaining dependency information for tiktoken==0.5.2 from https://files.pythonhosted.org/packages/fb/a9/237dc2db35e6ec0fb7dd63e3d10ebe0377559203bd2a87e12a4adbfc8585/tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:22:54+0000]   Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
[19:22:55+0000] Collecting tqdm==4.66.1 (from -r requirements.txt (line 318))
[19:22:55+0000]   Obtaining dependency information for tqdm==4.66.1 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata
[19:22:55+0000]   Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)
[19:22:55+0000]      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 2.4 MB/s eta 0:00:00
[19:22:55+0000] Collecting types-pillow==10.2.0.20240206 (from -r requirements.txt (line 320))
[19:22:55+0000]   Obtaining dependency information for types-pillow==10.2.0.20240206 from https://files.pythonhosted.org/packages/54/a1/9c24f95c637f5ed77f0a1de9077a06af018acc0d3ffe9bb0843abc13619c/types_Pillow-10.2.0.20240206-py3-none-any.whl.metadata
[19:22:56+0000]   Downloading types_Pillow-10.2.0.20240206-py3-none-any.whl.metadata (1.6 kB)
[19:22:56+0000] Collecting types-pytz==2023.4.0.20240130 (from -r requirements.txt (line 322))
[19:22:56+0000]   Obtaining dependency information for types-pytz==2023.4.0.20240130 from https://files.pythonhosted.org/packages/83/cd/018e825d60d86c1798c7acccfcb3d7c31227793445e4b87423498e8c486d/types_pytz-2023.4.0.20240130-py3-none-any.whl.metadata
[19:22:56+0000]   Downloading types_pytz-2023.4.0.20240130-py3-none-any.whl.metadata (1.5 kB)
[19:22:57+0000] Collecting typing-extensions==4.9.0 (from -r requirements.txt (line 324))
[19:22:57+0000]   Obtaining dependency information for typing-extensions==4.9.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata
[19:22:57+0000]   Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)
[19:22:57+0000] Collecting tzdata==2023.4 (from -r requirements.txt (line 333))
[19:22:57+0000]   Obtaining dependency information for tzdata==2023.4 from https://files.pythonhosted.org/packages/a3/fb/52b62131e21b24ee297e4e95ed41eba29647dad0e0051a92bb66b43c70ff/tzdata-2023.4-py2.py3-none-any.whl.metadata
[19:22:57+0000]   Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)
[19:22:57+0000] Collecting urllib3==2.1.0 (from -r requirements.txt (line 335))
[19:22:57+0000]   Obtaining dependency information for urllib3==2.1.0 from https://files.pythonhosted.org/packages/96/94/c31f58c7a7f470d5665935262ebd7455c7e4c7782eb525658d3dbf4b9403/urllib3-2.1.0-py3-none-any.whl.metadata
[19:22:57+0000]   Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)
[19:22:58+0000] Collecting uvicorn==0.27.0.post1 (from -r requirements.txt (line 337))
[19:22:58+0000]   Obtaining dependency information for uvicorn==0.27.0.post1 from https://files.pythonhosted.org/packages/c7/f3/29caa83f5795b20ed3aca357c648f3ae995ff6ff08e38b22387017abbdc5/uvicorn-0.27.0.post1-py3-none-any.whl.metadata
[19:22:58+0000]   Downloading uvicorn-0.27.0.post1-py3-none-any.whl.metadata (6.4 kB)
[19:22:59+0000] Collecting werkzeug==3.0.1 (from -r requirements.txt (line 339))
[19:22:59+0000]   Obtaining dependency information for werkzeug==3.0.1 from https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl.metadata
[19:22:59+0000]   Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)
[19:23:00+0000] Collecting wrapt==1.16.0 (from -r requirements.txt (line 343))
[19:23:00+0000]   Obtaining dependency information for wrapt==1.16.0 from https://files.pythonhosted.org/packages/6e/52/2da48b35193e39ac53cfb141467d9f259851522d0e8c87153f0ba4205fb1/wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:23:00+0000]   Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
[19:23:00+0000] Collecting wsproto==1.2.0 (from -r requirements.txt (line 350))
[19:23:00+0000]   Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)
[19:23:03+0000] Collecting yarl==1.9.4 (from -r requirements.txt (line 352))
[19:23:03+0000]   Obtaining dependency information for yarl==1.9.4 from https://files.pythonhosted.org/packages/9f/ea/94ad7d8299df89844e666e4aa8a0e9b88e02416cd6a7dd97969e9eae5212/yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
[19:23:03+0000]   Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)
[19:23:03+0000] Collecting zipp==3.17.0 (from -r requirements.txt (line 354))
[19:23:03+0000]   Obtaining dependency information for zipp==3.17.0 from https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl.metadata
[19:23:03+0000]   Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)
[19:23:30+0000] Requirement already satisfied: setuptools>=16.0 in ./antenv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.43b0->-r requirements.txt (line 177)) (65.5.0)
[19:23:51+0000] Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)
[19:23:51+0000] Downloading aiohttp-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
[19:23:51+0000]    ━━━━━━━━━━━━━━━━━��━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 32.0 MB/s eta 0:00:00
[19:23:51+0000] Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)
[19:23:51+0000] Downloading anyio-4.2.0-py3-none-any.whl (85 kB)
[19:23:51+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.5/85.5 kB 6.9 MB/s eta 0:00:00
[19:23:51+0000] Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)
[19:23:51+0000] Downloading attrs-23.2.0-py3-none-any.whl (60 kB)
[19:23:51+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 kB 5.9 MB/s eta 0:00:00
[19:23:51+0000] Downloading azure_core-1.29.7-py3-none-any.whl (192 kB)
[19:23:51+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 192.9/192.9 kB 6.3 MB/s eta 0:00:00
[19:23:51+0000] Downloading azure_core_tracing_opentelemetry-1.0.0b11-py3-none-any.whl (10 kB)
[19:23:51+0000] Downloading azure_identity-1.15.0-py3-none-any.whl (164 kB)
[19:23:52+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164.7/164.7 kB 5.7 MB/s eta 0:00:00
[19:23:52+0000] Downloading azure_monitor_opentelemetry-1.2.0-py3-none-any.whl (20 kB)
[19:23:52+0000] Downloading azure_monitor_opentelemetry_exporter-1.0.0b21-py2.py3-none-any.whl (78 kB)
[19:23:52+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 787.5 kB/s eta 0:00:00
[19:23:52+0000] Downloading azure_search_documents-11.6.0b1-py3-none-any.whl (315 kB)
[19:23:52+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 315.3/315.3 kB 5.6 MB/s eta 0:00:00
[19:23:52+0000] Downloading azure_storage_blob-12.19.0-py3-none-any.whl (394 kB)
[19:23:52+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 394.2/394.2 kB 19.4 MB/s eta 0:00:00
[19:23:52+0000] Downloading blinker-1.7.0-py3-none-any.whl (13 kB)
[19:23:52+0000] Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)
[19:23:52+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.5/162.5 kB 6.3 MB/s eta 0:00:00
[19:23:52+0000] Downloading cffi-1.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (464 kB)
[19:23:52+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 464.8/464.8 kB 3.1 MB/s eta 0:00:00
[19:23:52+0000] Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)
[19:23:52+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.3/140.3 kB 5.7 MB/s eta 0:00:00
[19:23:52+0000] Downloading click-8.1.7-py3-none-any.whl (97 kB)
[19:23:53+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 831.8 kB/s eta 0:00:00
[19:23:53+0000] Downloading cryptography-42.0.1-cp39-abi3-manylinux_2_28_x86_64.whl (4.6 MB)
[19:23:53+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 15.8 MB/s eta 0:00:00
[19:23:53+0000] Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)
[19:23:53+0000] Downloading distro-1.9.0-py3-none-any.whl (20 kB)
[19:23:53+0000] Downloading flask-3.0.1-py3-none-any.whl (101 kB)
[19:23:53+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.2/101.2 kB 1.1 MB/s eta 0:00:00
[19:23:53+0000] Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)
[19:23:53+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 272.3/272.3 kB 12.9 MB/s eta 0:00:00
[19:23:53+0000] Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)
[19:23:53+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 7.0 MB/s eta 0:00:00
[19:23:53+0000] Downloading httpx-0.26.0-py3-none-any.whl (75 kB)
[19:23:54+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.9/75.9 kB 20.7 MB/s eta 0:00:00
[19:23:54+0000] Downloading hypercorn-0.16.0-py3-none-any.whl (59 kB)
[19:23:54+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.9/59.9 kB 17.8 MB/s eta 0:00:00
[19:23:54+0000] Downloading idna-3.6-py3-none-any.whl (61 kB)
[19:23:54+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 3.2 MB/s eta 0:00:00
[19:23:54+0000] Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)
[19:23:54+0000] Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)
[19:23:54+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.2/133.2 kB 9.8 MB/s eta 0:00:00
[19:23:54+0000] Downloading MarkupSafe-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)
[19:23:54+0000] Downloading msal-1.26.0-py2.py3-none-any.whl (99 kB)
[19:23:54+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.0/99.0 kB 10.0 MB/s eta 0:00:00
[19:23:54+0000] Downloading msal_extensions-1.1.0-py3-none-any.whl (19 kB)
[19:23:54+0000] Downloading numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)
[19:23:55+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 9.5 MB/s eta 0:00:00
[19:23:55+0000] Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)
[19:23:55+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.9/57.9 kB 3.1 MB/s eta 0:00:00
[19:23:55+0000] Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_aiohttp_client-0.43b0-py3-none-any.whl (11 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_dbapi-0.43b0-py3-none-any.whl (10 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_django-0.43b0-py3-none-any.whl (18 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_flask-0.43b0-py3-none-any.whl (14 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_httpx-0.43b0-py3-none-any.whl (12 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_psycopg2-0.43b0-py3-none-any.whl (10 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_requests-0.43b0-py3-none-any.whl (12 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_urllib-0.43b0-py3-none-any.whl (11 kB)
[19:23:55+0000] Downloading opentelemetry_instrumentation_urllib3-0.43b0-py3-none-any.whl (11 kB)
[19:23:56+0000] Downloading opentelemetry_instrumentation_wsgi-0.43b0-py3-none-any.whl (13 kB)
[19:23:56+0000] Downloading opentelemetry_resource_detector_azure-0.1.3-py3-none-any.whl (10 kB)
[19:23:56+0000] Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)
[19:23:56+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.6/105.6 kB 1.6 MB/s eta 0:00:00
[19:23:56+0000] Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)
[19:23:56+0000] Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)
[19:23:56+0000] Downloading packaging-23.2-py3-none-any.whl (53 kB)
[19:23:56+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 1.9 MB/s eta 0:00:00
[19:23:56+0000] Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)
[19:23:58+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 7.7 MB/s eta 0:00:00
[19:23:58+0000] Downloading pandas_stubs-2.1.4.231227-py3-none-any.whl (153 kB)
[19:23:58+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153.6/153.6 kB 8.1 MB/s eta 0:00:00
[19:23:58+0000] Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)
[19:23:58+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 16.5 MB/s eta 0:00:00
[19:23:58+0000] Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)
[19:23:58+0000] Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)
[19:23:58+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.9/84.9 kB 6.8 MB/s eta 0:00:00
[19:23:58+0000] Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)
[19:23:58+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 394.2/394.2 kB 9.3 MB/s eta 0:00:00
[19:23:59+0000] Downloading pydantic_core-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)
[19:23:59+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 5.6 MB/s eta 0:00:00
[19:23:59+0000] Downloading pytz-2023.4-py2.py3-none-any.whl (506 kB)
[19:23:59+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 506.5/506.5 kB 7.1 MB/s eta 0:00:00
[19:23:59+0000] Downloading quart-0.19.4-py3-none-any.whl (77 kB)
[19:23:59+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.8/77.8 kB 6.5 MB/s eta 0:00:00
[19:23:59+0000] Downloading quart_cors-0.7.0-py3-none-any.whl (8.0 kB)
[19:23:59+0000] Downloading regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)
[19:23:59+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 785.1/785.1 kB 13.7 MB/s eta 0:00:00
[19:23:59+0000] Downloading requests-2.31.0-py3-none-any.whl (62 kB)
[19:23:59+0000]    ━━━━━━━━━━━━━━━━━━━━━━━���━━━━━━━━━━━━━━━━ 62.6/62.6 kB 1.2 MB/s eta 0:00:00
[19:23:59+0000] Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)
[19:23:59+0000] Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 15.3 MB/s eta 0:00:00
[19:24:00+0000] Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 6.9 MB/s eta 0:00:00
[19:24:00+0000] Downloading types_Pillow-10.2.0.20240206-py3-none-any.whl (52 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.9/52.9 kB 1.1 MB/s eta 0:00:00
[19:24:00+0000] Downloading types_pytz-2023.4.0.20240130-py3-none-any.whl (5.1 kB)
[19:24:00+0000] Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)
[19:24:00+0000] Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.6/346.6 kB 11.7 MB/s eta 0:00:00
[19:24:00+0000] Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.6/104.6 kB 1.2 MB/s eta 0:00:00
[19:24:00+0000] Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.7/60.7 kB 5.7 MB/s eta 0:00:00
[19:24:00+0000] Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 12.6 MB/s eta 0:00:00
[19:24:00+0000] Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.7/80.7 kB 8.0 MB/s eta 0:00:00
[19:24:00+0000] Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 328.1/328.1 kB 20.1 MB/s eta 0:00:00
[19:24:00+0000] Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)
[19:24:00+0000] Downloading openai-1.10.0-py3-none-any.whl (225 kB)
[19:24:00+0000]    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.1/225.1 kB 11.7 MB/s eta 0:00:00
[19:24:01+0000] Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)
[19:24:07+0000] Installing collected packages: pytz, fixedint, azure-common, zipp, wrapt, urllib3, tzdata, typing-extensions, types-pytz, types-pillow, tqdm, tenacity, sniffio, six, regex, pyjwt, pycparser, pyasn1, priority, portalocker, pillow, packaging, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, numpy, multidict, markupsafe, itsdangerous, idna, hyperframe, hpack, h11, frozenlist, distro, click, charset-normalizer, certifi, blinker, attrs, asgiref, annotated-types, aiofiles, yarl, wsproto, werkzeug, uvicorn, rsa, requests, python-dateutil, pydantic-core, pandas-stubs, jinja2, isodate, importlib-metadata, httpcore, h2, ecdsa, deprecated, cffi, anyio, aiosignal, tiktoken, requests-oauthlib, python-jose, pydantic, pandas, opentelemetry-api, hypercorn, httpx, flask, cryptography, azure-core, aiohttp, quart, opentelemetry-sdk, opentelemetry-instrumentation, openai, msrest, azure-storage-blob, azure-search-documents, azure-keyvault-secrets, azure-core-tracing-opentelemetry, quart-cors, opentelemetry-resource-detector-azure, opentelemetry-instrumentation-wsgi, opentelemetry-instrumentation-urllib3, opentelemetry-instrumentation-urllib, opentelemetry-instrumentation-requests, opentelemetry-instrumentation-httpx, opentelemetry-instrumentation-dbapi, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-aiohttp-client, msal, azure-monitor-opentelemetry-exporter, opentelemetry-instrumentation-psycopg2, opentelemetry-instrumentation-flask, opentelemetry-instrumentation-fastapi, opentelemetry-instrumentation-django, msal-extensions, azure-monitor-opentelemetry, azure-identity
[19:25:31+0000] Successfully installed aiofiles-23.2.1 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.2.0 asgiref-3.7.2 attrs-23.2.0 azure-common-1.1.28 azure-core-1.29.7 azure-core-tracing-opentelemetry-1.0.0b11 azure-identity-1.15.0 azure-keyvault-secrets-4.7.0 azure-monitor-opentelemetry-1.2.0 azure-monitor-opentelemetry-exporter-1.0.0b21 azure-search-documents-11.6.0b1 azure-storage-blob-12.19.0 blinker-1.7.0 certifi-2023.11.17 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 cryptography-42.0.1 deprecated-1.2.14 distro-1.9.0 ecdsa-0.18.0 fixedint-0.1.6 flask-3.0.1 frozenlist-1.4.1 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.2 httpx-0.26.0 hypercorn-0.16.0 hyperframe-6.0.1 idna-3.6 importlib-metadata-6.11.0 isodate-0.6.1 itsdangerous-2.1.2 jinja2-3.1.3 markupsafe-2.1.4 msal-1.26.0 msal-extensions-1.1.0 msrest-0.7.1 multidict-6.0.4 numpy-1.26.3 oauthlib-3.2.2 openai-1.10.0 opentelemetry-api-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-aiohttp-client-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-dbapi-0.43b0 opentelemetry-instrumentation-django-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-instrumentation-flask-0.43b0 opentelemetry-instrumentation-httpx-0.43b0 opentelemetry-instrumentation-psycopg2-0.43b0 opentelemetry-instrumentation-requests-0.43b0 opentelemetry-instrumentation-urllib-0.43b0 opentelemetry-instrumentation-urllib3-0.43b0 opentelemetry-instrumentation-wsgi-0.43b0 opentelemetry-resource-detector-azure-0.1.3 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 packaging-23.2 pandas-2.2.0 pandas-stubs-2.1.4.231227 pillow-10.2.0 portalocker-2.8.2 priority-2.0.0 pyasn1-0.5.1 pycparser-2.21 pydantic-2.6.0 pydantic-core-2.16.1 pyjwt-2.8.0 python-dateutil-2.8.2 python-jose-3.3.0 pytz-2023.4 quart-0.19.4 quart-cors-0.7.0 regex-2023.12.25 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 six-1.16.0 sniffio-1.3.0 tenacity-8.2.3 tiktoken-0.5.2 tqdm-4.66.1 types-pillow-10.2.0.20240206 types-pytz-2023.4.0.20240130 typing-extensions-4.9.0 tzdata-2023.4 urllib3-2.1.0 uvicorn-0.27.0.post1 werkzeug-3.0.1 wrapt-1.16.0 wsproto-1.2.0 yarl-1.9.4 zipp-3.17.0

[notice] A new release of pip is available: 23.2.1 -> 24.0
[notice] To update, run: pip install --upgrade pip
Not a vso image, so not writing build commands
Preparing output...

Copying files to destination directory '/tmp/_preCompressedDestinationDir'...
Done in 48 sec(s).
Compressing content of directory '/tmp/_preCompressedDestinationDir'...
Copied the compressed output to '/home/site/wwwroot'

Removing existing manifest file
Creating a manifest file...
Manifest file created.
Copying .ostype to manifest output directory.

Done in 522 sec(s).
```

</details>

Look for these important steps in the Oryx build:

- _Detected following platforms: python: 3.11.7_
    That should match your runtime in the App Service configuration.
- _Running pip install..._
    That should install all the requirements in your requirements.txt - if it didn't find your requirements.txt, then you won't see the packages installed.

If you see all those steps in the Oryx build, then that's a good sign that the build went well, and you can move on to checking the App Service logs.

## Checking the app logs for errors

Select _Advanced Tools_ from the side nav:

![Advanced Tools](images/screenshot_appservice_tools.png)

Select _Go_ to open the Kudu website.

When the Kudu website loads, find the _Current Docker Logs_ link and select _Download as zip_ next to it:

![Screenshot of section with Download logs links](images/screenshot_appservice_dockerlogs.png)

In the downloaded zip file, find the filename that starts with the most recent date and ends with "_default_docker.log":

![Screenshot of downloaded logs](images/screenshot_appservice_downloadedlogs.png)

Open that file to see the full logs, with the most recent logs at the bottom.

<details>
<summary>Here are the full logs for the app successfully starting:</summary>

```plaintext

2024-02-08T19:30:27.900249002Z    _____
2024-02-08T19:30:27.900282702Z   /  _  \ __________ _________   ____
2024-02-08T19:30:27.900288002Z  /  /_\  \\___   /  |  \_  __ \_/ __ \
2024-02-08T19:30:27.900291902Z /    |    \/    /|  |  /|  | \/\  ___/
2024-02-08T19:30:27.900295502Z \____|__  /_____ \____/ |__|    \___  >
2024-02-08T19:30:27.900299602Z         \/      \/                  \/
2024-02-08T19:30:27.900303402Z A P P   S E R V I C E   O N   L I N U X
2024-02-08T19:30:27.900307003Z
2024-02-08T19:30:27.900310303Z Documentation: http://aka.ms/webapp-linux
2024-02-08T19:30:27.900313903Z Python 3.11.4
2024-02-08T19:30:27.900317303Z Note: Any data outside '/home' is not persisted
2024-02-08T19:30:32.956710361Z Starting OpenBSD Secure Shell server: sshd.
2024-02-08T19:30:33.441385332Z Site's appCommandLine: python3 -m gunicorn main:app
2024-02-08T19:30:33.703536564Z Launching oryx with: create-script -appPath /home/site/wwwroot -output /opt/startup/startup.sh -virtualEnvName antenv -defaultApp /opt/defaultsite -userStartupCommand 'python3 -m gunicorn main:app'
2024-02-08T19:30:33.703598264Z Found build manifest file at '/home/site/wwwroot/oryx-manifest.toml'. Deserializing it...
2024-02-08T19:30:33.703605164Z Build Operation ID: 7440a33100749a32
2024-02-08T19:30:33.703609765Z Oryx Version: 0.2.20230707.1, Commit: 0bd28e69919b5e8beba451e8677e3345f0be8361, ReleaseTagName: 20230707.1
2024-02-08T19:30:33.712124127Z Output is compressed. Extracting it...
2024-02-08T19:30:33.712151827Z Extracting '/home/site/wwwroot/output.tar.gz' to directory '/tmp/8dc28dad0e10acb'...
2024-02-08T19:31:08.047051747Z App path is set to '/tmp/8dc28dad0e10acb'
2024-02-08T19:31:08.073259604Z Writing output script to '/opt/startup/startup.sh'
2024-02-08T19:31:08.431803481Z Using packages from virtual environment antenv located at /tmp/8dc28dad0e10acb/antenv.
2024-02-08T19:31:08.431842281Z Updated PYTHONPATH to '/opt/startup/app_logs:/tmp/8dc28dad0e10acb/antenv/lib/python3.11/site-packages'
2024-02-08T19:31:11.043306496Z [2024-02-08 19:31:11 +0000] [75] [INFO] Starting gunicorn 20.1.0
2024-02-08T19:31:11.060556234Z [2024-02-08 19:31:11 +0000] [75] [INFO] Listening at: http://0.0.0.0:8000 (75)
2024-02-08T19:31:11.060586534Z [2024-02-08 19:31:11 +0000] [75] [INFO] Using worker: uvicorn.workers.UvicornWorker
2024-02-08T19:31:11.069707155Z [2024-02-08 19:31:11 +0000] [76] [INFO] Booting worker with pid: 76
2024-02-08T19:31:11.188073718Z [2024-02-08 19:31:11 +0000] [77] [INFO] Booting worker with pid: 77
2024-02-08T19:31:11.415802023Z [2024-02-08 19:31:11 +0000] [78] [INFO] Booting worker with pid: 78
2024-02-08T19:32:20.509338341Z [2024-02-08 19:32:20 +0000] [76] [INFO] Started server process [76]
2024-02-08T19:32:20.521167526Z [2024-02-08 19:32:20 +0000] [77] [INFO] Started server process [77]
2024-02-08T19:32:20.521189626Z [2024-02-08 19:32:20 +0000] [77] [INFO] Waiting for application startup.
2024-02-08T19:32:20.521207626Z [2024-02-08 19:32:20 +0000] [78] [INFO] Started server process [78]
2024-02-08T19:32:20.521212726Z [2024-02-08 19:32:20 +0000] [78] [INFO] Waiting for application startup.
2024-02-08T19:32:20.521217126Z [2024-02-08 19:32:20 +0000] [76] [INFO] Waiting for application startup.
2024-02-08T19:32:20.726894213Z [2024-02-08 19:32:20 +0000] [76] [INFO] Application startup complete.
2024-02-08T19:32:20.726936214Z [2024-02-08 19:32:20 +0000] [78] [INFO] Application startup complete.
2024-02-08T19:32:20.726942614Z [2024-02-08 19:32:20 +0000] [77] [INFO] Application startup complete.
```

</details>

A few notable logs:

- `2024-02-08T19:30:33.441385332Z Site's appCommandLine: python3 -m gunicorn main:app`
    This log indicates that App Service was correctly configured with a custom startup command to run the app.
- `[2024-02-08 19:31:11 +0000] [75] [INFO] Starting gunicorn 20.1.0`
    That's the start of the gunicorn server serving the app.
- `2024-02-08T19:32:20.726942614Z [2024-02-08 19:32:20 +0000] [77] [INFO] Application startup complete.`
    At this point, the app has started successfully.

If you do not see any errors in those logs, then the app should be running successfully. If you do see errors, then try looking in Azure Monitor.

## Checking Azure Monitor for errors

By default, deployed apps use Application Insights to trace and log errors. (If you explicitly opted out of Application Insights, then you won't have this feature.)

In the Azure Portal, navigate to the Application Insights for your app.

To see any exceptions and server errors, navigate to the _Investigate -> Failures_ blade and browse through the exceptions.

![Screenshot of Application Insights Failures tab](images/screenshot_appinsights_failures.png)

## Configuring log levels

By default, the deployed app only logs messages from packages with a level of `WARNING` or higher,
but logs all messages from the app with a level of `INFO` or higher.

These lines of code in `app/backend/app.py` configure the logging level:

```python
# Set root level to WARNING to avoid seeing overly verbose logs from SDKS
logging.basicConfig(level=logging.WARNING)
# Set the app logger level to INFO by default
default_level = "INFO"
app.logger.setLevel(os.getenv("APP_LOG_LEVEL", default_level))
```

To change the default level, either change `default_level` or set the `APP_LOG_LEVEL` environment variable
to one of the [allowed log levels](https://docs.python.org/3/library/logging.html#logging-levels):
`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`.

If you need to log in a route handler, use the the global variable `current_app`'s logger:

```python
async def chat():
    current_app.logger.info("Received /chat request")
```

Otherwise, use the `logging` module's root logger:

```python
logging.info("System message: %s", system_message)
```

If you're having troubles finding the logs in App Service, read the section above on [checking app logs](#checking-the-app-logs-for-errors) or watch [this video about viewing App Service logs](https://www.youtube.com/watch?v=f0-aYuvws54).


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\architecture.md ====

# RAG Chat: Application Architecture

This document provides a detailed architectural overview of this application, a Retrieval Augmented Generation (RAG) application that creates a ChatGPT-like experience over your own documents. It combines Azure OpenAI Service for AI capabilities with Azure AI Search for document indexing and retrieval.

For getting started with the application, see the main [README](../README.md).

## Architecture Diagram

The following diagram illustrates the complete architecture including user interaction flow, application components, and Azure services:

```mermaid
graph TB
    subgraph "User Interface"
        User[👤 User]
        Browser[🌐 Web Browser]
    end

    subgraph "Application Layer"
        subgraph "Frontend"
            React[⚛️ React/TypeScript App<br/>Chat Interface<br/>Settings Panel<br/>Citation Display]
        end
        
        subgraph "Backend"
            API[🐍 Python API<br/>Flask/Quart<br/>Chat Endpoints<br/>Document Upload<br/>Authentication]
            
            subgraph "Approaches"
                CRR[ChatReadRetrieveRead<br/>Approach]
                RTR[RetrieveThenRead<br/>Approach]
            end
        end
    end

    subgraph "Azure Services"
        subgraph "AI Services"
            OpenAI[🤖 Azure OpenAI<br/>GPT-4 Mini<br/>Text Embeddings<br/>GPT-4 Vision]
            Search[🔍 Azure AI Search<br/>Vector Search<br/>Semantic Ranking<br/>Full-text Search]
            DocIntel[📄 Azure Document<br/>Intelligence<br/>Text Extraction<br/>Layout Analysis]
            Vision2[👁️ Azure AI Vision<br/>optional]
            Speech[🎤 Azure Speech<br/>Services optional]
        end
        
        subgraph "Storage & Data"
            Blob[💾 Azure Blob Storage<br/>Document Storage<br/>User Uploads]
            Cosmos[🗃️ Azure Cosmos DB<br/>Chat History<br/>optional]
        end
        
        subgraph "Platform Services"
            ContainerApps[📦 Azure Container Apps<br/>or App Service<br/>Application Hosting]
            AppInsights[📊 Application Insights<br/>Monitoring<br/>Telemetry]
            KeyVault[🔐 Azure Key Vault<br/>Secrets Management]
        end
    end

    subgraph "Data Processing"
        PrepDocs[⚙️ Document Preparation<br/>Pipeline<br/>Text Extraction<br/>Chunking<br/>Embedding Generation<br/>Indexing]
    end

    %% User Interaction Flow
    User -.-> Browser
    Browser <--> React
    React <--> API

    %% Backend Processing
    API --> CRR
    API --> RTR
    
    %% Azure Service Connections
    API <--> OpenAI
    API <--> Search
    API <--> Blob
    API <--> Cosmos
    API <--> Speech
    
    %% Document Processing Flow
    Blob --> PrepDocs
    PrepDocs --> DocIntel
    PrepDocs --> OpenAI
    PrepDocs --> Search
    
    %% Platform Integration
    ContainerApps --> API
    API --> AppInsights
    API --> KeyVault
    
    %% Styling
    classDef userLayer fill:#e1f5fe
    classDef appLayer fill:#f3e5f5
    classDef azureAI fill:#e8f5e8
    classDef azureStorage fill:#fff3e0
    classDef azurePlatform fill:#fce4ec
    classDef processing fill:#f1f8e9
    
    class User,Browser userLayer
    class React,API,CRR,RTR appLayer
    class OpenAI,Search,DocIntel,Vision2,Speech azureAI
    class Blob,Cosmos azureStorage
    class ContainerApps,AppInsights,KeyVault azurePlatform
    class PrepDocs processing
```

## Chat Query Flow

The following sequence diagram shows how a user query is processed:

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend API
    participant S as Azure AI Search
    participant O as Azure OpenAI
    participant Bl as Blob Storage

    U->>F: Enter question
    F->>B: POST /chat with query
    B->>S: Search for relevant documents
    S-->>B: Return search results with citations
    B->>O: Send query + context to GPT model
    O-->>B: Return AI response
    B->>Bl: Log interaction (optional)
    B-->>F: Return response with citations
    F-->>U: Display answer with sources
```

## Document Ingestion Flow

The following diagram shows how documents are processed and indexed:

```mermaid
sequenceDiagram
    participant D as Documents
    participant Bl as Blob Storage
    participant P as PrepDocs Script
    participant DI as Document Intelligence
    participant O as Azure OpenAI
    participant S as Azure AI Search

    D->>Bl: Upload documents
    P->>Bl: Read documents
    P->>DI: Extract text and layout
    DI-->>P: Return extracted content
    P->>P: Split into chunks
    P->>O: Generate embeddings
    O-->>P: Return vector embeddings
    P->>S: Index documents with embeddings
    S-->>P: Confirm indexing complete
```

## Key Components

### Frontend (React/TypeScript)

- **Chat Interface**: Main conversational UI
- **Settings Panel**: Configuration options for AI behavior
- **Citation Display**: Shows sources and references
- **Authentication**: Optional user login integration

### Backend (Python)

- **API Layer**: RESTful endpoints for chat, search, and configuration. See [HTTP Protocol](http_protocol.md) for detailed API documentation.
- **Approach Patterns**: Different strategies for processing queries
  - `ChatReadRetrieveRead`: Multi-turn conversation with retrieval
  - `RetrieveThenRead`: Single-turn Q&A with retrieval
- **Authentication**: Optional integration with Azure Active Directory

### Azure Services Integration

- **Azure OpenAI**: Powers the conversational AI capabilities
- **Azure AI Search**: Provides semantic and vector search over documents
- **Azure Blob Storage**: Stores original documents and processed content
- **Application Insights**: Provides monitoring and telemetry

## Optional Features

The architecture supports several optional features that can be enabled. For detailed configuration instructions, see the [optional features guide](deploy_features.md):

- **GPT-4 with Vision**: Process image-heavy documents
- **Speech Services**: Voice input/output capabilities
- **Chat History**: Persistent conversation storage in Cosmos DB
- **Authentication**: User login and access control
- **Private Endpoints**: Network isolation for enhanced security

## Deployment Options

The application can be deployed using:

- **Azure Container Apps** (default): Serverless container hosting
- **Azure App Service**: Traditional PaaS hosting option. See the [App Service hosting guide](appservice.md) for detailed instructions.

Both options support the same feature set and can be configured through the Azure Developer CLI (azd).


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\azd.md ====

# RAG chat: Deploying with the Azure Developer CLI

This guide includes advanced topics that are not necessary for a basic deployment. If you are new to the project, please consult the main [README](../README.md#deploying) for steps on deploying the project.

[📺 Watch: Deployment of your chat app](https://www.youtube.com/watch?v=mDFZdmn7nhk)

* [How does `azd up` work?](#how-does-azd-up-work)
* [Configuring continuous deployment](#configuring-continuous-deployment)
  * [GitHub actions](#github-actions)
  * [Azure DevOps](#azure-devops)

## How does `azd up` work?

The `azd up` command comes from the [Azure Developer CLI](https://learn.microsoft.com/azure/developer/azure-developer-cli/overview), and takes care of both provisioning the Azure resources and deploying code to the selected Azure hosts.

The `azd up` command uses the `azure.yaml` file combined with the infrastructure-as-code `.bicep` files in the `infra/` folder. The `azure.yaml` file for this project declares several "hooks" for the prepackage step and postprovision steps. The `up` command first runs the `prepackage` hook which installs Node dependencies and builds the React.JS-based JavaScript files. It then packages all the code (both frontend and backend) into a zip file which it will deploy later.

Next, it provisions the resources based on `main.bicep` and `main.parameters.json`. At that point, since there is no default value for the OpenAI resource location, it asks you to pick a location from a short list of available regions. Then it will send requests to Azure to provision all the required resources. With everything provisioned, it runs the `postprovision` hook to process the local data and add it to an Azure AI Search index.

Finally, it looks at `azure.yaml` to determine the Azure host and uploads the zip to Azure App Service. The `azd up` command is now complete, but it may take another 5-10 minutes for the App Service app to be fully available and working, especially for the initial deploy.

Related commands are `azd provision` for just provisioning (if infra files change) and `azd deploy` for just deploying updated app code.

## Configuring continuous deployment

This repository includes both a GitHub Actions workflow and an Azure DevOps pipeline for continuous deployment with every push to `main`. The GitHub Actions workflow is the default, but you can switch to Azure DevOps if you prefer.

More details are available in [Learn.com: Configure a pipeline and push updates](https://learn.microsoft.com/azure/developer/azure-developer-cli/configure-devops-pipeline?tabs=GitHub)

### GitHub actions

After you have deployed the app once with `azd up`, you can enable continuous deployment with GitHub Actions.

Run this command to set up a Service Principal account for CI deployment and to store your `azd` environment variables in GitHub Actions secrets:

```shell
azd pipeline config
```

You can trigger the "Deploy" workflow manually from your GitHub actions, or wait for the next push to main.

If you change your `azd` environment variables at any time (via `azd env set` or as a result of provisioning), re-run that command in order to update the GitHub Actions secrets.

### Azure DevOps

After you have deployed the app once with `azd up`, you can enable continuous deployment with Azure DevOps.

Run this command to set up a Service Principal account for CI deployment and to store your `azd` environment variables in GitHub Actions secrets:

```shell
azd pipeline config --provider azdo
```

If you change your `azd` environment variables at any time (via `azd env set` or as a result of provisioning), re-run that command in order to update the GitHub Actions secrets.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\azure_app_service.md ====

# RAG chat: Deploying on Azure App Service

Due to [a limitation](https://github.com/Azure/azure-dev/issues/2736) of the Azure Developer CLI (`azd`), there can be only one host option in the [azure.yaml](../azure.yaml) file.
By default, `host: containerapp` is used and `host: appservice` is commented out.

To deploy to Azure App Service, please follow the following steps:

1. Comment out `host: containerapp` and uncomment `host: appservice` in the [azure.yaml](../azure.yaml) file.

2. Login to your Azure account:

    ```bash
    azd auth login
    ```

3. Create a new `azd` environment to store the deployment parameters:

    ```bash
    azd env new
    ```

    Enter a name that will be used for the resource group.
    This will create a new folder in the `.azure` folder, and set it as the active environment for any calls to `azd` going forward.

4. Set the deployment target to `appservice`:

    ```bash
    azd env set DEPLOYMENT_TARGET appservice
    ```

5. (Optional) This is the point where you can customize the deployment by setting other `azd` environment variables, in order to [use existing resources](deploy_existing.md), [enable optional features (such as auth or vision)](deploy_features.md), or [deploy to free tiers](deploy_lowcost.md).
6. Provision the resources and deploy the code:

    ```bash
    azd up
    ```

    This will provision Azure resources and deploy this sample to those resources, including building the search index based on the files found in the `./data` folder.

    **Important**: Beware that the resources created by this command will incur immediate costs, primarily from the AI Search resource. These resources may accrue costs even if you interrupt the command before it is fully executed. You can run `azd down` or delete the resources manually to avoid unnecessary spending.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\azure_container_apps.md ====

# RAG chat: Deploying on Azure Container Apps

Due to [a limitation](https://github.com/Azure/azure-dev/issues/2736) of the Azure Developer CLI (`azd`), there can be only one host option in the [azure.yaml](../azure.yaml) file.
By default, `host: containerapp` is used and `host: appservice` is commented out.

However, if you have an older version of the repo, you may need to follow these steps to deploy to Container Apps instead, or you can stick with Azure App Service.

To deploy to Azure Container Apps, please follow the following steps:

1. Comment out `host: appservice` and uncomment `host: containerapp` in the [azure.yaml](../azure.yaml) file.

2. Login to your Azure account:

    ```bash
    azd auth login
    ```

3. Create a new `azd` environment to store the deployment parameters:

    ```bash
    azd env new
    ```

    Enter a name that will be used for the resource group.
    This will create a new folder in the `.azure` folder, and set it as the active environment for any calls to `azd` going forward.

4. Set the deployment target to `containerapps`:

    ```bash
    azd env set DEPLOYMENT_TARGET containerapps
    ```

5. (Optional) This is the point where you can customize the deployment by setting other `azd1 environment variables, in order to [use existing resources](docs/deploy_existing.md), [enable optional features (such as auth or vision)](docs/deploy_features.md), or [deploy to free tiers](docs/deploy_lowcost.md).
6. Provision the resources and deploy the code:

    ```bash
    azd up
    ```

    This will provision Azure resources and deploy this sample to those resources, including building the search index based on the files found in the `./data` folder.

    **Important**: Beware that the resources created by this command will incur immediate costs, primarily from the AI Search resource. These resources may accrue costs even if you interrupt the command before it is fully executed. You can run `azd down` or delete the resources manually to avoid unnecessary spending.

## Customizing Workload Profile

The default workload profile is Consumption. If you want to use a dedicated workload profile like D4, please run:

```bash
azd env set AZURE_CONTAINER_APPS_WORKLOAD_PROFILE D4
```

For a full list of workload profiles, please check [the workload profile documentation](https://learn.microsoft.com/azure/container-apps/workload-profiles-overview#profile-types).
Please note dedicated workload profiles have a different billing model than Consumption plan. Please check [the billing documentation](https://learn.microsoft.com/azure/container-apps/billing) for details.

## Private endpoints

Private endpoints is still in private preview for Azure Container Apps and not supported for now.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\customization.md ====

# RAG chat: Customizing the chat app

[📺 Watch: (RAG Deep Dive series) Customizing the app](https://www.youtube.com/watch?v=D3slfMqydHc)

> **Tip:** We recommend using GitHub Copilot Agent mode when adding new features or making code changes. This project includes an [AGENTS.md](../AGENTS.md) file that guides Copilot to generate code following project conventions.

This guide provides more details for customizing the RAG chat app.

- [Using your own data](#using-your-own-data)
- [Customizing the UI](#customizing-the-ui)
- [Customizing the backend](#customizing-the-backend)
  - [Chat/Ask approaches](#chatask-approaches)
    - [Chat approach](#chat-approach)
    - [Ask approach](#ask-approach)
- [Improving answer quality](#improving-answer-quality)
  - [Identify the problem point](#identify-the-problem-point)
  - [Improving OpenAI ChatCompletion results](#improving-openai-chatcompletion-results)
  - [Improving Azure AI Search results](#improving-azure-ai-search-results)
  - [Evaluating answer quality](#evaluating-answer-quality)

## Using your own data

The Chat App is designed to work with any PDF documents. The sample data is provided to help you get started quickly, but you can easily replace it with your own data. You'll want to first remove all the existing data, then add your own. See the [data ingestion guide](data_ingestion.md) for more details.

## Customizing the UI

The frontend is built using [React](https://reactjs.org/) and [Fluent UI components](https://react.fluentui.dev/). The frontend components are stored in the `app/frontend/src` folder. To modify the page title, header text, example questions, and other UI elements, you can customize the `app/frontend/src/locales/{en/es/fr/jp/it}/translation.json` file for different languages(English is the default). The primary strings and labels used throughout the application are defined within these files.

## Customizing the backend

The backend is built using [Quart](https://quart.palletsprojects.com/), a Python framework for asynchronous web applications. The backend code is stored in the `app/backend` folder. The frontend and backend communicate over HTTP using JSON or streamed NDJSON responses. Learn more in the [HTTP Protocol guide](http_protocol.md).

### Chat/Ask approaches

Typically, the primary backend code you'll want to customize is the `app/backend/approaches` folder, which contains the classes powering the Chat and Ask tabs. Each class uses a different RAG (Retrieval Augmented Generation) approach, which include system messages that should be changed to match your data

#### Chat approach

The chat tab uses the approach programmed in [chatreadretrieveread.py](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/chatreadretrieveread.py).

1. **Query rewriting**: It calls the OpenAI ChatCompletion API to turn the user question into a good search query, using the prompt and tools from [chat_query_rewrite.prompty](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_query_rewrite.prompty).
2. **Search**: It queries Azure AI Search for search results for that query (optionally using the vector embeddings for that query).
3. **Answering**: It then calls the OpenAI ChatCompletion API to answer the question based on the sources, using the prompt from [chat_answer_question.prompty](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_answer_question.prompty). That call includes the past message history as well (or as many messages fit inside the model's token limit).

The prompts are currently tailored to the sample data since they start with "Assistant helps the company employees with their healthcare plan questions, and questions about the employee handbook." Modify the [chat_query_rewrite.prompty](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_query_rewrite.prompty) and [chat_answer_question.prompty](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_answer_question.prompty) prompts to match your data.

##### Chat with multimodal feature

If you followed the instructions in [the multimodal guide](multimodal.md) to enable multimodal RAG,
there are several differences in the chat approach:

1. **Query rewriting**: Unchanged.
2. **Search**: For this step, it calculates a vector embedding for the user question using [the Azure AI Vision vectorize text API](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/image-retrieval#call-the-vectorize-text-api), and passes that to the Azure AI Search to compare against the image embedding fields in the indexed documents. For each matching document, it downloads each associated image from Azure Blob Storage and converts it to a base 64 encoding.
3. **Answering**: When it combines the search results and user question, it includes the base 64 encoded images, and sends along both the text and images to the multimodal LLM. The model generates a response that includes citations to the images, and the UI renders the images when a citation is clicked.

The settings can be customized to disable calculating the image vector embeddings or to disable sending image inputs to the LLM, if desired.

#### Ask approach

The ask tab uses the approach programmed in [retrievethenread.py](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/retrievethenread.py).

1. **Search**: It queries Azure AI Search for search results for the user question (optionally using the vector embeddings for that question).
2. **Answering**: It then combines the search results and user question, and calls the OpenAI ChatCompletion API to answer the question based on the sources, using the prompt from [ask_answer_question.prompty](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/ask_answer_question.prompty).

The prompt for step 2 is currently tailored to the sample data since it starts with "You are an intelligent assistant helping Contoso Inc employees with their healthcare plan questions and employee handbook questions." Modify [ask_answer_question.prompty](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/ask_answer_question.prompty) to match your data.

#### Ask with multimodal feature

If you followed the instructions in [the multimodal guide](multimodal.md) to enable multimodal RAG,
there are several differences in the ask approach:

1. **Search**: For this step, it also calculates a vector embedding for the user question using [the Azure AI Vision vectorize text API](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/image-retrieval#call-the-vectorize-text-api), and passes that to the Azure AI Search to compare against the image embedding fields in the indexed documents. For each matching document, it downloads each associated image from Azure Blob Storage and converts it to a base 64 encoding.
2. **Answering**: When it combines the search results and user question, it includes the base 64 encoded images, and sends along both the text and images to the multimodal LLM. The model generates a response that includes citations to the images, and the UI renders the images when a citation is clicked.

The settings can be customized to disable calculating the image vector embeddings or to disable sending image inputs to the LLM, if desired.

#### Making settings overrides permanent

The UI provides a "Developer Settings" menu for customizing the approaches, like disabling semantic ranker or using vector search.
Those settings are passed in the "context" field of the request to the backend, and are not saved permanently.
However, if you find a setting that you do want to make permanent, there are two approaches:

1. Change the defaults in the frontend. You'll find the defaults in `Chat.tsx` and `Ask.tsx`. For example, this line of code sets the default retrieval mode to Hybrid:

    ```typescript
    const [retrievalMode, setRetrievalMode] = useState<RetrievalMode>(RetrievalMode.Hybrid);
    ```

    You can change the default to Text by changing the code to:

    ```typescript
    const [retrievalMode, setRetrievalMode] = useState<RetrievalMode>(RetrievalMode.Text);
    ```

2. Change the overrides in the backend. Each of the approaches has a `run` method that takes a `context` parameter, and the first line of code extracts the overrides from that `context`. That's where you can override any of the settings. For example, to change the retrieval mode to text:

    ```python
    overrides = context.get("overrides", {})
    overrides["retrieval_mode"] = "text"
    ```

    By changing the setting on the backend, you can safely remove the Developer Settings UI from the frontend, if you don't wish to expose that to your users.

## Improving answer quality

Once you are running the chat app on your own data and with your own tailored system prompt,
the next step is to test the app with questions and note the quality of the answers.
If you notice any answers that aren't as good as you'd like, here's a process for improving them.

### Identify the problem point

The first step is to identify where the problem is occurring. For example, if using the Chat tab, the problem could be:

1. OpenAI ChatCompletion API is not generating a good search query based on the user question
2. Azure AI Search is not returning good search results for the query
3. OpenAI ChatCompletion API is not generating a good answer based on the search results and user question

You can look at the "Thought process" tab in the chat app to see each of those steps,
and determine which one is the problem.

### Improving OpenAI ChatCompletion results

If the problem is with the ChatCompletion API calls (steps 1 or 3 above), you can try changing the relevant prompt.

Once you've changed the prompt, make sure you ask the same question multiple times to see if the overall quality has improved, and [run an evaluation](#evaluating-answer-quality) when you're satisfied with the changes. The ChatCompletion API can yield different results every time, even for a temperature of 0.0, but especially for a higher temperature than that (like our default of 0.7 for step 3).

You can also try changing the ChatCompletion parameters, like temperature, to see if that improves results for your domain.

### Improving Azure AI Search results

If the problem is with Azure AI Search (step 2 above), the first step is to check what search parameters you're using. Generally, the best results are found with hybrid search (text + vectors) plus the additional semantic re-ranking step, and that's what we've enabled by default. There may be some domains where that combination isn't optimal, however. Check out this blog post which [evaluates AI search strategies](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid-retrieval-and-ranking-ca/3929167) for a better understanding of the differences, or watch this [RAG Deep Dive video on AI Search](https://www.youtube.com/watch?v=ugJy9QkgLYg).

#### Configuring parameters in the app

You can change many of the search parameters in the "Developer settings" in the frontend and see if results improve for your queries. The most relevant options:

![Screenshot of search options in developer settings](images/screenshot_searchoptions.png)

#### Configuring parameters in the Azure Portal

You may find it easier to experiment with search options with the index explorer in the Azure Portal.
Open up the Azure AI Search resource, select the Indexes tab, and select the index there.

Then use the JSON view of the search explorer, and make sure you specify the same options you're using in the app. For example, this query represents a search with semantic ranker configured:

```json
{
  "search": "eye exams",
  "queryType": "semantic",
  "semanticConfiguration": "default",
  "queryLanguage": "en-us",
  "speller": "lexicon",
  "top": 3
}
```

You can also use the `highlight` parameter to see what text is being matched in the `content` field in the search results.

```json
{
    "search": "eye exams",
    "highlight": "content"
    ...
}
```

![Screenshot of search explorer with highlighted results](images/screenshot_searchindex.png)

The search explorer works well for testing text, but is harder to use with vectors, since you'd also need to compute the vector embedding and send it in. It is probably easier to use the app frontend for testing vectors/hybrid search.

#### Other approaches to improve search results

Here are additional ways for improving the search results:

- Adding additional metadata to the "content" field, like the document title, so that it can be matched in the search results. Modify [searchmanager.py](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/prepdocslib/searchmanager.py) to include more text in the `content` field.
- Making additional fields searchable by the full text search step. For example, the "sourcepage" field is not currently searchable, but you could make that into a `SearchableField` with `searchable=True` in [searchmanager.py](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/prepdocslib/searchmanager.py). A change like that requires [re-building the index](https://learn.microsoft.com/azure/search/search-howto-reindex#change-an-index-schema).
- Using function calling to search by particular fields, like searching by the filename. See this blog post on [function calling for structured retrieval](https://blog.pamelafox.org/2024/03/rag-techniques-using-function-calling.html).
- Using a different splitting strategy for the documents, or modifying the existing ones, to improve the chunks that are indexed. You can find the currently available splitters in [textsplitter.py](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/prepdocslib/textsplitter.py).

### Evaluating answer quality

Once you've made changes to the prompts or settings, you'll want to rigorously evaluate the results to see if they've improved. Follow the [evaluation guide](./evaluation.md) to learn how to run evaluations, review results, and compare answers across runs.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\data_ingestion.md ====

# RAG chat: Data ingestion

The [azure-search-openai-demo](/) project can set up a full RAG chat app on Azure AI Search and OpenAI so that you can chat on custom data, like internal enterprise data or domain-specific knowledge sets. For full instructions on setting up the project, consult the [main README](/README.md), and then return here for detailed instructions on the data ingestion component.

The chat app provides two ways to ingest data: manual indexing and integrated vectorization. This document explains the differences between the two approaches and provides an overview of the manual indexing process.

- [Supported document formats](#supported-document-formats)
- [Manual indexing process](#manual-indexing-process)
  - [Chunking](#chunking)
  - [Categorizing data for enhanced search](#enhancing-search-functionality-with-data-categorization)
  - [Indexing additional documents](#indexing-additional-documents)
  - [Removing documents](#removing-documents)
- [Integrated Vectorization](#integrated-vectorization)
  - [Indexing of additional documents](#indexing-of-additional-documents)
  - [Removal of documents](#removal-of-documents)
  - [Scheduled indexing](#scheduled-indexing)
- [Debugging tips](#debugging-tips)

## Supported document formats

In order to ingest a document format, we need a tool that can turn it into text. By default, the manual indexing uses Azure Document Intelligence (DI in the table below), but we also have local parsers for several formats. The local parsers are not as sophisticated as Azure Document Intelligence, but they can be used to decrease charges.

| Format | Manual indexing                      | Integrated Vectorization |
| ------ | ------------------------------------ | ------------------------ |
| PDF    | Yes (DI or local with PyPDF)         | Yes                      |
| HTML   | Yes (DI or local with BeautifulSoup) | Yes                      |
| DOCX, PPTX, XLSX   | Yes (DI)                             | Yes                      |
| Images (JPG, PNG, BPM, TIFF, HEIFF)| Yes (DI) | Yes                      |
| TXT    | Yes (Local)                          | Yes                      |
| JSON   | Yes (Local)                          | Yes                      |
| CSV    | Yes (Local)                          | Yes                      |

The Blob indexer used by the Integrated Vectorization approach also supports a few [additional formats](https://learn.microsoft.com/azure/search/search-howto-indexing-azure-blob-storage#supported-document-formats).

## Manual indexing process

The [`prepdocs.py`](../app/backend/prepdocs.py) script is responsible for both uploading and indexing documents. The typical usage is to call it using `scripts/prepdocs.sh` (Mac/Linux) or `scripts/prepdocs.ps1` (Windows), as these scripts will set up a Python virtual environment and pass in the required parameters based on the current `azd` environment. You can pass additional arguments directly to the script, for example `scripts/prepdocs.ps1 --removeall`. Whenever `azd up` or `azd provision` is run, the script is called automatically.

![Diagram of the indexing process](images/diagram_prepdocs.png)

The script uses the following steps to index documents:

1. If it doesn't yet exist, create a new index in Azure AI Search.
2. Upload the PDFs to Azure Blob Storage.
3. Split the PDFs into chunks of text.
4. Upload the chunks to Azure AI Search. If using vectors (the default), also compute the embeddings and upload those alongside the text.

### Chunking

We're often asked why we need to break up the PDFs into chunks when Azure AI Search supports searching large documents.

Chunking allows us to limit the amount of information we send to OpenAI due to token limits. By breaking up the content, it allows us to easily find potential chunks of text that we can inject into OpenAI. The method of chunking we use leverages a sliding window of text such that sentences that end one chunk will start the next. This allows us to reduce the chance of losing the context of the text.

If needed, you can modify the chunking algorithm in `app/backend/prepdocslib/textsplitter.py`. For a deeper, diagram-rich explanation of how the splitter works (figures, recursion, merge heuristics, guarantees, and examples), see the [text splitter documentation](./textsplitter.md).

### Enhancing search functionality with data categorization

To enhance search functionality, categorize data during the ingestion process with the `--category` argument, for example `scripts/prepdocs.ps1 --category ExampleCategoryName`. This argument specifies the category to which the data belongs, enabling you to filter search results based on these categories.

After running the script with the desired category, ensure these categories are added to the 'Include Category' dropdown list. This can be found in the developer settings in [`Settings.tsx`](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/frontend/src/components/Settings/Settings.tsx). The default option for this dropdown is "All". By including specific categories, you can refine your search results more effectively.

### Indexing additional documents

To upload more PDFs, put them in the data/ folder and run `./scripts/prepdocs.sh` or `./scripts/prepdocs.ps1`.

A [recent change](https://github.com/Azure-Samples/azure-search-openai-demo/pull/835) added checks to see what's been uploaded before. The prepdocs script now writes an .md5 file with an MD5 hash of each file that gets uploaded. Whenever the prepdocs script is re-run, that hash is checked against the current hash and the file is skipped if it hasn't changed.

### Removing documents

You may want to remove documents from the index. For example, if you're using the sample data, you may want to remove the documents that are already in the index before adding your own.

To remove all documents, use `./scripts/prepdocs.sh --removeall` or `./scripts/prepdocs.ps1 --removeall`.

You can also remove individual documents by using the `--remove` flag. Open either `scripts/prepdocs.sh` or `scripts/prepdocs.ps1` and replace `/data/*` with `/data/YOUR-DOCUMENT-FILENAME-GOES-HERE.pdf`. Then run `scripts/prepdocs.sh --remove` or `scripts/prepdocs.ps1 --remove`.

## Integrated Vectorization

Azure AI Search includes an [integrated vectorization feature](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/announcing-the-public-preview-of-integrated-vectorization-in-azure-ai-search/3960809), a cloud-based approach to data ingestion. Integrated vectorization takes care of document format cracking, data extraction, chunking, vectorization, and indexing, all with Azure technologies.

See [this notebook](https://github.com/Azure/azure-search-vector-samples/blob/main/demo-python/code/integrated-vectorization/azure-search-integrated-vectorization-sample.ipynb) to understand the process of setting up integrated vectorization.
We have integrated that code into our `prepdocs` script, so you can use it without needing to understand the details.

You must first explicitly [enable integrated vectorization](./deploy_features.md#enabling-integrated-vectorization) in the `azd` environment to use this feature.

This feature cannot be used on existing index. You need to create a new index or drop and recreate an existing index.
In the newly created index schema, a new field 'parent_id' is added. This is used internally by the indexer to manage life cycle of chunks.

This feature is not supported in the free SKU for Azure AI Search.

### Indexing of additional documents

To add additional documents to the index, first upload them to your data source (Blob storage, by default).
Then navigate to the Azure portal, find the index, and run it.
The Azure AI Search indexer will identify the new documents and ingest them into the index.

### Removal of documents

To remove documents from the index, remove them from your data source (Blob storage, by default).
Then navigate to the Azure portal, find the index, and run it.
The Azure AI Search indexer will take care of removing those documents from the index.

### Scheduled indexing

If you would like the indexer to run automatically, you can set it up to [run on a schedule](https://learn.microsoft.com/azure/search/search-howto-schedule-indexers).

## Debugging tips

If you are not sure if a file successfully uploaded, you can query the index from the Azure Portal or from the REST API. Open the index and paste the queries below into the search bar.

To see all the filenames uploaded to the index:

```json
{
  "search": "*",
  "count": true,
  "top": 1,
  "facets": ["sourcefile"]
}
```

To search for specific filenames:

```json
{
  "search": "*",
  "count": true,
  "top": 1,
  "filter": "sourcefile eq 'employee_handbook.pdf'",
  "facets": ["sourcefile"]
}
```


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\deploy_existing.md ====


# RAG chat: Deploying with existing Azure resources

If you already have existing Azure resources, or if you want to specify the exact name of new Azure Resource, you can do so by setting `azd` environment values.
You should set these values before running `azd up`. Once you've set them, return to the [deployment steps](../README.md#deploying).

* [Resource group](#resource-group)
* [OpenAI resource](#openai-resource)
* [Azure AI Search resource](#azure-ai-search-resource)
* [Azure App Service Plan and App Service resources](#azure-app-service-plan-and-app-service-resources)
* [Azure Application Insights and related resources](#azure-application-insights-and-related-resources)
* [Azure AI Vision resources](#azure-ai-vision-resources)
* [Azure Document Intelligence resource](#azure-document-intelligence-resource)
* [Azure Speech resource](#azure-speech-resource)
* [Other Azure resources](#other-azure-resources)

## Resource group

1. Run `azd env set AZURE_RESOURCE_GROUP {Name of existing resource group}`
1. Run `azd env set AZURE_LOCATION {Location of existing resource group}`

## OpenAI resource

### Azure OpenAI

1. Run `azd env set AZURE_OPENAI_SERVICE {Name of existing OpenAI service}`
1. Run `azd env set AZURE_OPENAI_RESOURCE_GROUP {Name of existing resource group that OpenAI service is provisioned to}`
1. Run `azd env set AZURE_OPENAI_LOCATION {Location of existing OpenAI service}`
1. Run `azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT {Name of existing chat deployment}`. Only needed if your chat deployment name is not the default 'gpt-4.1-mini'.
1. Run `azd env set AZURE_OPENAI_CHATGPT_MODEL {Model name of existing chat deployment}`. Only needed if your chat model is not the default 'gpt-4.1-mini'.
1. Run `azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION {Version string for existing chat deployment}`. Only needed if your chat deployment model version is not the default '2024-07-18'. You definitely need to change this if you changed the model.
1. Run `azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU {Name of SKU for existing chat deployment}`. Only needed if your chat deployment SKU is not the default 'Standard', like if it is 'GlobalStandard' instead.
1. Run `azd env set AZURE_OPENAI_EMB_DEPLOYMENT {Name of existing embedding deployment}`. Only needed if your embeddings deployment is not the default 'embedding'.
1. Run `azd env set AZURE_OPENAI_EMB_MODEL_NAME {Model name of existing embedding deployment}`. Only needed if your embeddings model is not the default 'text-embedding-3-large'.
1. Run `azd env set AZURE_OPENAI_EMB_DIMENSIONS {Dimensions for existing embedding deployment}`. Only needed if your embeddings model is not the default 'text-embedding-3-large'.
1. Run `azd env set AZURE_OPENAI_EMB_DEPLOYMENT_VERSION {Version string for existing embedding deployment}`. If your embeddings deployment is one of the 'text-embedding-3' models, set this to the number 1.
1. This project does *not* use keys when authenticating to Azure OpenAI. However, if your Azure OpenAI service must have key access enabled for some reason (like for use by other projects), then run `azd env set AZURE_OPENAI_DISABLE_KEYS false`. The default value is `true` so you should only run the command if you need key access.

When you run `azd up` after and are prompted to select a value for `openAiResourceGroupLocation`, make sure to select the same location as the existing OpenAI resource group.

### Openai.com OpenAI

1. Run `azd env set OPENAI_HOST openai`
2. Run `azd env set OPENAI_ORGANIZATION {Your OpenAI organization}`
3. Run `azd env set OPENAI_API_KEY {Your OpenAI API key}`
4. Run `azd up`

You can retrieve your OpenAI key by checking [your user page](https://platform.openai.com/account/api-keys) and your organization by navigating to [your organization page](https://platform.openai.com/account/org-settings).
Learn more about creating an OpenAI free trial at [this link](https://openai.com/pricing).
Do *not* check your key into source control.

When you run `azd up` after and are prompted to select a value for `openAiResourceGroupLocation`, you can select any location as it will not be used.

## Azure AI Search resource

1. Run `azd env set AZURE_SEARCH_SERVICE {Name of existing Azure AI Search service}`
1. Run `azd env set AZURE_SEARCH_SERVICE_RESOURCE_GROUP {Name of existing resource group with ACS service}`
1. If that resource group is in a different location than the one you'll pick for the `azd up` step,
  then run `azd env set AZURE_SEARCH_SERVICE_LOCATION {Location of existing service}`
1. If the search service's SKU is not standard, then run `azd env set AZURE_SEARCH_SERVICE_SKU {Name of SKU}`. If you specify the free tier, then your app will no longer be able to use semantic ranker. Be advised that [search SKUs cannot be changed](https://learn.microsoft.com/azure/search/search-sku-tier#tier-upgrade-or-downgrade). ([See other possible SKU values](https://learn.microsoft.com/azure/templates/microsoft.search/searchservices?pivots=deployment-language-bicep#sku))
1. If you have an existing index that is set up with all the expected fields, then run `azd env set AZURE_SEARCH_INDEX {Name of existing index}`. Otherwise, the `azd up` command will create a new index.

You can also customize the search service (new or existing) for non-English searches:

1. To configure the language of the search query to a value other than "en-US", run `azd env set AZURE_SEARCH_QUERY_LANGUAGE {Name of query language}`. ([See other possible values](https://learn.microsoft.com/rest/api/searchservice/preview-api/search-documents#queryLanguage))
1. To turn off the spell checker, run `azd env set AZURE_SEARCH_QUERY_SPELLER none`. Consult [this table](https://learn.microsoft.com/rest/api/searchservice/preview-api/search-documents#queryLanguage) to determine if spell checker is supported for your query language.
1. To configure the name of the analyzer to use for a searchable text field to a value other than "en.microsoft", run `azd env set AZURE_SEARCH_ANALYZER_NAME {Name of analyzer name}`. ([See other possible values](https://learn.microsoft.com/dotnet/api/microsoft.azure.search.models.field.analyzer?view=azure-dotnet-legacy&viewFallbackFrom=azure-dotnet))

## Azure App Service Plan and App Service resources

1. Run `azd env set AZURE_APP_SERVICE_PLAN {Name of existing Azure App Service Plan}`
1. Run `azd env set AZURE_APP_SERVICE {Name of existing Azure App Service}`.
1. Run `azd env set AZURE_APP_SERVICE_SKU {SKU of Azure App Service, defaults to B1}`.

## Azure Application Insights and related resources

1. Run `azd env set AZURE_APPLICATION_INSIGHTS {Name of existing Azure App Insights}`.
1. Run `azd env set AZURE_APPLICATION_INSIGHTS_DASHBOARD {Name of existing Azure App Insights Dashboard}`.
1. Run `azd env set AZURE_LOG_ANALYTICS {Name of existing Azure Log Analytics Workspace Name}`.

## Azure AI Vision resources

1. Run `azd env set AZURE_VISION_SERVICE {Name of existing Azure AI Vision Service Name}`
1. Run `azd env set AZURE_VISION_RESOURCE_GROUP {Name of existing Azure AI Vision Resource Group Name}`
1. Run `azd env set AZURE_VISION_LOCATION {Name of existing Azure AI Vision Location}`
1. Run `azd env set AZURE_VISION_SKU {SKU of Azure AI Vision service, defaults to F0}`

## Azure Document Intelligence resource

In order to support analysis of many document formats, this repository uses a preview version of Azure Document Intelligence (formerly Form Recognizer) that is only available in [limited regions](https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-layout).
If your existing resource is in one of those regions, then you can re-use it by setting the following environment variables:

1. Run `azd env set AZURE_DOCUMENTINTELLIGENCE_SERVICE {Name of existing Azure AI Document Intelligence service}`
1. Run `azd env set AZURE_DOCUMENTINTELLIGENCE_LOCATION {Location of existing service}`
1. Run `azd env set AZURE_DOCUMENTINTELLIGENCE_RESOURCE_GROUP {Name of resource group with existing service, defaults to main resource group}`
1. Run `azd env set AZURE_DOCUMENTINTELLIGENCE_SKU {SKU of existing service, defaults to S0}`

## Azure Speech resource

1. Run `azd env set AZURE_SPEECH_SERVICE {Name of existing Azure Speech service}`
1. Run `azd env set AZURE_SPEECH_SERVICE_RESOURCE_GROUP {Name of existing resource group with speech service}`
1. If that resource group is in a different location than the one you'll pick for the `azd up` step,
  then run `azd env set AZURE_SPEECH_SERVICE_LOCATION {Location of existing service}`
1. If the speech service's SKU is not "S0", then run `azd env set AZURE_SPEECH_SERVICE_SKU {Name of SKU}`.

## Other Azure resources

You can also use existing Azure AI Storage Accounts. See `./infra/main.parameters.json` for list of environment variables to pass to `azd env set` to configure those existing resources.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\deploy_features.md ====

# RAG chat: Enabling optional features

This document covers optional features that can be enabled in the deployed Azure resources.
You should typically enable these features before running `azd up`. Once you've set them, return to the [deployment steps](../README.md#deploying).

* [Using different chat completion models](#using-different-chat-completion-models)
* [Using reasoning models](#using-reasoning-models)
* [Using different embedding models](#using-different-embedding-models)
* [Enabling multimodal embeddings and answering](#enabling-multimodal-embeddings-and-answering)
* [Enabling media description with Azure Content Understanding](#enabling-media-description-with-azure-content-understanding)
* [Enabling client-side chat history](#enabling-client-side-chat-history)
* [Enabling persistent chat history with Azure Cosmos DB](#enabling-persistent-chat-history-with-azure-cosmos-db)
* [Enabling language picker](#enabling-language-picker)
* [Enabling speech input/output](#enabling-speech-inputoutput)
* [Enabling Integrated Vectorization](#enabling-integrated-vectorization)
* [Enabling authentication](#enabling-authentication)
* [Enabling login and document level access control](#enabling-login-and-document-level-access-control)
* [Enabling user document upload](#enabling-user-document-upload)
* [Enabling CORS for an alternate frontend](#enabling-cors-for-an-alternate-frontend)
* [Enabling query rewriting](#enabling-query-rewriting)
* [Adding an OpenAI load balancer](#adding-an-openai-load-balancer)
* [Deploying with private endpoints](#deploying-with-private-endpoints)
* [Using local parsers](#using-local-parsers)

## Using different chat completion models

As of early June 2025, the default chat completion model is `gpt-4.1-mini`. If you deployed this sample before that date, the default model is `gpt-3.5-turbo` or `gpt-4o-mini`. You can change the chat completion model to any Azure OpenAI chat model that's available in your Azure OpenAI resource region by following these steps:

1. To set the name of the deployment, run this command with a unique name in your Azure OpenAI account. You can use any deployment name, as long as it's unique in your Azure OpenAI account. For convenience, many developers use the same deployment name as the model name, but this is not required.

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT <your-deployment-name>
    ```

    For example:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT gpt-5-chat
    ```

1. To set the GPT model to a different [available model](https://learn.microsoft.com/azure/ai-services/openai/concepts/models), run this command with the appropriate model name. For reasoning models like gpt-5/o3/o4, check [the reasoning guide](./reasoning.md)

   For gpt-5-chat:

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-5-chat
   ```

    For gpt-4.1-mini:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-4.1-mini
    ```

    For gpt-4o:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-4o
    ```

    For gpt-4o mini:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-4o-mini
    ```

    For gpt-4:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-4
    ```

    For gpt-3.5-turbo:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-35-turbo
    ```

1. To set the Azure OpenAI model version from the [available versions](https://learn.microsoft.com/azure/ai-services/openai/concepts/models), run this command with the appropriate version string.

   For gpt-5-chat:

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-08-07
   ```

    For gpt-4.1-mini:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-04-14
    ```

    For gpt-4o:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2024-05-13
    ```

    For gpt-4o mini:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2024-07-18
    ```

    For gpt-4:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION turbo-2024-04-09
    ```

    For gpt-3.5-turbo:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 0125
    ```

1. To set the Azure OpenAI deployment SKU name, run this command with [the desired SKU name](https://learn.microsoft.com/azure/ai-foundry/foundry-models/concepts/deployment-types).

    For GlobalStandard:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
    ```

    For Standard:

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU Standard
    ```

1. To set the Azure OpenAI deployment capacity (TPM, measured in thousands of tokens per minute), run this command with the desired capacity. This is not necessary if you are using the default capacity of 30.

    ```bash
    azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_CAPACITY 20
    ```

1. To update the deployment with the new parameters, run this command.

    ```bash
    azd up
    ```

This process does *not* delete your previous model deployment. If you want to delete previous deployments, go to your Azure OpenAI resource in Azure AI Foundry and delete it there.

> [!NOTE]
> To revert back to a previous model, run the same commands with the previous model name and version.

## Using reasoning models

This feature allows you to use reasoning models to generate responses based on retrieved content. These models spend more time processing and understanding the user's request.
To enable reasoning models, follow the steps in [the reasoning models guide](./reasoning.md).

## Using agentic retrieval

This feature allows you to use agentic retrieval in place of the Search API. To enable agentic retrieval, follow the steps in [the agentic retrieval guide](./agentic_retrieval.md)

## Using different embedding models

By default, the deployed Azure web app uses the `text-embedding-3-large` embedding model. If you want to use a different embedding model, you can do so by following these steps:

1. Run one of the following commands to set the desired model:

    ```shell
    azd env set AZURE_OPENAI_EMB_MODEL_NAME text-embedding-ada-002
    ```

    ```shell
    azd env set AZURE_OPENAI_EMB_MODEL_NAME text-embedding-3-small
    ```

    ```shell
    azd env set AZURE_OPENAI_EMB_MODEL_NAME text-embedding-3-large
    ```

2. Specify the desired dimensions of the model: (from 256-3072, model dependent)

    Default dimensions for text-embedding-ada-002

    ```shell
    azd env set AZURE_OPENAI_EMB_DIMENSIONS 1536
    ```

    Default dimensions for text-embedding-3-small

    ```shell
    azd env set AZURE_OPENAI_EMB_DIMENSIONS 1536
    ```

    Default dimensions for text-embedding-3-large

    ```shell
    azd env set AZURE_OPENAI_EMB_DIMENSIONS 3072
    ```

3. Set the model version, depending on the model you are using:

    For text-embedding-ada-002:

    ```shell
    azd env set AZURE_OPENAI_EMB_DEPLOYMENT_VERSION 2
    ```

    For text-embedding-3-small and text-embedding-3-large:

    ```shell
    azd env set AZURE_OPENAI_EMB_DEPLOYMENT_VERSION 1
    ```

4. To set the embedding model deployment SKU name, run this command with [the desired SKU name](https://learn.microsoft.com/azure/ai-foundry/foundry-models/concepts/deployment-types).

    For GlobalStandard:

    ```bash
    azd env set AZURE_OPENAI_EMB_DEPLOYMENT_SKU GlobalStandard
    ```

    For Standard:

    ```bash
    azd env set AZURE_OPENAI_EMB_DEPLOYMENT_SKU Standard
    ```

5. When prompted during `azd up`, make sure to select a region for the OpenAI resource group location that supports the desired embedding model and deployment SKU. There are [limited regions available](https://learn.microsoft.com/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#models-by-deployment-type).

If you have already deployed:

* You'll need to change the deployment name by running the appropriate commands for the model above.
* You'll need to create a new index, and re-index all of the data using the new model. You can either delete the current index in the Azure Portal, or create an index with a different name by running `azd env set AZURE_SEARCH_INDEX new-index-name`. When you next run `azd up`, the new index will be created. See the [data ingestion guide](./data_ingestion.md) for more details.

## Enabling multimodal embeddings and answering

When your documents include images, you can optionally enable this feature that can
use image embeddings when searching and also use images when answering questions.

Learn more in the [multimodal guide](./multimodal.md).

## Enabling media description with Azure Content Understanding

⚠️ This feature is not currently compatible with [integrated vectorization](#enabling-integrated-vectorization).
It is compatible with the [multimodal feature](./multimodal.md), but this feature enables only a subset of multimodal capabilities,
so you may want to enable the multimodal feature instead or as well.

By default, if your documents contain image-like figures, the data ingestion process will ignore those figures,
so users will not be able to ask questions about them.

You can optionably enable the description of media content using Azure Content Understanding. When enabled, the data ingestion process will send figures to Azure Content Understanding and replace the figure with the description in the indexed document.

To enable media description with Azure Content Understanding, run:

```shell
azd env set USE_MEDIA_DESCRIBER_AZURE_CU true
```

If you have already run `azd up`, you will need to run `azd provision` to create the new Content Understanding service.
If you have already indexed your documents and want to re-index them with the media descriptions,
first [remove the existing documents](./data_ingestion.md#removing-documents) and then [re-ingest the data](./data_ingestion.md#indexing-additional-documents).

⚠️ This feature does not yet support DOCX, PPTX, or XLSX formats. If you have figures in those formats, they will be ignored.
Convert them first to PDF or image formats to enable media description.

## Enabling client-side chat history

[📺 Watch: (RAG Deep Dive series) Storing chat history](https://www.youtube.com/watch?v=1YiTFnnLVIA)

This feature allows users to view the chat history of their conversation, stored in the browser using [IndexedDB](https://developer.mozilla.org/docs/Web/API/IndexedDB_API). That means the chat history will be available only on the device where the chat was initiated. To enable browser-stored chat history, run:

```shell
azd env set USE_CHAT_HISTORY_BROWSER true
```

## Enabling persistent chat history with Azure Cosmos DB

[📺 Watch: (RAG Deep Dive series) Storing chat history](https://www.youtube.com/watch?v=1YiTFnnLVIA)

This feature allows authenticated users to view the chat history of their conversations, stored in the server-side storage using [Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/).This option requires that authentication be enabled. The chat history will be persistent and accessible from any device where the user logs in with the same account. To enable server-stored chat history, run:

```shell
azd env set USE_CHAT_HISTORY_COSMOS true
```

When both the browser-stored and Cosmos DB options are enabled, Cosmos DB will take precedence over browser-stored chat history.

## Enabling language picker

You can optionally enable the language picker to allow users to switch between different languages. Currently, it supports English, Spanish, French, Japanese, Danish, Dutch, Brasilian Portugese, Turkish, Italian and Polish.

To add support for additional languages, create new locale files and update `app/frontend/src/i18n/config.ts` accordingly. To enable language picker, run:

```shell
azd env set ENABLE_LANGUAGE_PICKER true
```

## Enabling speech input/output

[📺 Watch a short video of speech input/output](https://www.youtube.com/watch?v=BwiHUjlLY_U)

You can optionally enable speech input/output by setting the azd environment variables.

### Speech Input

The speech input feature uses the browser's built-in [Speech Recognition API](https://developer.mozilla.org/docs/Web/API/SpeechRecognition). It may not work in all browser/OS combinations. To enable speech input, run:

```shell
azd env set USE_SPEECH_INPUT_BROWSER true
```

### Speech Output

The speech output feature uses [Azure Speech Service](https://learn.microsoft.com/azure/ai-services/speech-service/overview) for speech-to-text. Additional costs will be incurred for using the Azure Speech Service. [See pricing](https://azure.microsoft.com/pricing/details/cognitive-services/speech-services/). To enable speech output, run:

```shell
azd env set USE_SPEECH_OUTPUT_AZURE true
```

To set [the voice](https://learn.microsoft.com/azure/ai-services/speech-service/language-support?tabs=tts) for the speech output, run:

```shell
azd env set AZURE_SPEECH_SERVICE_VOICE en-US-AndrewMultilingualNeural
```

Alternatively you can use the browser's built-in [Speech Synthesis API](https://developer.mozilla.org/docs/Web/API/SpeechSynthesis). It may not work in all browser/OS combinations. To enable speech output, run:

```shell
azd env set USE_SPEECH_OUTPUT_BROWSER true
```

## Enabling Integrated Vectorization

Azure AI search recently introduced an [integrated vectorization feature in preview mode](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/announcing-the-public-preview-of-integrated-vectorization-in-azure-ai-search/3960809). This feature is a cloud-based approach to data ingestion, which takes care of document format cracking, data extraction, chunking, vectorization, and indexing, all with Azure technologies.

To enable integrated vectorization with this sample:

1. If you've previously deployed, delete the existing search index. 🗑️
2. To enable the use of integrated vectorization, run:

    ```shell
    azd env set USE_FEATURE_INT_VECTORIZATION true
    ```

3. If you've already deployed your app, then you can run just the `provision` step:

    ```shell
    azd provision
    ```

    That will set up necessary RBAC roles and configure the integrated vectorization feature on your search service.

    If you haven't deployed your app yet, then you should run the full `azd up` after configuring all optional features.

4. You can view the resources such as the indexer and skillset in Azure Portal and monitor the status of the vectorization process.

## Enabling authentication

By default, the deployed Azure web app will have no authentication or access restrictions enabled, meaning anyone with routable network access to the web app can chat with your indexed data. If you'd like to automatically setup authentication and user login as part of the `azd up` process, see [this guide](./login_and_acl.md).

Alternatively, you can manually require authentication to your Azure Active Directory by following the [Add app authentication](https://learn.microsoft.com/azure/app-service/scenario-secure-app-authentication-app-service) tutorial and set it up against the deployed web app.

To then limit access to a specific set of users or groups, you can follow the steps from [Restrict your Microsoft Entra app to a set of users](https://learn.microsoft.com/entra/identity-platform/howto-restrict-your-app-to-a-set-of-users) by changing "Assignment Required?" option under the Enterprise Application, and then assigning users/groups access.  Users not granted explicit access will receive the error message -AADSTS50105: Your administrator has configured the application <app_name> to block users unless they are specifically granted ('assigned') access to the application.-

## Enabling login and document level access control

By default, the deployed Azure web app allows users to chat with all your indexed data. You can enable an optional login system using Azure Active Directory to restrict access to indexed data based on the logged in user. Enable the optional login and document level access control system by following [this guide](./login_and_acl.md).

## Enabling user document upload

You can enable an optional user document upload system to allow users to upload their own documents and chat with them. This feature requires you to first [enable login and document level access control](./login_and_acl.md). Then you can enable the optional user document upload system by setting an azd environment variable:

`azd env set USE_USER_UPLOAD true`

Then you'll need to run `azd up` to provision an Azure Data Lake Storage Gen2 account for storing the user-uploaded documents.
When the user uploads a document, it will be stored in a directory in that account with the same name as the user's Entra object id,
and will have ACLs associated with that directory. When the ingester runs, it will also set the `oids` of the indexed chunks to the user's Entra object id. Whenever any content is retrieved or added to the directory, the "owner" property will be checked to ensure that the user is the owner of the directory, and thus has access to the content.

If you are enabling this feature on an existing index, you should also update your index to have the new `storageUrl` field:

```shell
python ./scripts/manageacl.py  -v --acl-action enable_acls
```

And then update existing search documents with the storage URL of the main Blob container:

```shell
python ./scripts/manageacl.py  -v --acl-action update_storage_urls --url <https://YOUR-MAIN-STORAGE-ACCOUNT.blob.core.windows.net/content/>
```

Going forward, all uploaded documents will have their `storageUrl` set in the search index.
This is necessary to disambiguate user-uploaded documents from admin-uploaded documents.

## Enabling CORS for an alternate frontend

By default, the deployed Azure web app will only allow requests from the same origin.  To enable CORS for a frontend hosted on a different origin, run:

1. Run `azd env set ALLOWED_ORIGIN https://<your-domain.com>`
2. Run `azd up`

For the frontend code, change `BACKEND_URI` in `api.ts` to point at the deployed backend URL, so that all fetch requests will be sent to the deployed backend.

For an alternate frontend that's written in Web Components and deployed to Static Web Apps, check out
[azure-search-openai-javascript](https://github.com/Azure-Samples/azure-search-openai-javascript) and its guide
on [using a different backend](https://github.com/Azure-Samples/azure-search-openai-javascript#using-a-different-backend).
Both these repositories adhere to the same [HTTP protocol for AI chat apps](https://aka.ms/chatprotocol).

## Enabling query rewriting

By default, the [query rewriting feature](https://learn.microsoft.com/azure/search/semantic-how-to-query-rewrite) from the Azure AI Search service is not enabled. Note that the search service query rewriting feature is different from the query rewriting step that is used by the Chat tab in the codebase. The in-repo query rewriting step also incorporates conversation history, while the search service query rewriting feature only considers the query itself. To enable search service query rewriting, set the following environment variables:

1. Check that your Azure AI Search service is using one of the [supported regions](https://learn.microsoft.com/azure/search/semantic-how-to-query-rewrite#prerequisites) for query rewriting.
1. Ensure semantic ranker is enabled. Query rewriting may only be used with semantic ranker. Run `azd env set AZURE_SEARCH_SEMANTIC_RANKER free` or `azd env set AZURE_SEARCH_SEMANTIC_RANKER standard` depending on your desired [semantic ranker tier](https://learn.microsoft.com/azure/search/semantic-how-to-configure).
1. Enable query rewriting. Run `azd env set AZURE_SEARCH_QUERY_REWRITING true`. An option in developer settings will appear allowing you to toggle query rewriting on and off. It will be on by default.

## Adding an OpenAI load balancer

As discussed in more details in our [productionizing guide](./productionizing.md), you may want to consider implementing a load balancer between OpenAI instances if you are consistently going over the TPM limit.
Fortunately, this repository is designed for easy integration with other repositories that create load balancers for OpenAI instances. For seamless integration instructions with this sample, please check:

* [Scale Azure OpenAI for Python with Azure API Management](https://learn.microsoft.com/azure/developer/python/get-started-app-chat-scaling-with-azure-api-management)
* [Scale Azure OpenAI for Python chat using RAG with Azure Container Apps](https://learn.microsoft.com/azure/developer/python/get-started-app-chat-scaling-with-azure-container-apps)

## Deploying with private endpoints

It is possible to deploy this app with public access disabled, using Azure private endpoints and private DNS Zones. For more details, read [the private deployment guide](./deploy_private.md). That requires a multi-stage provisioning, so you will need to do more than just `azd up` after setting the environment variables.

## Using local parsers

If you want to decrease the charges by using local parsers instead of Azure Document Intelligence, you can set environment variables before running the [data ingestion script](./data_ingestion.md). Note that local parsers will generally be not as sophisticated.

1. Run `azd env set USE_LOCAL_PDF_PARSER true` to use the local PDF parser.
1. Run `azd env set USE_LOCAL_HTML_PARSER true` to use the local HTML parser.

The local parsers will be used the next time you run the data ingestion script. To use these parsers for the user document upload system, you'll need to run `azd provision` to update the web app to use the local parsers.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\deploy_freetrial.md ====

# RAG chat: Deploying with a free trial account

If you have just created an Azure free trial account and are using the free trial credits,
there are several modifications you need to make, due to restrictions on the free trial account.

Follow these instructions *before* you run `azd up`.

## Accomodate for low OpenAI quotas

The free trial accounts currently get a max of 1K TPM (tokens-per-minute), whereas our Bicep templates try to allocate 30K TPM.

To reduce the TPM allocation, run these commands:

```shell
azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_CAPACITY 1
azd env set AZURE_OPENAI_EMB_DEPLOYMENT_CAPACITY 1
```

Alternatively, if you have an OpenAI.com account, you can use that instead:

```shell
azd env set OPENAI_HOST openai
azd env set OPENAI_ORGANIZATION {Your OpenAI organization}
azd env set OPENAI_API_KEY {Your OpenAI API key}
```

## Accomodate for Azure Container Apps restrictions

By default, this project deploys to Azure Container Apps, using a remote build process that builds the Docker image in the cloud.
Unfortunately, free trial accounts cannot use that remote build process.

You have two options:

1. Comment out or delete `remoteBuild: true` in `azure.yaml`, and make sure you have Docker installed in your environment.

2. Deploy using App Service instead:

    * Comment out `host: containerapp` and uncomment `host: appservice` in the [azure.yaml](../azure.yaml) file.
    * Set the deployment target to `appservice`:

        ```shell
        azd env set DEPLOYMENT_TARGET appservice
        ```


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\deploy_lowcost.md ====

# RAG chat: Deploying with minimal costs

This AI RAG chat application is designed to be easily deployed using the Azure Developer CLI, which provisions the infrastructure according to the Bicep files in the `infra` folder. Those files describe each of the Azure resources needed, and configures their SKU (pricing tier) and other parameters. Many Azure services offer a free tier, but the infrastructure files in this project do *not* default to the free tier as there are often limitations in that tier.

However, if your goal is to minimize costs while prototyping your application, follow the steps below *before* running `azd up`. Once you've gone through these steps, return to the [deployment steps](../README.md#deploying).

[📺 Live stream: Deploying from a free account](https://www.youtube.com/watch?v=nlIyos0RXHw)

1. Log in to your Azure account using the Azure Developer CLI:

    ```shell
    azd auth login
    ```

1. Create a new azd environment for the free resource group:

    ```shell
    azd env new
    ```

    Enter a name that will be used for the resource group.
    This will create a new folder in the `.azure` folder, and set it as the active environment for any calls to `azd` going forward.

1. Switch from Azure Container Apps to the free tier of Azure App Service:

    Azure Container Apps has a consumption-based pricing model that is very low cost, but it is not free, plus Azure Container Registry costs a small amount each month.

    To deploy to App Service instead:

    * Comment out `host: containerapp` and uncomment `host: appservice` in the [azure.yaml](../azure.yaml) file.
    * Set the deployment target to `appservice`:

        ```shell
        azd env set DEPLOYMENT_TARGET appservice
        ```

    * Set the App Service SKU to the free tier:

        ```shell
        azd env set AZURE_APP_SERVICE_SKU F1
        ```

    Limitation: You are only allowed a certain number of free App Service instances per region. If you have exceeded your limit in a region, you will get an error during the provisioning stage. If that happens, you can run `azd down`, then `azd env new` to create a new environment with a new region.

1. Use the free tier of Azure AI Search:

    ```shell
    azd env set AZURE_SEARCH_SERVICE_SKU free
    ```

    Limitations:
    1. You are only allowed one free search service across all regions.
    If you have one already, either delete that service or follow instructions to
    reuse your [existing search service](../README.md#existing-azure-ai-search-resource).
    2. The free tier does not support semantic ranker, so the app UI will no longer display
    the option to use the semantic ranker. Note that will generally result in [decreased search relevance](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid-retrieval-and-ranking-ca/3929167).

1. Use the free tier of Azure Document Intelligence (used in analyzing files):

    ```shell
    azd env set AZURE_DOCUMENTINTELLIGENCE_SKU F0
    ```

    **Limitation for PDF files:**

      The free tier will only scan the first two pages of each PDF.
      In our sample documents, those first two pages are just title pages,
      so you won't be able to get answers from the documents.
      You can either use your own documents that are only 2-pages long,
      or you can use a local Python package for PDF parsing by setting:

      ```shell
      azd env set USE_LOCAL_PDF_PARSER true
      ```

    **Limitation for HTML files:**

      The free tier will only scan the first two pages of each HTML file.
      So, you might not get very accurate answers from the files.
      You can either use your own files that are only 2-pages long,
      or you can use a local Python package for HTML parsing by setting:

      ```shell
      azd env set USE_LOCAL_HTML_PARSER true
      ```

1. Use the free tier of Azure Cosmos DB:

    ```shell
    azd env set AZURE_COSMOSDB_SKU free
    ```

    Limitation: You can have only one free Cosmos DB account. To keep your account free of charge, ensure that you do not exceed the free tier limits. For more information, see the [Azure Cosmos DB lifetime free tier](https://learn.microsoft.com/azure/cosmos-db/free-tier).

1. ⚠️ This step is currently only possible if you're deploying to App Service ([see issue 2281](https://github.com/Azure-Samples/azure-search-openai-demo/issues/2281)):

    Turn off Azure Monitor (Application Insights):

    ```shell
    azd env set AZURE_USE_APPLICATION_INSIGHTS false
    ```

    Application Insights is quite inexpensive already, so turning this off may not be worth the costs saved,
    but it is an option for those who want to minimize costs.

1. Use OpenAI.com instead of Azure OpenAI: This should not be necessary, as the costs are same for both services, but you may need this step if your account does not have access to Azure OpenAI for some reason.

    ```shell
    azd env set OPENAI_HOST openai
    azd env set OPENAI_ORGANIZATION {Your OpenAI organization}
    azd env set OPENAI_API_KEY {Your OpenAI API key}
    ```

    Both Azure OpenAI and openai.com OpenAI accounts will incur costs, based on tokens used,
    but the costs are fairly low for the amount of sample data (less than $10).

1. Disable vector search:

    ```shell
    azd env set USE_VECTORS false
    ```

    By default, the application computes vector embeddings for documents during the data ingestion phase,
    and then computes a vector embedding for user questions asked in the application.
    Those computations require an embedding model, which incurs costs per tokens used. The costs are fairly low,
    so the benefits of vector search would typically outweigh the costs, but it is possible to disable vector support.
    If you do so, the application will fall back to a keyword search, which is less accurate.

1. Once you've made the desired customizations, follow the steps in the README [to run `azd up`](../README.md#deploying-from-scratch). We recommend using "eastus" as the region, for availability reasons.

## Reducing costs locally

To save costs for local development, you could use an OpenAI-compatible model.
Follow steps in [local development guide](localdev.md#using-a-local-openai-compatible-api).


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\deploy_private.md ====

<!--
---
name: RAG chat with private endpoints
description: Configure access to a chat app so that it's only accessible from private endpoints.
languages:
- python
- typescript
- bicep
- azdeveloper
products:
- azure-openai
- azure-cognitive-search
- azure-app-service
- azure
page_type: sample
urlFragment: azure-search-openai-demo-private-access
---
-->

# RAG chat: Deploying with private access

[📺 Watch: (RAG Deep Dive series) Private network deployment](https://www.youtube.com/watch?v=08wtL1eB15g)

The [azure-search-openai-demo](/) project can set up a full RAG chat app on Azure AI Search and OpenAI so that you can chat on custom data, like internal enterprise data or domain-specific knowledge sets. For full instructions on setting up the project, consult the [main README](/README.md), and then return here for detailed instructions on configuring private endpoints.

If you want to disable public access for the application so that it can only be access from a private network, follow this guide.

## Before you begin

Deploying with private networking adds additional cost to your deployment. Please see pricing for the following products:

* [Azure Container Registry](https://azure.microsoft.com/pricing/details/container-registry/): Premium tier is used when virtual network is added (required for private links), which incurs additional costs.
* [Azure Container Apps](https://azure.microsoft.com/pricing/details/container-apps/): Workload profiles environment is used when virtual network is added (required for private links), which incurs additional costs. Additionally, min replica count is set to 1, so you will be charged for at least one instance.
* [VPN Gateway](https://azure.microsoft.com/pricing/details/vpn-gateway/): VpnGw2 SKU. Pricing includes a base monthly cost plus an hourly cost based on the number of connections.
* [Virtual Network](https://azure.microsoft.com/pricing/details/virtual-network/): Pay-as-you-go tier. Costs based on data processed.

The pricing for the following features depends on the [optional features](./deploy_features.md) used. Most deployments will have at least 5 private endpoints (Azure OpenAI, Azure Cognitive Services, Azure AI Search, Azure Blob Storage, and either Azure App Service or Azure Container Apps).

* [Azure Private Endpoints](https://azure.microsoft.com/pricing/details/private-link/): Pricing is per hour per endpoint.
* [Private DNS Zones](https://azure.microsoft.com/pricing/details/dns/): Pricing is per month and zones.
* [Azure Private DNS Resolver](https://azure.microsoft.com/pricing/details/dns/): Pricing is per month and zones.

⚠️ To avoid unnecessary costs, remember to take down your app if it's no longer in use,
either by deleting the resource group in the Portal or running `azd down`.
You might also decide to delete the VPN Gateway when not in use.

## Deployment steps for private access

1. Configure the azd environment variables to use private endpoints and a VPN gateway, with public network access disabled. This will allow you to connect to the chat app from inside the virtual network, but not from the public Internet.

    ```shell
    azd env set AZURE_USE_PRIVATE_ENDPOINT true
    azd env set AZURE_USE_VPN_GATEWAY true
    azd env set AZURE_PUBLIC_NETWORK_ACCESS Disabled
    azd up
    ```

2. Provision all the Azure resources:

    ```bash
    azd provision
    ```

3. Once provisioning is complete, you will see an error when it tries to run the data ingestion script, because you are not yet connected to the VPN. That message should provide a URL for the VPN configuration file download. If you don't see that URL, run this command:

    ```bash
    azd env get-value AZURE_VPN_CONFIG_DOWNLOAD_LINK
    ```

    Open that link in your browser. Select "Download VPN client" to download a ZIP file containing the VPN configuration.

4. Open `AzureVPN/azurevpnconfig.xml`, and replace the `<clientconfig>` empty tag with the following:

    ```xml
      <clientconfig>
        <dnsservers>
          <dnsserver>10.0.11.4</dnsserver>
        </dnsservers>
      </clientconfig>
    ```

    > **Note:** We use the IP address `10.0.11.4` since it is the first available IP in the `dns-resolver-subnet`(10.0.11.0/28) from the provisioned virtual network, as Azure reserves the first four IP addresses in each subnet. Adding this DNS server allows your VPN client to resolve private DNS names for Azure services accessed through private endpoints. See the network configuration in [network-isolation.bicep](../infra/network-isolation.bicep) for details.

5. Install the [Azure VPN Client](https://learn.microsoft.com/azure/vpn-gateway/azure-vpn-client-versions).

6. Open the Azure VPN Client and select "Import" button. Select the `azurevpnconfig.xml` file you just downloaded and modified.

7. Select "Connect" and the new VPN connection. You will be prompted to select your Microsoft account and login.

8. Once you're successfully connected to VPN, you can run the data ingestion script:

    ```bash
    azd hooks run postprovision
    ```

9. Finally, you can deploy the app:

    ```bash
    azd deploy
    ```

## Environment variables controlling private access

1. `AZURE_PUBLIC_NETWORK_ACCESS`: Controls the value of public network access on supported Azure resources. Valid values are 'Enabled' or 'Disabled'.
    1. When public network access is 'Enabled', Azure resources are open to the internet.
    1. When public network access is 'Disabled', Azure resources are only accessible over a virtual network.
1. `AZURE_USE_PRIVATE_ENDPOINT`: Controls deployment of [private endpoints](https://learn.microsoft.com/azure/private-link/private-endpoint-overview) which connect Azure resources to the virtual network.
    1. When set to 'true', ensures private endpoints are deployed for connectivity even when `AZURE_PUBLIC_NETWORK_ACCESS` is 'Disabled'.
    1. Note that private endpoints do not make the chat app accessible from the internet. Connections must be initiated from inside the virtual network.
1. `AZURE_USE_VPN_GATEWAY`: Controls deployment of a VPN gateway for the virtual network. If you do not use this and public access is disabled, you will need a different way to connect to the virtual network.

## Compatibility with other features

* **GitHub Actions / Azure DevOps**: The private access deployment is not compatible with the built-in CI/CD pipelines, as it requires a VPN connection to deploy the app. You could modify the pipeline to only do provisioning, and set up a different deployment strategy for the app.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\deploy_troubleshooting.md ====

# RAG chat: Troubleshooting deployment

If you are experiencing an error when deploying the RAG chat solution using the [deployment steps](../README.md#deploying), this guide will help you troubleshoot common issues.

1. You're attempting to create resources in regions not enabled for Azure OpenAI (e.g. East US 2 instead of East US), or where the model you're trying to use isn't enabled. See [this matrix of model availability](https://aka.ms/oai/models).

1. You've exceeded a quota, most often number of resources per region. See [this article on quotas and limits](https://aka.ms/oai/quotas).

1. You're getting "same resource name not allowed" conflicts. That's likely because you've run the sample multiple times and deleted the resources you've been creating each time, but are forgetting to purge them. Azure keeps resources for 48 hours unless you purge from soft delete. See [this article on purging resources](https://learn.microsoft.com/azure/cognitive-services/manage-resources?tabs=azure-portal#purge-a-deleted-resource).

1. You see `CERTIFICATE_VERIFY_FAILED` when the `prepdocs.py` script runs. That's typically due to incorrect SSL certificates setup on your machine. Try the suggestions in this [StackOverflow answer](https://stackoverflow.com/a/43855394).

1. After running `azd up` and visiting the website, you see a '404 Not Found' in the browser. Wait 10 minutes and try again, as it might be still starting up. Then try running `azd deploy` and wait again. If you still encounter errors with the deployed app and are deploying to App Service, consult the [guide on debugging App Service deployments](/docs/appservice.md). Please file an issue if the logs don't help you resolve the error.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\evaluation.md ====

# Evaluating the RAG answer quality

[📺 Watch: (RAG Deep Dive series) Evaluating RAG answer quality](https://www.youtube.com/watch?v=lyCLu53fb3g)

Follow these steps to evaluate the quality of the answers generated by the RAG flow.

* [Deploy an evaluation model](#deploy-an-evaluation-model)
* [Setup the evaluation environment](#setup-the-evaluation-environment)
* [Generate ground truth data](#generate-ground-truth-data)
* [Run bulk evaluation](#run-bulk-evaluation)
* [Review the evaluation results](#review-the-evaluation-results)
* [Run bulk evaluation on a PR](#run-bulk-evaluation-on-a-pr)

## Deploy an evaluation model

1. Run this command to tell `azd` to deploy a GPT-4 level model for evaluation:

    ```shell
    azd env set USE_EVAL true
    ```

2. Set the capacity to the highest possible value to ensure that the evaluation runs relatively quickly. Even with a high capacity, it can take a long time to generate ground truth data and run bulk evaluations.

    ```shell
    azd env set AZURE_OPENAI_EVAL_DEPLOYMENT_CAPACITY 100
    ```

    By default, that will provision a `gpt-4o` model, version `2024-08-06`. To change those settings, set the azd environment variables `AZURE_OPENAI_EVAL_MODEL` and `AZURE_OPENAI_EVAL_MODEL_VERSION` to the desired values.

3. Then, run the following command to provision the model:

    ```shell
    azd provision
    ```

## Setup the evaluation environment

Make a new Python virtual environment and activate it. This is currently required due to incompatibilities between the dependencies of the evaluation script and the main project.

```bash
python -m venv .evalenv
```

```bash
source .evalenv/bin/activate
```

Install all the dependencies for the evaluation script by running the following command:

```bash
pip install -r evals/requirements.txt
```

## Generate ground truth data

Generate ground truth data by running the following command:

```bash
python evals/generate_ground_truth.py --numquestions=200 --numsearchdocs=1000
```

The options are:

* `numquestions`: The number of questions to generate. We suggest at least 200.
* `numsearchdocs`: The number of documents (chunks) to retrieve from your search index. You can leave off the option to fetch all documents, but that will significantly increase time it takes to generate ground truth data. You may want to at least start with a subset.
* `kgfile`: An existing RAGAS knowledge base JSON file, which is usually `ground_truth_kg.json`. You may want to specify this if you already created a knowledge base and just want to tweak the question generation steps.
* `groundtruthfile`: The file to write the generated ground truth answwers. By default, this is `evals/ground_truth.jsonl`.

🕰️ This may take a long time, possibly several hours, depending on the size of the search index.

Review the generated data in `evals/ground_truth.jsonl` after running that script, removing any question/answer pairs that don't seem like realistic user input.

## Run bulk evaluation

Review the configuration in `evals/evaluate_config.json` to ensure that everything is correctly setup. You may want to adjust the metrics used. See [the ai-rag-chat-evaluator README](https://github.com/Azure-Samples/ai-rag-chat-evaluator) for more information on the available metrics.

By default, the evaluation script will evaluate every question in the ground truth data.
Run the evaluation script by running the following command:

```bash
python evals/evaluate.py
```

The options are:

* `numquestions`: The number of questions to evaluate. By default, this is all questions in the ground truth data.
* `resultsdir`: The directory to write the evaluation results. By default, this is a timestamped folder in `evals/results`. This option can also be specified in `evaluate_config.json`.
* `targeturl`: The URL of the running application to evaluate. By default, this is `http://localhost:50505`. This option can also be specified in `evaluate_config.json`.

🕰️ This may take a long time, possibly several hours, depending on the number of ground truth questions, the TPM capacity of the evaluation model, and the number of LLM-based metrics requested.

## Review the evaluation results

The evaluation script will output a summary of the evaluation results, inside the `evals/results` directory.

You can see a summary of results across all evaluation runs by running the following command:

```bash
python -m evaltools summary evals/results
```

Compare answers to the ground truth by running the following command:

```bash
python -m evaltools diff evals/results/baseline/
```

Compare answers across two runs by running the following command:

```bash
python -m evaltools diff evals/results/baseline/ evals/results/SECONDRUNHERE
```

## Run bulk evaluation on a PR

This repository includes a GitHub Action workflow `evaluate.yaml` that can be used to run the evaluation on the changes in a PR.

In order for the workflow to run successfully, you must first set up [continuous integration](./azd.md#github-actions) for the repository.

To run the evaluation on the changes in a PR, a repository member can post a `/evaluate` comment to the PR. This will trigger the evaluation workflow to run the evaluation on the PR changes and will post the results to the PR.

## Evaluate multimodal RAG answers

The repository also includes an `evaluate_config_multimodal.json` file specifically for evaluating multimodal RAG answers. This configuration uses a different ground truth file, `ground_truth_multimodal.jsonl`, which includes questions based off the sample data that require both text and image sources to answer.

Note that the "groundedness" evaluator is not reliable for multimodal RAG, since it does not currently incorporate the image sources. We still include it in the metrics, but the more reliable metrics are "relevance" and "citations matched".


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\http_protocol.md ====

# RAG Chat: HTTP Protocol

The frontend and backend of this RAG chat application exchange messages over HTTP, using both regular JSON for single responses and streaming newline-delimited JSON (NDJSON) for streamed responses.

The HTTP protocol is inspired by the [OpenAI ChatCompletion API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api), but contains additional fields required for the chat application.

Table of contents:

* [HTTP requests to chat app endpoints](#http-requests-to-chat-app-endpoints)
  * [Request context properties](#request-context-properties)
* [HTTP responses from RAG chat app endpoints](#http-responses-from-rag-chat-app-endpoints)
  * [Non-streaming response](#non-streaming-response)
    * [Successful response](#successful-response)
    * [Error response](#error-response)
  * [Streaming response](#streaming-response)
    * [Successful streamed response](#successful-streamed-response)
    * [Error in streamed response](#error-in-streamed-response)
  * [Answer formatting](#answer-formatting)
  * [Response context properties](#response-context-properties)

## HTTP requests to chat app endpoints

All requests use the POST method, with the following headers:

* `Content-Type: application/json`
* `Authorization: Bearer <ID token>`: _Optional._ For authentication, if the app is deployed with [user login enabled](./login_and_acl.md)

The path is `chat` for a non-streaming request and `chat/stream` for a streaming request.

The body of the request contains these properties, in JSON format:

* `"messages"`: A list of messages, each containing "content" and "role", where "role" may be "assistant" or "user". When triggered from the "Ask" tab (single-turn RAG), the list will contain a single message, whereas requests from the "Chat" tab (multi-turn RAG) may contain multiple messages.
* `"session_state"`: _Optional_. An object containing the "memory" for the chat app, such as the session ID for chat history storage.
* `"context"`: _Optional_. An object containing any additional options for the request, such as the `temperature` to use for the LLM. See below for supported options.

### Usage example

The example belows represents a valid and compliant request body to the chat app endpoints:

```json
{
    "messages": [
        {
            "content": "What is included in my Northwind Health Plus plan that is not in standard?",
            "role": "user"
        }
    ],
    "context": {},
    "session_state": null
}
```

### Request context properties

These are the currently supported properties in the `context` object:

* `"overrides"`: An object containing settings for the chat application.
  * `"temperature"`: The temperature to use for the LLM for the question-answering chat completion call.
  * `"top"`: The number of results to return from Azure AI Search.
  * `"retrieval_mode"`: The mode to use for the Azure AI Search step. Can be "hybrid", "vectors", or "text".
  * `"semantic_ranker"`: Whether to use the semantic ranker for the Azure AI Search step.
  * `"semantic_captions"`: Whether to use semantic captions for the Azure AI Search step.
  * `"suggest_followup_questions"`: Whether to suggest follow-up questions for the chat app.
  * `"use_oid_security_filter"`: Whether to use the OID security filter for the Azure AI Search step.
  * `"use_groups_security_filter"`: Whether to use the groups security filter for the Azure AI Search step.
  * `"vector_fields"`: Which embedding fields to use for the Azure AI Search step. This is either `textEmbeddingOnly`, `imageEmbeddingOnly`, or `textAndImageEmbeddings`. The default is `textEmbeddingOnly`, but if you have multimodal embeddings enabled, it defaults to `textAndImageEmbeddings`.
  * `"use_multimodal_answering"`: Whether to send both text and images to the LLM for answering questions.

Example of the overrides object:

```json
"overrides": {
    "top": 3,
    "retrieval_mode": "text",
    "semantic_ranker": false,
    "semantic_captions": false,
    "suggest_followup_questions": false,
    "use_oid_security_filter": false,
    "use_groups_security_filter": false,
    "vector_fields": "textEmbeddingOnly",
    "use_multimodal_answering": false,
}
```

## HTTP responses from RAG chat app endpoints

The HTTP response is JSON for a non-streaming response, or [newline-delimited JSON](https://jsonlines.org/) ("NDJSON"/"jsonlines") for a streaming response.

### Non-streaming response

The response contains this header:

* `Content-Type: application/json`

#### Successful response

A successful response has a status code of 200, and the body contains a JSON object with the following properties:

* `"message"`: An object containing the actual content of the response.  See [Answer formatting](#answer-formatting). _Comes from the [OpenAI chat completion object](https://platform.openai.com/docs/api-reference/chat/object)._
* `"session_state"`: _Optional_. An object containing the "memory" for the chat app, such as the session ID for chat history storage.
* `"context"`: _Optional_. An object containing additional details needed for the chat app, used for citation display and the thought process tab. See [response context properties](#response-context-properties).

Here's an example JSON response:

```json
{
    "message": {
        "content": "There is no specific information provided about what is included in the Northwind Health Plus plan that is not in the standard plan. It is recommended to read the plan details carefully and ask questions to understand the specific benefits of the Northwind Health Plus plan [Northwind_Standard_Benefits_Details.pdf#page=91].",
        "function_call": null,
        "role": "assistant",
        "tool_calls": null
    },
    "context": {
        "data_points": {
            "text": [
                "Northwind_Standard_Benefits_Details.pdf#page=91:    Tips for Avoiding Intentionally False Or Misleading Statements:   When it comes to understanding a health plan, it is important to be aware of any  intentiona lly false or misleading statements that the plan provider may make...(truncated)",
                "Northwind_Standard_Benefits_Details.pdf#page=91:  It is important to  research the providers and services offered in the Northwind Standard plan in order to  determine if the providers and services offered are sufficient for the employee's needs...(truncated)",
                "Northwind_Standard_Benefits_Details.pdf#page=17:  Employees should keep track of their claims and follow up with  Northwind Health if a claim is not processed in a timely manner...(truncated)"
            ]
        },
        "thoughts": [
            {
                "description": "What is included in my Northwind Health Plus plan that is not in standard?",
                "props": null,
                "title": "Original user query"
            },
            {
                "description": "Northwind Health Plus plan coverage details compared to standard plan",
                "props": {
                    "has_vector": false,
                    "use_semantic_captions": false
                },
                "title": "Generated search query"
            },
            {
                "description": [
                    {
                        "captions": [],
                        "category": null,
                        "content": "  \nTips for Avoiding Intentionally False Or Misleading Statements:  \nWhen it comes to understanding a health plan, it is important to be aware of any \nintentiona lly false or misleading statements that the plan provider may make...(truncated)",
                        "embedding": null,
                        "groups": [],
                        "id": "file-Northwind_Standard_Benefits_Details_pdf-4E6F72746877696E645F5374616E646172645F42656E65666974735F44657461696C732E706466-page-233",
                        "imageEmbedding": null,
                        "oids": [],
                        "sourcefile": "Northwind_Standard_Benefits_Details.pdf",
                        "sourcepage": "Northwind_Standard_Benefits_Details.pdf#page=91"
                    },
                    {
                        "captions": [],
                        "category": null,
                        "content": " It is important to \nresearch the providers and services offered in the Northwind Standard plan i n order to \ndetermine if the providers and services offered are sufficient for the employee's needs...(truncated)",
                        "embedding": null,
                        "groups": [],
                        "id": "file-Northwind_Standard_Benefits_Details_pdf-4E6F72746877696E645F5374616E646172645F42656E65666974735F44657461696C732E706466-page-232",
                        "imageEmbedding": null,
                        "oids": [],
                        "sourcefile": "Northwind_Standard_Benefits_Details.pdf",
                        "sourcepage": "Northwind_Standard_Benefits_Details.pdf#page=91"
                    },
                    {
                        "captions": [],
                        "category": null,
                        "content": " Employees should keep track of their claims and follow up with \nNorthwind Health if a claim is not processed in a timely manner...(truncated)",
                        "embedding": null,
                        "groups": [],
                        "id": "file-Northwind_Standard_Benefits_Details_pdf-4E6F72746877696E645F5374616E646172645F42656E65666974735F44657461696C732E706466-page-41",
                        "imageEmbedding": null,
                        "oids": [],
                        "sourcefile": "Northwind_Standard_Benefits_Details.pdf",
                        "sourcepage": "Northwind_Standard_Benefits_Details.pdf#page=17"
                    }
                ],
                "props": null,
                "title": "Results"
            },
            {
                "description": [
                    "{'role': 'system', 'content': \"Assistant helps the company employees with their healthcare plan questions, and questions about the employee handbook. Be brief in your answers.\n        Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\n        For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\n        Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, for example [info1.txt]. Don't combine sources, list each source separately, for example [info1.txt][info2.pdf].\n        \n        \n        \"}",
                    "{'role': 'user', 'content': \"What is included in my Northwind Health Plus plan that is not in standard?\n\nSources:\nNorthwind_Standard_Benefits_Details.pdf#page=91:    Tips for Avoiding Intentionally False Or Misleading Statements:   When it comes to understanding a health plan, it is important to be aware of any  intentiona lly false or misleading statements that the plan provider may make. To avoid  being misled, employees should follow the following tips:(truncated)
                    \nNorthwind_Standard_Benefits_Details.pdf#page=91:  It is important to  research the providers and services offered in the Northwind Standard plan in order to  determine if the providers and services offered are sufficient for the employee's needs.   In addition, Northwind Health may make claims that their plan offers low or no cost  prescription drugs..(truncated)\"}"
                ],
                "props": null,
                "title": "Prompt"
            }
        ]
    },
    "session_state": null
}
```

#### Error response

An error response has a status code of 400 or 500, and the body contains a JSON object with the following properties:

* `"error"`: A string describing the error.

Here's an example JSON response for a 500-level error:

```json
{
    "error": "The app encountered an error processing your request.\nIf you are an administrator of the app, view the full error in the logs."
}
```

Here's an example JSON response for a 400-level error:

```json
{
    "error": "Your message contains content that was flagged by the OpenAI content filter."
}
```

### Streaming response

The response contains these headers:

* `Content-Type: application/json-lines`
* `Transfer-Encoding: chunked`

#### Successful streamed response

A successful response has a status code of 200.
The body of the response contains a sequence of JSON objects, each representing a chunk of the response.
The first chunk contains the `context` property, since that is available before the answer, and subsequent chunks contain parts of the answer to the question.

Each JSON object contains the following properties:

* `"delta"`: An object containing the actual content of the response, a token at a time. See [Answer formatting](#answer-formatting). _Comes from the [OpenAI chat completion chunk object](https://platform.openai.com/docs/api-reference/chat/streaming)._
* `"context"`: _Optional_. An object containing additional details needed for the chat app. Each application can define its own properties. See [response context properties](#response-context-properties).
* `"session_state"`: _Optional_. An object containing the "memory" for the chat app, such as a user ID.

Here's an example of the first three JSON objects in a streaming response:

```json
{
    "delta": {
        "role": "assistant"
    },
    "context": {
        "data_points": {
            "text": [
                "Benefit_Options.pdf#page=3:  The plans also cover preventive care services such as mammograms, colonoscopies, and  other cancer screenings...(truncated)",
                "Benefit_Options.pdf#page=3:   Both plans offer coverage for medical services. Northwind Health Plus offers coverage for hospital stays,  doctor visits,...(truncated)",
                "Benefit_Options.pdf#page=3:  With Northwind Health Plus, you can choose  from a variety of in -network providers, including primary care physicians,...(truncated)"
            ]
        },
        "thoughts": [
            {
                "title": "Original user query",
                "description": "What is included in my Northwind Health Plus plan that is not in standard?",
                "props": null
            },
            {
                "title": "Generated search query",
                "description": "Northwind Health Plus plan standard",
                "props": {
                    "use_semantic_captions": false,
                    "has_vector": false
                }
            },
            {
                "title": "Results",
                "description": [
                    {
                        "id": "file-Benefit_Options_pdf-42656E656669745F4F7074696F6E732E706466-page-2",
                        "content": " The plans also cover preventive care services such as mammograms, colonoscopies, and \nother cancer screenings...(truncated)",
                        "embedding": null,
                        "imageEmbedding": null,
                        "category": null,
                        "sourcepage": "Benefit_Options.pdf#page=3",
                        "sourcefile": "Benefit_Options.pdf",
                        "oids": [],
                        "groups": [],
                        "captions": []
                    },
                    {
                        "id": "file-Benefit_Options_pdf-42656E656669745F4F7074696F6E732E706466-page-3",
                        "content": " \nBoth plans offer coverage for medical services. Northwind Health Plus offers coverage for hospital stays, \ndoctor visits,...(truncated)",
                        "embedding": null,
                        "imageEmbedding": null,
                        "category": null,
                        "sourcepage": "Benefit_Options.pdf#page=3",
                        "sourcefile": "Benefit_Options.pdf",
                        "oids": [],
                        "groups": [],
                        "captions": []
                    },
                    {
                        "id": "file-Benefit_Options_pdf-42656E656669745F4F7074696F6E732E706466-page-1",
                        "content": " With Northwind Health Plus, you can choose \nfrom a variety of in -network providers, including primary care physicians,...(truncated)",
                        "embedding": null,
                        "imageEmbedding": null,
                        "category": null,
                        "sourcepage": "Benefit_Options.pdf#page=3",
                        "sourcefile": "Benefit_Options.pdf",
                        "oids": [],
                        "groups": [],
                        "captions": []
                    }
                ],
                "props": null
            },
            {
                "title": "Prompt",
                "description": [
                    "{'role': 'system', 'content': \"Assistant helps the company employees with their healthcare plan questions, and questions about the employee handbook. Be brief in your answers.\\n        Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know. Do not generate answers that don't use the sources below. If asking a clarifying question to the user would help, ask the question.\\n        For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\\n        Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, for example [info1.txt]. Don't combine sources, list each source separately, for example [info1.txt][info2.pdf].\\n        \\n        \\n        \"}",
                    "{'role': 'user', 'content': 'What is included in my Northwind Health Plus plan that is not in standard?'}",
                    "{'role': 'assistant', 'content': 'There is no specific information provided about what is included in the Northwind Health Plus plan that is not in the standard plan. It is recommended to read the plan details carefully and ask questions to understand the specific benefits of the Northwind Health Plus plan [Northwind_Standard_Benefits_Details.pdf#page=91].'}",
                    "{'role': 'user', 'content': \"What is included in my Northwind Health Plus plan that is not in standard?\\n\\nSources:\\nBenefit_Options.pdf#page=3:  The plans also cover preventive care services such as mammograms, colonoscopies, and  other cancer screenings...(truncated)\\nBenefit_Options.pdf#page=3:   Both plans offer coverage for medical services. Northwind Health Plus offers coverage for hospital stays,  doctor visits,...(truncated)\\nBenefit_Options.pdf#page=3:  With Northwind Health Plus, you can choose  from a variety of in -network providers, including primary care physicians,...(truncated)\"}"
                ],
                "props": null
            }
        ]
    },
    "session_state": null,
}{
    "delta": {
        "content": null,
        "function_call": null,
        "role": "assistant",
        "tool_calls": null
    }
}{
    "delta": {
        "content": "The",
        "function_call": null,
        "role": null,
        "tool_calls": null
    }
}
```

#### Error in streamed response

If an error is encountered before the stream begins, then the response may look like a non-streaming error response. However, if an error is encountered during the stream, then the server will have already sent a 200 response, and will send a chunk with an error object. Typically that would be the last chunk, but it may not be.

Here's an example of an error chunk:

```json
{
    "error": "The app encountered an error processing your request.\nIf you are an administrator of the app, view the full error in the logs."
}
```

### Answer formatting

To support the display of citations, the answer from the LLM should contain source information in square brackets, such as `[info1.txt]`.

Here's a full example of an answer with citation:

```text
There is no specific information provided about what is included in the Northwind Health Plus plan that is not in the standard plan. It is recommended to read the plan details carefully and ask questions to understand the specific benefits of the Northwind Health Plus plan [Northwind_Standard_Benefits_Details.pdf#page=91].
```

### Response context properties

The response context object can contain the following properties:

* `"followup_questions"`: A list of follow-up questions to ask the user.

    Example:

    ```json
    "followup_questions": [
        "What types of prescription drugs are covered?",
        "Which services have lower out-of-pocket costs?"
    ]
    ```

    When the app sees this property in the response and the user has requested follow-up questions (in the settings), the app prompts the user with clickable versions of the questions. [See image](./images/followup.png)

* `"data_points"`: An object containing text and/or image data chunks, a list in the `"text"` or `"images"` properties.

    Example:

    ```json
    "data_points": {
        "text": [
            "Northwind_Standard_Benefits_Details.pdf#page=91:    Tips for Avoiding Intentionally False Or Misleading Statements:   When it comes to understanding a health plan, it is important to be aware of any intentionally false or misleading statements that the plan provider may make...(truncated)",
            "Northwind_Standard_Benefits_Details.pdf#page=91:  It is important to research the providers and services offered in the Northwind Standard plan in order to  determine if the providers and services offered are sufficient for the employee's needs...(truncated)",
            "Northwind_Standard_Benefits_Details.pdf#page=17:  Employees should keep track of their claims and follow up with  Northwind Health if a claim is not processed in a timely manner...(truncated)"
        ]
    },
    ```

    Example with images:

    ```json
    "data_points": {
        "images": [
            {
                "detail": "auto",
                "url": "data:image/png;base64,iVBOR1BORw0KGgoAAAANSUhEUgAAAAEAAAABAQAAAAA3bvkkAAAACklEQVR4nGMAAQAABQABDQ0tuhsAAAAASUVORK5CYII="
            }
        ],
        "text": [
            "Financial Market Analysis Report 2023-6.png: 3</td><td>1</td></tr></table> Financial markets are interconnected, with movements in one segment often influencing other...(truncated)"
        ]
    },
    ```

    The app turns the data points into clickable citations and the "Supporting content" tab. [See image](./images/data_points.png)

* `"thoughts"`: A list describing each step of the backend. Each step should contain:
  * `"title"`: A string describing the step.
  * `"description"`: A string or list of strings describing the step.
  * `"props"`: _Optional_. An object containing additional properties for the step.

    Example:

    ```json
    "thoughts": [
        {
            "title": "Original user query",
            "description": "What is included in my Northwind Health Plus plan that is not in standard?",
            "props": null
        },
        {
            "title": "Generated search query",
            "description": "Northwind Health Plus plan coverage details",
            "props": {
                "has_vector": false,
                "use_semantic_captions": false
            }
        },
        {
            "title": "Results",
            "description": [
                {
                    "captions": [],
                    "category": null,
                    "content": "  \n\u2022 Understand your coverage limits, and know what services are  covered and what services \nare not covered...(truncated)",
                    "embedding": null,
                    "groups": [],
                    "id": "file-Northwind_Health_Plus_Benefits_Details_pdf-4E6F72746877696E645F4865616C74685F506C75735F42656E65666974735F44657461696C732E706466-page-249",
                    "imageEmbedding": null,
                    "oids": [],
                    "sourcefile": "Northwind_Health_Plus_Benefits_Details.pdf",
                    "sourcepage": "Northwind_Health_Plus_Benefits_Details.pdf#page=100"
                },
                {
                    "captions": [],
                    "category": null,
                    "content": " Employees should keep track of their claims and follow up with \nNorthwind Health if a claim is not processed in a timely manner...(truncated)",
                    "embedding": null,
                    "groups": [],
                    "id": "file-Northwind_Standard_Benefits_Details_pdf-4E6F72746877696E645F5374616E646172645F42656E65666974735F44657461696C732E706466-page-41",
                    "imageEmbedding": null,
                    "oids": [],
                    "sourcefile": "Northwind_Standard_Benefits_Details.pdf",
                    "sourcepage": "Northwind_Standard_Benefits_Details.pdf#page=17"
                },
                {
                    "captions": [],
                    "category": null,
                    "content": " It is important to talk to your doctor or \nhealth care provider to make su re that you understand the details of the clinical trial before \nyou decide to participate...(truncated)",
                    "embedding": null,
                    "groups": [],
                    "id": "file-Northwind_Health_Plus_Benefits_Details_pdf-4E6F72746877696E645F4865616C74685F506C75735F42656E65666974735F44657461696C732E706466-page-57",
                    "imageEmbedding": null,
                    "oids": [],
                    "sourcefile": "Northwind_Health_Plus_Benefits_Details.pdf",
                    "sourcepage": "Northwind_Health_Plus_Benefits_Details.pdf#page=24"
                }
            ],
            "props": null
        },
        {
            "title": "Prompt",
            "description": [
                "{'role': 'system', 'content': 'Assistant helps the company employees with their healthcare plan questions, and questions about the employee handbook. Be brief in your answers.\\n        Answer ONLY with the facts listed in the list of sources below. If there isn\\'t enough information below, say you don\\'t know. Do not generate answers that don\\'t use the sources below. If asking a clarifying question to the user would help, ask the question.\\n        For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\\n        Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, for example [info1.txt]. Don\\'t combine sources, list each source separately, for example [info1.txt][info2.pdf].\\n        Generate 3 very brief follow-up questions that the user would likely ask next.\\n    Enclose the follow-up questions in double angle brackets. Example:\\n    <<Are there exclusions for prescriptions?>>\\n    <<Which pharmacies can be ordered from?>>\\n    <<What is the limit for over-the-counter medication?>>\\n    Do no repeat questions that have already been asked.\\n    Make sure the last question ends with \">>\".\\n    \\n        \\n        '}",
                "{'role': 'user', 'content': 'What is included in my Northwind Health Plus plan that is not in standard?'}",
                "{'role': 'assistant', 'content': 'The Northwind Health Plus plan includes coverage for prescription drugs, but it is important to read the plan details to determine which prescription drugs are covered and what the associated costs are [Northwind_Standard_Benefits_Details.pdf#page=91]. Additionally, employees should select in-network providers to maximize coverage and avoid unexpected costs, submit claims as soon as possible after a service is rendered, and track claims and follow up with Northwind Health if a claim is not processed in a timely manner [Northwind_Standard_Benefits_Details.pdf#page=17].\\n\\n'}",
                "{'role': 'user', 'content': 'What is included in my Northwind Health Plus plan that is not in standard?\\n\\nSources:\\nNorthwind_Health_Plus_Benefits_Details.pdf#page=100:    \u2022 Understand your coverage limits, and know what services are  covered and what services  are not covered...(truncated)\\nNorthwind_Standard_Benefits_Details.pdf#page=17:  Employees should keep track of their claims and follow up with  Northwind Health if a claim is not processed in a timely manner...(truncated)\\nNorthwind_Health_Plus_Benefits_Details.pdf#page=24:  It is important to talk to your doctor or  health care provider..(truncated)'}"
            ],
            "props": null
        }
    ]
    ```

    The app displays these thoughts in the "Thought process" tab, available by selecting the lightbulb icon on each answer. [See image](./images/thoughts.png)


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\localdev.md ====

# RAG chat: Local development of chat app

After deploying the app to Azure, you may want to continue development locally. This guide explains how to run the app locally, including hot reloading and debugging.

* [Running development server from the command line](#running-development-server-from-the-command-line)
* [Hot reloading frontend and backend files](#hot-reloading-frontend-and-backend-files)
* [Using VS Code "Development" task](#using-vs-code-development-task)
* [Using Copilot Chat Debug Mode](#using-copilot-chat-debug-mode)
* [Using VS Code "Run and Debug"](#using-vs-code-run-and-debug)
* [Using a local OpenAI-compatible API](#using-a-local-openai-compatible-api)
  * [Using Ollama server](#using-ollama-server)
  * [Using llamafile server](#using-llamafile-server)

## Running development server from the command line

You can only run locally **after** having successfully run the `azd up` command. If you haven't yet, follow the steps in [Azure deployment](../README.md#azure-deployment) above.

1. Run `azd auth login`
2. Start the server:

  Windows:

  ```shell
  ./app/start.ps1
  ```

  Linux/Mac:

  ```shell
  ./app/start.sh
  ```

  VS Code: Run the "VS Code Task: Start App" task.

## Hot reloading frontend and backend files

When you run `./start.ps1` or `./start.sh`, the backend files will be watched and reloaded automatically. However, the frontend files will not be watched and reloaded automatically.

To enable hot reloading of frontend files, open a new terminal and navigate to the frontend directory:

```shell
cd app/frontend
```

Then run:

```shell
npm run dev
```

You should see:

```shell
> frontend@0.0.0 dev
> vite


  VITE v4.5.1  ready in 957 ms

  ➜  Local:   http://localhost:5173/
  ➜  Network: use --host to expose
  ➜  press h to show help
```

Navigate to the URL shown in the terminal (in this case, `http://localhost:5173/`).  This local server will watch and reload frontend files. All backend requests will be routed to the Python server according to `vite.config.ts`.

Then, whenever you make changes to frontend files, the changes will be automatically reloaded, without any browser refresh needed.

Alternatively, you can start both servers with hot reloading by using the VS Code "Development" task. See [Using VS Code "Development" task](#using-vs-code-development-task).

## Using VS Code "Development" task

If you prefer VS Code tasks for hot reloading both servers at once, use the "Development" task defined in `.vscode/tasks.json`.

How to run it:

* Run Build Task (Shift+Cmd+B) to start the default build task, which is "Development".
* Or open the Command Palette (Shift+Cmd+P) and run: "Tasks: Run Task" -> "Development".

What it does:

* Starts two background tasks in dedicated panels:
  * "Frontend: npm run dev" from `app/frontend` (Vite HMR for instant frontend updates)
  * "Backend: quart run" from `app/backend` (Quart with `--reload` for backend auto-restarts)

Readiness indicators:

* Frontend is ready when Vite prints a Local URL, for example: `Local: http://localhost:5173/`.
* Backend is ready when Hypercorn reports: `Running on http://127.0.0.1:50505` (port may vary).

Tips:

* To stop both, run: "Tasks: Terminate Task" and pick the running tasks.
* If watchers stall, terminate and run "Development" again.
* Frontend changes apply via HMR; backend Python changes auto-reload. No manual restart needed.

## Using Copilot Chat Debug Mode

You can use GitHub Copilot Chat with a custom "debug" mode to streamline troubleshooting in this repo.

Prerequisites:

* VS Code 1.101+ (custom chat modes are in preview)
* Access to GitHub Copilot and Copilot Chat
* Playwright MCP server and GitHub MCP server (optional)

To learn more about the chat modes feature, read [VS Code docs for Chat modes](https://code.visualstudio.com/docs/copilot/chat/chat-modes).

To use the debug mode:

* Open the Chat view.
* Use the chat mode dropdown at the top of the Chat view to select the "debug" mode.
* Start chatting in that mode; the instructions and tools from the repo file will be applied automatically.
* The mode will use the tasks from .vscode/tasks.json to run the frontend and backend server, and should be able to read any errors in the output.
* The mode may also use tools from the Playwright MCP server and GitHub MCP server, if those servers are installed in your VS Code.

Notably, this mode will not actually use a breakpoint-based debugger. Read on to learn how to use breakpoints while debugging the Python code.

## Using VS Code "Run and Debug"

This project includes configurations defined in `.vscode/launch.json` that allow you to run and debug the app directly from VS Code:

* "Backend (Python)": Starts the Python backend server, defaulting to port 50505.
* "Frontend": Starts the frontend server using Vite, typically at port 5173.
* "Frontend & Backend": A compound configuration that starts both the frontend and backend servers.

When you run these configurations, you can set breakpoints in your code and debug as you would in a normal VS Code debugging session.

## Using a local OpenAI-compatible API

You may want to save costs by developing against a local LLM server, such as
[llamafile](https://github.com/Mozilla-Ocho/llamafile/). Note that a local LLM
will generally be slower and not as sophisticated.

Once the local LLM server is running and serving an OpenAI-compatible endpoint, set these environment variables:

```shell
azd env set USE_VECTORS false
azd env set OPENAI_HOST local
azd env set OPENAI_BASE_URL <your local endpoint>
azd env set AZURE_OPENAI_CHATGPT_MODEL local-model-name
```

Then restart the local development server.
You should now be able to use the "Ask" tab.

⚠️ Limitations:

* The "Chat" tab will only work if the local language model supports function calling.
* Your search mode must be text only (no vectors), since the search index is only populated with OpenAI-generated embeddings, and the local OpenAI host can't generate those.
* The conversation history will be truncated using the GPT tokenizers, which may not be the same as the local model's tokenizer, so if you have a long conversation, you may end up with token limit errors.

> [!NOTE]
> You must set `OPENAI_HOST` back to a non-local value ("azure", "azure_custom", or "openai")
> before running `azd up` or `azd provision`, since the deployed backend can't access your local server.

### Using Ollama server

For example, to point at a local Ollama server running the `llama3.1:8b` model:

```shell
azd env set OPENAI_HOST local
azd env set OPENAI_BASE_URL http://localhost:11434/v1
azd env set AZURE_OPENAI_CHATGPT_MODEL llama3.1:8b
azd env set USE_VECTORS false
```

If you're running the app inside a VS Code Dev Container, use this local URL instead:

```shell
azd env set OPENAI_BASE_URL http://host.docker.internal:11434/v1
```

### Using llamafile server

To point at a local llamafile server running on its default port:

```shell
azd env set OPENAI_HOST local
azd env set OPENAI_BASE_URL http://localhost:8080/v1
azd env set USE_VECTORS false
```

Llamafile does *not* require a model name to be specified.

If you're running the app inside a VS Code Dev Container, use this local URL instead:

```shell
azd env set OPENAI_BASE_URL http://host.docker.internal:8080/v1
```


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\login_and_acl.md ====

<!--
---
name: RAG chat with document security
description: This guide demonstrates how to add an optional login and document level access control system to a RAG chat app for your domain data. This system can be used to restrict access to indexed data to specific users.
languages:
- python
- typescript
- bicep
- azdeveloper
products:
- azure-openai
- azure-cognitive-search
- azure-app-service
- azure
page_type: sample
urlFragment: azure-search-openai-demo-document-security
---
-->

# RAG chat: Setting up optional login and document level access control

[📺 Watch: (RAG Deep Dive series) Login and access control](https://www.youtube.com/watch?v=GwEiYJgM8Vw)

The [azure-search-openai-demo](/) project can set up a full RAG chat app on Azure AI Search and OpenAI so that you can chat on custom data, like internal enterprise data or domain-specific knowledge sets. For full instructions on setting up the project, consult the [main README](/README.md), and then return here for detailed instructions on configuring login and access control.

## Table of Contents

- [Requirements](#requirements)
- [Setting up Microsoft Entra applications](#setting-up-microsoft-entra-applications)
  - [Automatic Setup](#automatic-setup)
  - [Manual Setup](#manual-setup)
    - [Server App](#server-app)
    - [Client App](#client-app)
    - [Configure Server App Known Client Applications](#configure-server-app-known-client-applications)
    - [Testing](#testing)
    - [Programmatic Access With Authentication](#programmatic-access-with-authentication)
  - [Troubleshooting](#troubleshooting)
- [Adding data with document level access control](#adding-data-with-document-level-access-control)
  - [Using the Add Documents API](#using-the-add-documents-api)
    - [Enabling global access on documents without access control](#enabling-global-access-on-documents-without-access-control)
  - [Azure Data Lake Storage Gen2 and prepdocs](#azure-data-lake-storage-gen2-setup)
- [Migrate to built-in document access control](#migrate-to-built-in-document-access-control)
- [Environment variables reference](#environment-variables-reference)
  - [Authentication behavior by environment](#authentication-behavior-by-environment)

This guide demonstrates how to add an optional login and document level access control system to the sample. This system can be used to restrict access to indexed data to specific users based on what [Microsoft Entra groups](https://learn.microsoft.com/entra/fundamentals/how-to-manage-groups) they are a part of, or their [user object id](https://learn.microsoft.com/partner-center/find-ids-and-domain-names#find-the-user-object-id). This system utilizes the [built-in document access and control from Azure AI Search](https://learn.microsoft.com/azure/search/search-query-access-control-rbac-enforcement).

![AppLoginArchitecture](/docs/images/applogincomponents.png)

## Requirements

**IMPORTANT:** In order to add optional login and document level access control, you'll need the following in addition to the normal sample requirements

- **Azure account permissions**: Your Azure account must have [permission to manage applications in Microsoft Entra](https://learn.microsoft.com/entra/identity/role-based-access-control/permissions-reference#cloud-application-administrator).

## Setting up Microsoft Entra applications

Two Microsoft Entra applications must be registered in order to make the optional login and document level access control system work correctly. One app is for the client UI. The client UI is implemented as a [single page application](https://learn.microsoft.com/entra/identity-platform/scenario-spa-app-registration). The other app is for the API server. The API server uses a [confidential client](https://learn.microsoft.com/entra/identity-platform/msal-client-applications) to call the [Microsoft Graph API](https://learn.microsoft.com/graph/use-the-api).

### Automatic Setup

The easiest way to setup the two apps is to use the `azd` CLI. We've written scripts that will automatically create the two apps and configure them for use with the sample. To trigger the automatic setup, run the following commands:

1. **Enable authentication for the app**
  Run the following command to show the login UI and use Entra authentication by default:

    ```shell
    azd env set AZURE_USE_AUTHENTICATION true
    ```

1. **Enable access control on your search index**

    - **If the index does not exist yet:**
      Run the `prepdocs` script.

    - **If the index already exists:**
      Execute this command to enable ACLs:

      ```shell
      python ./scripts/manageacl.py --acl-action enable_acls
      ```

1. (Optional) **Enforce access control**
  To ensure that the app restricts search results to only documents that the user has access to, run the following command:

    ```shell
    azd env set AZURE_ENFORCE_ACCESS_CONTROL true
    ```

1. (Optional) **Allow global document access**
  To allow upload of documents that have global access when there are no document-specific access controls assigned, run the following command:

    ```shell
    azd env set AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS true
    ```

1. (Optional) **Allow unauthenticated access**
  To allow unauthenticated users to use the app, run the following command:

    ```shell
    azd env set AZURE_ENABLE_UNAUTHENTICATED_ACCESS true
    ```

    Note: These users will not be able to search on documents that have access control assigned, so `AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS` should also be set to true to give them access to the remaining documents.

1. **Set the authentication tenant ID**
  Specify the tenant ID associated with authentication by running:

    ```shell
    azd env set AZURE_AUTH_TENANT_ID <YOUR-TENANT-ID>
    ```

1. **Login to the authentication tenant (if needed)**
  If your auth tenant ID is different from your currently logged in tenant ID, run:

    ```shell
    azd auth login --tenant-id <YOUR-TENANT-ID>
    ```

1. **Deploy the app**
  Finally, run the following command to provision and deploy the app:

    ```shell
    azd up
    ```

### Manual Setup

The following instructions explain how to setup the two apps using the Azure Portal.

#### Server App

- Sign in to the [Azure portal](https://portal.azure.com/).
- Select the Microsoft Entra ID service.
- In the left hand menu, select **Application Registrations**.
- Select **New Registration**.
  - In the **Name** section, enter a meaningful application name. This name will be displayed to users of the app, for example `Azure Search OpenAI Chat API`.
  - Under **Supported account types**, select **Accounts in this organizational directory only**.
- Select **Register** to create the application
- In the app's registration screen, find the **Application (client) ID**.
  - Run the following `azd` command to save this ID: `azd env set AZURE_SERVER_APP_ID <Application (client) ID>`.

- Microsoft Entra supports three types of credentials to authenticate an app using the [client credentials](https://learn.microsoft.com/entra/identity-platform/v2-oauth2-client-creds-grant-flow): passwords (app secrets), certificates, and federated identity credentials. For a higher level of security, either [certificates](https://learn.microsoft.com/entra/identity-platform/howto-create-self-signed-certificate) or federated identity credentials are recommended. This sample currently uses an app secret for ease of provisioning.

- Select **Certificates & secrets** in the left hand menu.
- In the **Client secrets** section, select **New client secret**.
  - Type a description, for example `Azure Search OpenAI Chat Key`.
  - Select one of the available key durations.
  - The generated key value will be displayed after you select **Add**.
  - Copy the generated key value and run the following `azd` command to save this ID: `azd env set AZURE_SERVER_APP_SECRET <generated key value>`.
- Select **API Permissions** in the left hand menu. By default, the [delegated `User.Read`](https://learn.microsoft.com/graph/permissions-reference#user-permissions) permission should be present. This permission is required to read the signed-in user's profile.
  - Select **Add a permission**, and then **Microsoft Graph**.
  - Select **Delegated permissions**.
  - Search for and and select `User.Read`.
  - Select **Add permissions**.
- Select **API Permissions** in the left hand menu. The server app will use the `user_impersonation` permission from Azure AI Search to issue a token for security filtering on behalf of the logged in user.
  - Select **Add a permission**, and then **APIs my organization uses**.
  - Search for and select **Azure Cognitive Search**.
  - Select **Delegated permissions**.
  - Search for and and select `user_impersonation`.
  - Select **Add permissions**.
- Select **Expose an API** in the left hand menu. The server app works by using the [On Behalf Of Flow](https://learn.microsoft.com/entra/identity-platform/v2-oauth2-on-behalf-of-flow#protocol-diagram), which requires the server app to expose at least 1 API.
  - The application must define a URI to expose APIs. Select **Add** next to **Application ID URI**.
    - By default, the Application ID URI is set to `api://<application client id>`. Accept the default by selecting **Save**.
  - Under **Scopes defined by this API**, select **Add a scope**.
  - Fill in the values as indicated:
    - For **Scope name**, use **access_as_user**.
    - For **Who can consent?**, select **Admins and users**.
    - For **Admin consent display name**, type **Access Azure Search OpenAI Chat API**.
    - For **Admin consent description**, type **Allows the app to access Azure Search OpenAI Chat API as the signed-in user.**.
    - For **User consent display name**, type **Access Azure Search OpenAI Chat API**.
    - For **User consent description**, type **Allow the app to access Azure Search OpenAI Chat API on your behalf**.
    - Leave **State** set to **Enabled**.
    - Select **Add scope** at the bottom to save the scope.

#### Client App

- Sign in to the [Azure portal](https://portal.azure.com/).
- Select the Microsoft Entra ID service.
- In the left hand menu, select **Application Registrations**.
- Select **New Registration**.
  - In the **Name** section, enter a meaningful application name. This name will be displayed to users of the app, for example `Azure Search OpenAI Chat Web App`.
  - Under **Supported account types**, select **Accounts in this organizational directory only**.
  - Under `Redirect URI (optional)` section, select `Single-page application (SPA)` in the combo-box and enter the following redirect URI:
    - If you are running the sample locally, add the endpoints `http://localhost:50505/redirect` and `http://localhost:5173/redirect`
    - If you are running the sample on Azure, add the endpoints provided by `azd up`: `https://<your-endpoint>.azurewebsites.net/redirect`.
    - If you are running the sample from Github Codespaces, add the Codespaces endpoint: `https://<your-codespace>-50505.app.github.dev/redirect`
- Select **Register** to create the application
- In the app's registration screen, find the **Application (client) ID**.
  - Run the following `azd` command to save this ID: `azd env set AZURE_CLIENT_APP_ID <Application (client) ID>`.
- In the left hand menu, select **Authentication**.
  - Under Web, add a redirect URI with the endpoint provided by `azd up`: `https://<your-endpoint>.azurewebsites.net/.auth/login/aad/callback`.
  - Under **Implicit grant and hybrid flows**, select **ID Tokens (used for implicit and hybrid flows)**
  - Select **Save**
- In the left hand menu, select **API permissions**. You will add permission to access the **access_as_user** API on the server app. This permission is required for the [On Behalf Of Flow](https://learn.microsoft.com/entra/identity-platform/v2-oauth2-on-behalf-of-flow#protocol-diagram) to work.
  - Select **Add a permission**, and then **My APIs**.
  - In the list of applications, select your server application **Azure Search OpenAI Chat API**
  - Ensure **Delegated permissions** is selected.
  - In the **Select permissions** section, select the **access_as_user** permission
  - Select **Add permissions**.
- Stay in the **API permissions** section and select **Add a permission**.
  - Select **Microsoft Graph**.
  - Select **Delegated permissions**.
  - Search for and select `User.Read`.
  - Select **Add permissions**.

#### Configure Server App Known Client Applications

Consent from the user must be obtained for use of the client and server app. The client app can prompt the user for consent through a dialog when they log in. The server app has no ability to show a dialog for consent. Client apps can be [added to the list of known clients](https://learn.microsoft.com/entra/identity-platform/v2-oauth2-on-behalf-of-flow#gaining-consent-for-the-middle-tier-application) to access the server app, so a consent dialog is shown for the server app.

- Navigate to the server app registration
- In the left hand menu, select **Manifest**
- Replace `"knownClientApplications": []` with `"knownClientApplications": ["<client application id>"]`
- Select **Save**

#### Testing

If you are running setup for the first time, ensure you have run `azd env set AZURE_ADLS_GEN2_STORAGE_ACCOUNT <YOUR-STORAGE_ACCOUNT>` before running `azd up`. If you do not set this environment variable, your index will not be initialized with access control support when `prepdocs` is run for the first time. To manually enable access control in your index, use the [manual setup script](#azure-data-lake-storage-gen2-setup).

Ensure you run `azd env set AZURE_USE_AUTHENTICATION` to enable the login UI once you have setup the two Microsoft Entra apps before you deploy or run the application. The login UI will not appear unless all [required environment variables](#environment-variables-reference) have been setup.

#### Programmatic Access with Authentication

If you want to use the chat endpoint without the UI and still use authentication, you must disable [App Service built-in authentication](https://learn.microsoft.com/azure/app-service/overview-authentication-authorization) and use only the app's MSAL-based authentication flow. Ensure the `AZURE_DISABLE_APP_SERVICES_AUTHENTICATION` environment variable is set before deploying.

Get an access token that can be used for calling the chat API using the following code:

```python
from azure.identity import DefaultAzureCredential
import os

token = DefaultAzureCredential().get_token(f"api://{os.environ['AZURE_SERVER_APP_ID']}/access_as_user", tenant_id=os.getenv('AZURE_AUTH_TENANT_ID', os.getenv('AZURE_TENANT_ID')))

print(token.token)
```

### Troubleshooting

- If your primary tenant restricts the ability to create Entra applications, you'll need to use a separate tenant to create the Entra applications. You can create a new tenant by following [these instructions](https://learn.microsoft.com/entra/identity-platform/quickstart-create-new-tenant). Then run `azd env set AZURE_AUTH_TENANT_ID <YOUR-AUTH-TENANT-ID>` before running `azd up`.
- If any Entra apps need to be recreated, you can avoid redeploying the app by [changing the app settings in the portal](https://learn.microsoft.com/azure/app-service/configure-common?tabs=portal#configure-app-settings). Any of the [required environment variables](#environment-variables-reference) can be changed. Once the environment variables have been changed, restart the web app.
- It's possible a consent dialog will not appear when you log into the app for the first time. If this consent dialog doesn't appear, you will be unable to use the security filters because the API server app does not have permission to read your authorization information. A consent dialog can be forced to appear by adding `"prompt": "consent"` to the `loginRequest` property in [`authentication.py`](/app/backend/core/authentication.py)
- It's possible that your tenant admin has placed a restriction on consent to apps with [unverified publishers](https://learn.microsoft.com/entra/identity-platform/publisher-verification-overview). In this case, only admins may consent to the client and server apps, and normal user accounts are unable to use the login system until the admin consents on behalf of the entire organization.
- It's possible that your tenant admin requires [admin approval of all new apps](https://learn.microsoft.com/entra/identity/enterprise-apps/manage-consent-requests). Regardless of whether you select the delegated or admin permissions, the app will not work without tenant admin consent. See this guide for [granting consent to an app](https://learn.microsoft.com/entra/identity/enterprise-apps/grant-admin-consent?pivots=portal).

## Adding data with document level access control

The sample supports 2 main strategies for adding data with document level access control.

- [Using the Add Documents API](#using-the-add-documents-api). Sample scripts are provided which use the Azure AI Search Service Add Documents API to directly manage access control information on _existing documents_ in the index.
- [Using prepdocs and Azure Data Lake Storage Gen 2](#azure-data-lake-storage-gen2-setup). Sample scripts are provided which set up an [Azure Data Lake Storage Gen 2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction) account, set the [access control information](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-access-control) on files and folders stored there, and ingest those documents into the search index with their  access control information.

### Using the Add Documents API

Manually enable document level access control on a search index and manually set access control values using the [manageacl.py](/scripts/manageacl.py) script.

Prior to running the script:

- Run `azd up` or use `azd env set` to manually set the `AZURE_SEARCH_SERVICE` and `AZURE_SEARCH_INDEX` azd environment variables
- Activate the Python virtual environment for your shell session

The script supports the following commands. All commands support `-v` for verbose logging.

- `python ./scripts/manageacl.py --acl-action enable_acls`: Creates the required `oids` (User ID) and `groups` (Group IDs) [security filter](https://learn.microsoft.com/azure/search/search-security-trimming-for-azure-search) fields for document level access control on your index, as well as the `storageUrl` field for storing the Blob storage URL. Does nothing if these fields already exist.

  Example usage:

  ```shell
  python ./scripts/manageacl.py -v --acl-action enable_acls
  ```

- `python ./scripts/manageacl.py --acl-type [oids or groups]--acl-action view --url [https://url.pdf]`: Prints access control values associated with either User IDs or Group IDs for the document at the specified URL.

  Example to view all Group IDs:

  ```shell
  python ./scripts/manageacl.py -v --acl-type groups --acl-action view --url https://st12345.blob.core.windows.net/content/Benefit_Options.pdf
  ```

- `python ./scripts/manageacl.py --acl-type [oids or groups] --acl-action add --acl [ID of group or user] --url [https://url.pdf]`: Adds an access control value associated with either User IDs or Group IDs for the document at the specified URL.

  Example to add a Group ID:

  ```shell
  python ./scripts/manageacl.py -v --acl-type groups --acl-action add --acl xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx --url https://st12345.blob.core.windows.net/content/Benefit_Options.pdf
  ```

- `python ./scripts/manageacl.py --acl-type [oids or groups]--acl-action remove_all --url [https://url.pdf]`: Removes all access control values associated with either User IDs or Group IDs for a specific document.

  Example to remove all Group IDs:

  ```shell
  python ./scripts/manageacl.py -v --acl-type groups --acl-action remove_all --url https://st12345.blob.core.windows.net/content/Benefit_Options.pdf
  ```

- `python ./scripts/manageacl.py --url [https://url.pdf] --acl-type [oids or groups]--acl-action remove --acl [ID of group or user]`: Removes an access control value associated with either User IDs or Group IDs for a specific document.

  Example to remove a specific User ID:

  ```shell
  python ./scripts/manageacl.py -v --acl-type oids --acl-action remove --acl xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx --url https://st12345.blob.core.windows.net/content/Benefit_Options.pdf
  ```

#### Enabling global access on documents without access control

- `python ./scripts/manageacl.py --acl-action enable_global_access`: Set the special [`["all"]`](https://learn.microsoft.com/azure/search/search-index-access-control-lists-and-rbac-push-api#special-acl-values-all-and-none) on the `oids` (User ID) and `groups` (Group IDs) security filter fields in your index on documents that do not have any existing `oids` or `groups` access control. This will enable any signed-in user to query these documents.

### Azure Data Lake Storage Gen2 Setup

[Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction) implements an [access control model](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-access-control) that can be used for document level access control. The [adlsgen2setup.py](/scripts/adlsgen2setup.py) script uploads the sample data included in the [data](./data) folder to a Data Lake Storage Gen2 storage account. The [Storage Blob Data Owner](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-access-control-model#role-based-access-control-azure-rbac) role is required to use the script.

In order to use this script, an existing Data Lake Storage Gen2 storage account is required. Run `azd env set AZURE_ADLS_GEN2_STORAGE_ACCOUNT <your-storage-account>` prior to running the script.

Then run the script inside your Python environment:

```shell
python /scripts/adlsgen2setup.py './data/*' --data-access-control './scripts/sampleacls.json' -v
```

The script performs the following steps:

- Creates example [groups](https://learn.microsoft.com/entra/fundamentals/how-to-manage-groups) listed in the [sampleacls.json](/scripts/sampleacls.json) file.
- Creates a filesystem / container `gptkbcontainer` in the storage account.
- Creates the directories listed in the [sampleacls.json](/scripts/sampleacls.json) file.
- Uploads the sample PDFs referenced in the [sampleacls.json](/scripts/sampleacls.json) file into the appropriate directories.
- [Recursively sets Access Control Lists (ACLs)](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-acl-cli) using the information from the [sampleacls.json](/scripts/sampleacls.json) file.

In order to use the sample access control, you need to join these groups in your Microsoft Entra tenant.

Note that this optional script may not work in Codespaces if your administrator has applied a [Conditional Access policy](https://learn.microsoft.com/entra/identity/conditional-access/overview) to your tenant.

#### Azure Data Lake Storage Gen2 Prep Docs

Once a Data Lake Storage Gen2 storage account has been setup with sample data and access control lists, [prepdocs.py](/app/backend/prepdocs.py) can be used to automatically process PDFs in the storage account and store them with their [access control lists in the search index](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-access-control).

To run this script with a Data Lake Storage Gen2 account, first set the following environment variables:

- `AZURE_ADLS_GEN2_STORAGE_ACCOUNT`: Name of existing [Data Lake Storage Gen2 storage account](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).
- (Optional) `AZURE_ADLS_GEN2_FILESYSTEM`: Name of existing Data Lake Storage Gen2 filesystem / container in the storage account. If empty, `gptkbcontainer` is used.
- (Optional) `AZURE_ADLS_GEN2_FILESYSTEM_PATH`: Specific path in the Data Lake Storage Gen2 filesystem / container to process. Only PDFs contained in this path will be processed.

Once the environment variables are set, run the script using the following command: `/scripts/prepdocs.ps1` or `/scripts/prepdocs.sh`.

## Migrate to built-in document access control

Previous versions of the sample used [security filters](https://learn.microsoft.com/azure/search/search-security-trimming-for-azure-search) to implement document-level access control.
To support [built-in access control](https://learn.microsoft.com/azure/search/search-query-access-control-rbac-enforcement), deployment takes the following steps:

1. Adds the `user_impersonation` permission for Azure AI Search to the server app
2. Enables [permission filtering](https://learn.microsoft.com/azure/search/search-index-access-control-lists-and-rbac-push-api#create-an-index-with-permission-filter-fields) on the existing index.
3. Sets the [x-ms-query-source-authorization](https://learn.microsoft.com/azure/search/search-query-access-control-rbac-enforcement#how-query-time-enforcement-works) header on every query when `AZURE_ENFORCE_ACCESS_CONTROL` is enabled.

When `AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS` was enabled, previous versions of the sample interpreted no access control on a document as meaning that the document was globally available. Built-in document access control requires [`["all"]`](https://learn.microsoft.com/azure/search/search-index-access-control-lists-and-rbac-push-api#special-acl-values-all-and-none) to be set for each globally available document. You can run a [one-time migration](#enabling-global-access-on-documents-without-access-control) on your existing index to enable global access for these documents.

## Environment variables reference

The following environment variables are used to setup the optional login and document level access control:

- `AZURE_USE_AUTHENTICATION`: Enables Entra ID login and document level access control. Set to true before running `azd up`.
- `AZURE_ENFORCE_ACCESS_CONTROL`: Enforces Entra ID based login and document level access control on documents with access control assigned. Set to true before running `azd up`. If `AZURE_ENFORCE_ACCESS_CONTROL` is enabled and `AZURE_ENABLE_UNAUTHENTICATED_ACCESS` is not enabled, then authentication is required to use the app.
- `AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS`: Enables prepdocs upload code to support setting user ids and group ids to `["all"]` when uploading documents that have no access control assigned. This will enable the built-in document level access control to return these documents if `AZURE_ENFORCE_ACCESS_CONTROL` is enabled. If you are migrating from a previous version where this was not required, you'll have to perform a [one-time migration](#migrate-to-built-in-document-access-control) to enable global document access.
- `AZURE_ENABLE_UNAUTHENTICATED_ACCESS`: Allows unauthenticated users to access the chat app. If `AZURE_ENFORCE_ACCESS_CONTROL` is enabled, unauthenticated users cannot search on documents.
- `AZURE_DISABLE_APP_SERVICES_AUTHENTICATION`: Disables [use of built-in authentication for App Services](https://learn.microsoft.com/azure/app-service/overview-authentication-authorization). An authentication flow based on the MSAL SDKs is used instead. Useful when you want to provide programmatic access to the chat endpoints with authentication.
- `AZURE_SERVER_APP_ID`: (Required) Application ID of the Microsoft Entra app for the API server.
- `AZURE_SERVER_APP_SECRET`: [Client secret](https://learn.microsoft.com/entra/identity-platform/v2-oauth2-client-creds-grant-flow) used by the API server to authenticate using the Microsoft Entra server app.
- `AZURE_CLIENT_APP_ID`: Application ID of the Microsoft Entra app for the client UI.
- `AZURE_AUTH_TENANT_ID`: [Tenant ID](https://learn.microsoft.com/entra/fundamentals/how-to-find-tenant) associated with the Microsoft Entra tenant used for login and document level access control. Defaults to `AZURE_TENANT_ID` if not defined.
- `AZURE_ADLS_GEN2_STORAGE_ACCOUNT`: (Optional) Name of existing [Data Lake Storage Gen2 storage account](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction) for storing sample data with [access control lists](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-access-control). Only used with the optional Data Lake Storage Gen2 [setup](#azure-data-lake-storage-gen2-setup) and [prep docs](#azure-data-lake-storage-gen2-prep-docs) scripts.
- `AZURE_ADLS_GEN2_FILESYSTEM`: (Optional) Name of existing [Data Lake Storage Gen2 filesystem](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction) for storing sample data with [access control lists](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-access-control). Only used with the optional Data Lake Storage Gen2 [setup](#azure-data-lake-storage-gen2-setup) and [prep docs](#azure-data-lake-storage-gen2-prep-docs) scripts.
- `AZURE_ADLS_GEN2_FILESYSTEM_PATH`: (Optional) Name of existing path in a [Data Lake Storage Gen2 filesystem](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction) for storing sample data with [access control lists](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-access-control). Only used with the optional Data Lake Storage Gen2 [prep docs](#azure-data-lake-storage-gen2-prep-docs) script.

### Authentication behavior by environment

This application uses an in-memory token cache. User sessions are only available in memory while the application is running. When the application server is restarted, all users will need to log-in again.

The following table describes the impact of the `AZURE_USE_AUTHENTICATION` and `AZURE_ENFORCE_ACCESS_CONTROL` variables depending on the environment you are deploying the application in:

| AZURE_USE_AUTHENTICATION | AZURE_ENFORCE_ACCESS_CONTROL | Environment | Default Behavior |
|-|-|-|-|
| True | False | App Services | Use integrated auth <br /> Login page blocks access to app <br /> User can opt-into access control in developer settings <br /> Allows unrestricted access to sources |
| True | True | App Services | Use integrated auth <br /> Login page blocks access to app <br /> User must use access control |
| True | False | Local or Codespaces | Do not use integrated auth <br /> Can use app without login <br /> User can opt-into access control in developer settings <br /> Allows unrestricted access to sources |
| True | True | Local or Codespaces | Do not use integrated auth <br /> Cannot use app without login <br /> Behavior is chat box is greyed out with default “Please login message” <br /> User must use login button to make chat box usable <br /> User must use access control when logged in |
| False | False | All | No login or access control |
| False | True | All | Invalid setting |


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\monitoring.md ====

# RAG chat: Monitoring with Application Insights

By default, deployed apps use Application Insights for the tracing of each request, along with the logging of errors.

* [Performance](#performance)
* [Failures](#failures)
* [Dashboard](#dashboard)
* [Customizing the traces](#customizing-the-traces)

## Performance

To see the performance data, go to the Application Insights resource in your resource group, click on the "Investigate -> Performance" blade and navigate to any HTTP request to see the timing data.
To inspect the performance of chat requests, use the "Drill into Samples" button to see end-to-end traces of all the API calls made for any chat request:

![Tracing screenshot](images/transaction-tracing.png)

## Failures

To see any exceptions and server errors, navigate to the "Investigate -> Failures" blade and use the filtering tools to locate a specific exception. You can see Python stack traces on the right-hand side.

## Dashboard

You can see chart summaries on a dashboard by running the following command:

```shell
azd monitor
```

You can modify the contents of that dashboard by updating `infra/backend-dashboard.bicep`, which is a Bicep file that defines the dashboard contents and layout.

## Customizing the traces

The tracing is done using these OpenTelemetry Python packages:

* [azure-monitor-opentelemetry](https://pypi.org/project/azure-monitor-opentelemetry/)
* [opentelemetry-instrumentation-asgi](https://pypi.org/project/opentelemetry-instrumentation-asgi/)
* [opentelemetry-instrumentation-httpx](https://pypi.org/project/opentelemetry-instrumentation-httpx/)
* [opentelemetry-instrumentation-aiohttp-client](https://pypi.org/project/opentelemetry-instrumentation-aiohttp-client/)
* [opentelemetry-instrumentation-openai](https://pypi.org/project/opentelemetry-instrumentation-openai/)

Those packages are configured in the `app.py` file:

```python
if os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING"):
    configure_azure_monitor()
    # This tracks HTTP requests made by aiohttp:
    AioHttpClientInstrumentor().instrument()
    # This tracks HTTP requests made by httpx:
    HTTPXClientInstrumentor().instrument()
    # This tracks OpenAI SDK requests:
    OpenAIInstrumentor().instrument()
    # This middleware tracks app route requests:
    app.asgi_app = OpenTelemetryMiddleware(app.asgi_app)
```

You can pass in parameters to `configure_azure_monitor()` to customize the tracing, like to add custom span processors.
You can also set [OpenTelemetry environment variables](https://opentelemetry.io/docs/reference/specification/sdk-environment-variables/) to customize the tracing, like to set the sampling rate.
See the [azure-monitor-opentelemetry](https://pypi.org/project/azure-monitor-opentelemetry/) documentation for more details.

By default, [opentelemetry-instrumentation-openai](https://pypi.org/project/opentelemetry-instrumentation-openai/) traces all requests made to the OpenAI API, including the messages and responses. To disable that for privacy reasons, set the `TRACELOOP_TRACE_CONTENT=false` environment variable.

To set environment variables, update `appEnvVariables` in `infra/main.bicep` and re-run `azd up`.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\multimodal.md ====

# RAG chat: Support for multimodal documents

This repository includes an optional feature that uses multimodal embedding models and multimodal chat completion models
to better handle documents that contain images, such as financial reports with charts and graphs.

With this feature enabled, the data ingestion process will extract images from your documents
using Document Intelligence, store the images in Azure Blob Storage, vectorize the images using the Azure AI Vision service, and store the image embeddings in the Azure AI Search index.

During the RAG flow, the app will perform a multi-vector query using both text and image embeddings, and then send any images associated with the retrieved document chunks to the chat completion model for answering questions. This feature assumes that your chat completion model supports multimodal inputs, such as `gpt-4o`, `gpt-4o-mini`, `gpt-5`, or `gpt-5-mini`.

With this feature enabled, the following changes are made:

* **Search index**: We add a new field "images" to the Azure AI Search index to store information about the images associated with a chunk. The field is a complex field that contains the embedding returned by the multimodal Azure AI Vision API, the bounding box, and the URL of the image in Azure Blob Storage.
* **Data ingestion**: In addition to the usual data ingestion flow, the document extraction process will extract images from the documents using Document Intelligence, store the images in Azure Blob Storage with a citation at the top border, and vectorize the images using the Azure AI Vision service.
* **Question answering**: We search the index using both the text and multimodal embeddings. We send both the text and the image to the LLM, and ask it to answer the question based on both kinds of sources.
* **Citations**: The frontend displays both image sources and text sources, to help users understand how the answer was generated.

## Prerequisites

* The use of a chat completion model that supports multimodal inputs. The default model for the repository is currently `gpt-4.1-mini`, which does support multimodal inputs. The `gpt-4o-mini` technically supports multimodal inputs, but due to how image tokens are calculated, you need a much higher deployment capacity to use it effectively. Please try `gpt-4.1-mini` first, and experiment with other models later.

## Deployment

1. **Enable multimodal capabilities**

   Set the azd environment variable to enable the multimodal feature:

   ```shell
   azd env set USE_MULTIMODAL true
   ```

2. **Provision the multimodal resources**

   Either run `azd up` if you haven't run it before, or run `azd provision` to provision the multimodal resources. This will create a new Azure AI Vision account and update the Azure AI Search index to include the new image embedding field.

3. **Re-index the data:**

   If you have already indexed data, you will need to re-index it to include the new image embeddings.
   We recommend creating a new Azure AI Search index to avoid conflicts with the existing index.

   ```shell
   azd env set AZURE_SEARCH_INDEX multimodal-index
   ```

   Then delete the `.md5` hash files in the data folder(s) and run the data ingestion process again to re-index the data:

   Linux/Mac:

   ```shell
   ./scripts/prepdocs.sh
   ```

   Windows:

   ```shell
   .\scripts\prepdocs.ps1
   ```

4. **Try out the feature:**

   ![Screenshot of app with Developer Settings open, showing multimodal settings highlighted](./images/multimodal.png)

   * If you're using the sample data, try one of the sample questions about the financial documents.
   * Check the "Thought process" tab to see how the multimodal approach was used
   * Check the "Supporting content" tab to see the text and images that were used to generate the answer.
   * Open "Developer settings" and try different options for "Included vector fields" and "LLM input sources" to see how they affect the results.

## Customize the multimodal approach

You can customize the RAG flow approach with a few additional environment variables.
You can also modify those settings in the "Developer Settings" in the chat UI,
to experiment with different options before committing to them.

### Control vector retrieval

Set variables to control whether Azure AI Search will do retrieval using the text embeddings, image embeddings, or both.
By default, it will retrieve using both text and image embeddings.

To disable retrieval with text embeddings, run:

```shell
azd env set RAG_SEARCH_TEXT_EMBEDDINGS false
```

To disable retrieval with image embeddings, run:

```shell
azd env set RAG_SEARCH_IMAGE_EMBEDDINGS false
```

Many developers may find that they can turn off image embeddings and still have high quality retrieval, since the text embeddings are based off text chunks that include figure descriptions.

### Control LLM input sources

Set variables to control whether the chat completion model will use text inputs, image inputs, or both:

To disable text inputs, run:

```shell
azd env set RAG_SEND_TEXT_SOURCES false
```

To disable image inputs, run:

```shell
azd env set RAG_SEND_IMAGE_SOURCES false
```

It is unlikely that you would want to turn off text sources, unless your RAG is based on documents that are 100% image-based.
However, you may want to turn off image inputs to save on token costs and improve performance,
and you may still see good results with just text inputs, since the inputs contain the figure descriptions.

## Compatibility

* This feature is **not** compatible with [integrated vectorization](./deploy_features.md#enabling-integrated-vectorization), as the currently configured built-in skills do not process images or store image embeddings. Azure AI Search does now offer built-in skills for multimodal support, as demonstrated in [azure-ai-search-multimodal-sample](https://github.com/Azure-Samples/azure-ai-search-multimodal-sample), but we have not integrated them in this project. Instead, we are working on making a custom skill based off the data ingestion code in this repository, and hosting that skill on Azure Functions. Stay tuned to the releases to find out when that's available.
* This feature *is* compatible with the [reasoning models](./reasoning.md) feature, as long as you use a model that [supports image inputs](https://learn.microsoft.com/azure/ai-services/openai/how-to/reasoning?tabs=python-secure%2Cpy#api--feature-support).


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\other_samples.md ====

# RAG chat: Alternative RAG chat samples

There are an increasingly large number of ways to build RAG chat apps.

* [Most similar to this repo](#most-similar-to-this-repo)
* [azurechat](#azurechat)
* [sample-app-aoai-chatGPT](#sample-app-aoai-chatgpt)

## Most similar to this repo

Inspired by this repo, there are similar RAG chat apps for other languages:

* [**JavaScript**](https://aka.ms/azai/js/code)
* [**.NET**](https://aka.ms/azai/net/code)
* [**Java**](https://aka.ms/azai/java/code)

They do not all support the same features as this repo, but they provide a good starting point for building a RAG chat app in your preferred language.

## azurechat

Another popular sample is the Azure Chat Solution Accelerator:
[https://github.com/microsoft/azurechat](https://github.com/microsoft/azurechat)

AzureChat deploys a private chat app with a ChatGPT-like UX on Azure, with built‑in capabilities for chatting over internal data and files and optional extensions.

Key differences versus this repository:

* **Technology stack**: AzureChat uses a full JavaScript/TypeScript stack with a Node.js backend; this repo uses Python (Quart) for backend services.
* **Use case emphasis**: AzureChat offers more features around user personalization, while this repo offers more features needed for enterprise scenarios like data ACLs and evaluation.

Feature comparison:

| Feature | azure-search-openai-demo | azurechat |
| --- | --- | --- |
| Vector support | ✅ Yes | ✅ Yes |
| Data ingestion | ✅ Yes ([Many formats](data_ingestion.md#supported-document-formats)) | ✅ Yes |
| Persistent chat history | ✅ Yes | ✅ Yes |
| Multimodal | ✅ Yes | ✅ Yes |
| Voice/Speech I/O | ✅ Yes | ✅ Yes |
| File upload | ✅ Yes | ✅ Yes |
| Auth + ACL | ✅ Yes (Enterprise-focused) | ✅ Yes (Personal-focused) |
| Access control | ✅ Yes (Document-level) | ❌ Limited |

Technology comparison:

| Tech | azure-search-openai-demo | azurechat |
| --- | --- | --- |
| Frontend | React (TypeScript) | React (TypeScript) |
| Backend | Python (Quart) | Node.js (TypeScript) |
| Database | Azure AI Search | Azure AI Search |
| Deployment | Azure Developer CLI (azd) | Azure Developer CLI (azd) |

## sample-app-aoai-chatGPT

Another popular repository for this use case is:
[https://github.com/Microsoft/sample-app-aoai-chatGPT/](https://github.com/Microsoft/sample-app-aoai-chatGPT/)

That repository is designed for use by customers using Azure OpenAI studio and Azure Portal for setup. It also includes `azd` support for folks who want to deploy it completely from scratch.

The primary differences:

* This repository includes multiple RAG (retrieval-augmented generation) approaches that chain the results of multiple API calls (to Azure OpenAI and ACS) together in different ways. The other repository uses only the built-in data sources option for the ChatCompletions API, which uses a RAG approach on the specified ACS index. That should work for most uses, but if you needed more flexibility, this sample may be a better option.
* This repository is also a bit more experimental in other ways, since it's not tied to the Azure OpenAI Studio like the other repository.

Feature comparison:

| Feature | azure-search-openai-demo | sample-app-aoai-chatGPT |
| --- | --- | --- |
| Vector support | ✅ Yes | ✅ Yes |
| Data ingestion | ✅ Yes ([Many formats](data_ingestion.md#supported-document-formats)) | ✅ Yes ([Many formats](https://learn.microsoft.com/azure/ai-services/openai/concepts/use-your-data?tabs=ai-search#data-formats-and-file-types)) |
| Persistent chat history | ✅ Yes | ✅ Yes |
| User feedback | ❌ No | ✅ Yes |
| GPT-4-vision |  ✅ Yes | ❌ No |
| Auth + ACL |  ✅ Yes | ✅ Yes |
| User upload |  ✅ Yes | ❌ No |
| Speech I/O | ✅ Yes | ❌ No |

Technology comparison:

| Tech | azure-search-openai-demo | sample-app-aoai-chatGPT |
| --- | --- | --- |
| Frontend | React | React |
| Backend | Python (Quart) | Python (Quart) |
| Vector DB | Azure AI Search | Azure AI Search, CosmosDB Mongo vCore, ElasticSearch, Pinecone, AzureML |
| Deployment | Azure Developer CLI (azd) | Azure Portal, az, azd |


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\productionizing.md ====

# RAG chat: Productionizing the app

This sample is designed to be a starting point for your own production application,
but you should do a thorough review of the security and performance before deploying
to production. Here are some things to consider:

* [Azure resource configuration](#azure-resource-configuration)
* [Additional security measures](#additional-security-measures)
* [Load testing](#load-testing)
* [Evaluation](#evaluation)

## Azure resource configuration

### OpenAI Capacity

The default TPM (tokens per minute) is set to 30K. That is equivalent
to approximately 30 conversations per minute (assuming 1K per user message/response).
You can increase the capacity by changing the `chatGptDeploymentCapacity` and `embeddingDeploymentCapacity`
parameters in `infra/main.bicep` to your account's maximum capacity.
You can also view the Quotas tab in [Azure OpenAI studio](https://oai.azure.com/)
to understand how much capacity you have.

If the maximum TPM isn't enough for your expected load, you have a few options:

* Use a backoff mechanism to retry the request. This is helpful if you're running into a short-term quota due to bursts of activity but aren't over long-term quota. The [tenacity](https://tenacity.readthedocs.io/en/latest/) library is a good option for this, and this [pull request](https://github.com/Azure-Samples/azure-search-openai-demo/pull/500) shows how to apply it to this app.

* If you are consistently going over the TPM, then consider implementing a load balancer between OpenAI instances. Most developers implement that using Azure API Management or container-based load balancers. A native Python approach that integrates with the OpenAI Python API Library is also possible. For integration instructions with this sample, please check:
  * [Scale Azure OpenAI for Python with Azure API Management](https://learn.microsoft.com/azure/developer/python/get-started-app-chat-scaling-with-azure-api-management)
  * [Scale Azure OpenAI for Python chat using RAG with Azure Container Apps](https://learn.microsoft.com/azure/developer/python/get-started-app-chat-scaling-with-azure-container-apps)
  * [Pull request: Scale Azure OpenAI for Python with the Python openai-priority-loadbalancer](https://github.com/Azure-Samples/azure-search-openai-demo/pull/1626)

### Azure Storage

The default storage account uses the `Standard_LRS` SKU.
To improve your resiliency, we recommend using `Standard_ZRS` for production deployments,
which you can specify using the `sku` property under the `storage` module in `infra/main.bicep`.

### Azure AI Search

The default search service uses the "Basic" SKU
with the free semantic ranker option, which gives you 1000 free queries a month.
After 1000 queries, you will get an error message about exceeding the semantic ranker free capacity.

* Assuming your app will experience more than 1000 questions per month,
  you should upgrade the semantic ranker SKU from "free" to "standard" SKU:

  ```shell
  azd env set AZURE_SEARCH_SEMANTIC_RANKER standard
  ```

  Or disable semantic search entirely:

  ```shell
  azd env set AZURE_SEARCH_SEMANTIC_RANKER disabled
  ```

* The search service can handle fairly large indexes, but it does have per-SKU limits on storage sizes, maximum vector dimensions, etc. You may want to upgrade the SKU to either a Standard or Storage Optimized SKU, depending on your expected load.
However, you [cannot change the SKU](https://learn.microsoft.com/azure/search/search-sku-tier#tier-upgrade-or-downgrade) of an existing search service, so you will need to re-index the data or manually copy it over.
You can change the SKU by setting the `AZURE_SEARCH_SERVICE_SKU` azd environment variable to [an allowed SKU](https://learn.microsoft.com/azure/templates/microsoft.search/searchservices?pivots=deployment-language-bicep#sku).

  ```shell
  azd env set AZURE_SEARCH_SERVICE_SKU standard
  ```

  See the [Azure AI Search service limits documentation](https://learn.microsoft.com/azure/search/search-limits-quotas-capacity) for more details.

* If you see errors about search service capacity being exceeded, you may find it helpful to increase
the number of replicas by changing `replicaCount` in `infra/core/search/search-services.bicep`
or manually scaling it from the Azure Portal.

### Azure App Service

The default app service plan uses the `Basic` SKU with 1 CPU core and 1.75 GB RAM.
We recommend using a Premium level SKU, starting with 1 CPU core.
You can use auto-scaling rules or scheduled scaling rules,
and scale up the maximum/minimum based on load.

### Azure Container Apps

The default container app uses a "Consumption" workload profile with 1 CPU core and 2 GB RAM,
and scaling rules that allow for scaling all the way down to 0 replicas when idle.
For production, consider either increasing the CPU cores and memory or
[switching to a "Dedicated" workload profile](azure_container_apps.md#customizing-workload-profile),
and configure the scaling rules to keep at least two replicas running at all times.
Learn more in the [Azure Container Apps documentation](https://learn.microsoft.com/azure/container-apps).

## Additional security measures

* **Authentication**: By default, the deployed app is publicly accessible.
  We recommend restricting access to authenticated users.
  See [Enabling authentication](./deploy_features.md#enabling-authentication) to learn how to enable authentication.
* **Networking**: We recommend [deploying inside a Virtual Network](./deploy_private.md). If the app is only for
  internal enterprise use, use a private DNS zone. Also consider using Azure API Management (APIM)
  for firewalls and other forms of protection.
  For more details, read [Azure OpenAI Landing Zone reference architecture](https://techcommunity.microsoft.com/blog/azurearchitectureblog/azure-openai-landing-zone-reference-architecture/3882102).

## Load testing

We recommend running a loadtest for your expected number of users.
You can use the [locust tool](https://docs.locust.io/) with the `locustfile.py` in this sample
or set up a loadtest with Azure Load Testing.

First make sure you have the locust package installed in your Python environment:

```shell
python -m pip install locust
```

Then run the locust command, specifying the name of the User class to use from `locustfile.py`. We've provided a `ChatUser` class that simulates a user asking questions and receiving answers.

```shell
locust ChatUser
```

Open the locust UI at [http://localhost:8089/](http://localhost:8089/), the URI displayed in the terminal.

Start a new test with the URI of your website, e.g. `https://my-chat-app.azurewebsites.net`.
Do *not* end the URI with a slash. You can start by pointing at your localhost if you're concerned
more about load on OpenAI/AI Search than the host platform.

For the number of users and spawn rate, we recommend starting with 20 users and 1 users/second.
From there, you can keep increasing the number of users to simulate your expected load.

Here's an example loadtest for 50 users and a spawn rate of 1 per second:

![Screenshot of Locust charts showing 5 requests per second](images/screenshot_locust.png)

After each test, check the local or App Service logs to see if there are any errors.

## Evaluation

Before you make your chat app available to users, you'll want to rigorously evaluate the answer quality. You can use tools in [the AI RAG Chat evaluator](https://github.com/Azure-Samples/ai-rag-chat-evaluator) repository to run evaluations, review results, and compare answers across runs.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\README.md ====

# Additional documentation

Consult the main [README](../README.md) for general information about the project.
These are advanced topics that are not necessary for a basic deployment.

- Deploying:
  - [Troubleshooting deployment](docs/deploy_troubleshooting.md)
    - [Debugging the app on App Service](appservice.md)
  - [Deploying with azd: deep dive and CI/CD](azd.md)
  - [Deploying with existing Azure resources](deploy_existing.md)
  - [Deploying from a free account](deploy_lowcost.md)
  - [Enabling optional features](deploy_features.md)
    - [All features](docs/deploy_features.md)
    - [Login and access control](login_and_acl.md)
    - [Multimodal](multimodal.md)
    - [Private endpoints](deploy_private.md)
    - [Agentic retrieval](agentic_retrieval.md)
  - [Sharing deployment environments](sharing_environments.md)
- [Local development](localdev.md)
- [Customizing the app](customization.md)
- [App architecture](architecture.md)
- [HTTP Protocol](http_protocol.md)
- [Data ingestion](data_ingestion.md)
- [Evaluation](docs/evaluation.md)
- [Safety evaluation](safety_evaluation.md)
- [Monitoring with Application Insights](monitoring.md)
- [Productionizing](productionizing.md)
- [Alternative RAG chat samples](other_samples.md)


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\reasoning.md ====

# RAG chat: Using reasoning models

This repository includes an optional feature that uses reasoning models to generate responses based on retrieved content. These models spend more time processing and understanding the user's request.

## Using the feature

### Supported Models

* gpt-5
* gpt-5-mini
* gpt-5-nano
* o4-mini
* o3
* o3-mini
* o1

### Prerequisites

* The ability to deploy a reasoning model in the [supported regions](https://learn.microsoft.com/azure/ai-services/openai/concepts/models#standard-deployment-model-availability). If you're not sure, try to create a o3-mini deployment from your Azure OpenAI deployments page.

### Deployment

1. **Enable reasoning:**

   Set the environment variables for your Azure OpenAI GPT deployments to your reasoning model

   For gpt-5:

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-5
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT gpt-5
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-08-07
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
   azd env set AZURE_OPENAI_API_VERSION 2025-04-01-preview
   ```

   For gpt-5-mini:

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-5-mini
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT gpt-5-mini
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-08-07
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
   azd env set AZURE_OPENAI_API_VERSION 2025-04-01-preview
   ```

   For gpt-5-nano:

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL gpt-5-nano
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT gpt-5-nano
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-08-07
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
   azd env set AZURE_OPENAI_API_VERSION 2025-04-01-preview
   ```

   For o4-mini:

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL o4-mini
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT o4-mini
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-04-16
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
   azd env set AZURE_OPENAI_API_VERSION 2025-04-01-preview
   ```

   For o3:

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL o3
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT o3
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-04-16
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
   azd env set AZURE_OPENAI_API_VERSION 2025-04-01-preview
   ```

   For o3-mini: (No vision support)

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL o4-mini
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT o4-mini
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2025-04-16
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
   azd env set AZURE_OPENAI_API_VERSION 2025-04-01-preview
   ```

   For o1: (No streaming support)

   ```shell
   azd env set AZURE_OPENAI_CHATGPT_MODEL o1
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT o1
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_VERSION 2024-12-17
   azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT_SKU GlobalStandard
   azd env set AZURE_OPENAI_API_VERSION 2024-12-01-preview
   ```

2. **(Optional) Set default reasoning effort**

   You can configure how much effort the reasoning model spends on processing and understanding the user's request. Valid options are `minimal` (for GPT-5 models only), `low`, `medium`, and `high`. Reasoning effort defaults to `medium` if not set.

   Set the environment variable for reasoning effort:

   ```shell
   azd env set AZURE_OPENAI_REASONING_EFFORT minimal
   ```

3. **Update the infrastructure and application:**

   Execute `azd up` to provision the infrastructure changes (only the new model, if you ran `up` previously) and deploy the application code with the updated environment variables.

4. **Try out the feature:**

   Open the web app and start a new chat. The reasoning model will be used for all chat completion requests, including the query rewriting step.

5. **Experiment with reasoning effort:**

   Select the developer options in the web app and change "Reasoning Effort" to `low`, `medium`, or `high`. This will override the default reasoning effort of "medium".

   ![Reasoning configuration screenshot](./images/reasoning.png)

6. **Understand token usage:**

   The reasoning models use additional billed tokens behind the scenes for the thinking process.
   To see the token usage, select the lightbulb icon on a chat answer. This will open the "Thought process" tab, which shows the reasoning model's thought process and the token usage for each chat completion.

   ![Thought process token usage](./images/token-usage.png)


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\safety_evaluation.md ====

# Evaluating RAG answer safety

When deploying a RAG app to production, you should evaluate the safety of the answers generated by the RAG flow. This is important to ensure that the answers are appropriate and do not contain any harmful or sensitive content. This project includes scripts that use  Azure AI services to simulate an adversarial user and evaluate the safety of the answers generated in response to those adversarial queries.

* [Deploy an Azure AI project](#deploy-an-azure-ai-project)
* [Simulate and evaluate adversarial users](#simulate-and-evaluate-adversarial-users)
* [Review the safety evaluation results](#review-the-safety-evaluation-results)

## Deploy an Azure AI project

In order to use the adversarial simulator and safety evaluators, you need an Azure AI project inside an Azure AI Hub.

1. Run this command to tell `azd` to provision an Azure AI project and hub:

    ```shell
    azd env set USE_AI_PROJECT true
    ```

2. Then, run the following command to provision the project:

    ```shell
    azd provision
    ```

## Simulate and evaluate adversarial users

Run the following command to simulate adversarial queries and evaluate the safety of the answers generated in response to those queries:

```shell
python evals/safety_evaluation.py --target_url <TARGET_URL> --max_simulations <MAX_RESULTS>
```

* `--target_url`: The target URL for the callback. Default is `http://localhost:50505/chat`.
* `--max_simulations`: The maximum number of simulated user queries. Default is `200`. The higher the number, the longer the evaluation will take. The default of `200` simulations will take about 25 minutes to run, which includes both the time to generate the simulated data and the time to evaluate it.

## Review the safety evaluation results

That command will generate a report in `safety_results.json`.
Results will look like this:

```json
{
  "hate_unfairness": {
    "low_count": 20,
    "score_total": 8,
    "mean_score": 0.4,
    "low_rate": 1.0
  },
  "sexual": {
    "low_count": 20,
    "score_total": 9,
    "mean_score": 0.45,
    "low_rate": 1.0
  },
  "violence": {
    "low_count": 20,
    "score_total": 9,
    "mean_score": 0.45,
    "low_rate": 1.0
  },
  "self_harm": {
    "low_count": 20,
    "score_total": 10,
    "mean_score": 0.5,
    "low_rate": 1.0
  }
}
```

The ideal score is `low_rate` of 1.0 and `mean_score` of 0.0. The `low_rate` indicates the fraction of answers that were reported as "Low" or "Very low" by an evaluator. The `mean_score` is the average score of all the answers, where 0 is a very safe answer and 7 is a very unsafe answer.

## Resources

To learn more about the Azure AI services used in this project, look through the script and reference the following documentation:

* [Generate simulated data for evaluation](https://learn.microsoft.com/azure/ai-studio/how-to/develop/simulator-interaction-data)
* [Evaluate with the Azure AI Evaluation SDK](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk)


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\sharing_environments.md ====

# RAG chat: Sharing deployment environments

If you've deployed the RAG chat solution already following the steps in the [deployment guide](../README.md#deploying), you may want to share the environment with a colleague.
Either you or they can follow these steps:

1. Install the [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli)
1. Run `azd init -t azure-search-openai-demo` or clone this repository.
1. Run `azd env refresh -e {environment name}`
   They will need the azd environment name, subscription ID, and location to run this command. You can find those values in your `.azure/{env name}/.env` file.  This will populate their azd environment's `.env` file with all the settings needed to run the app locally.
1. Set the environment variable `AZURE_PRINCIPAL_ID` either in that `.env` file or in the active shell to their Azure ID, which they can get with `az ad signed-in-user show`.
1. Run `./scripts/roles.ps1` or `.scripts/roles.sh` to assign all of the necessary roles to the user.  If they do not have the necessary permission to create roles in the subscription, then you may need to run this script for them. Once the script runs, they should be able to run the app locally.


==== File: C:\Users\NEHA\Downloads\azure-search-openai-demo-main\docs\textsplitter.md ====

# RAG Chat: Text splitting algorithm overview

This document explains the chunking logic implemented in the [data ingestion pipeline](./data_ingestion.md). The [splitter module](../app/backend/prepdocslib/textsplitter.py) contains both a `SimpleTextSplitter` (used only for JSON files) and a `SentenceTextSplitter` (used for all other formats). This document focuses on the `SentenceTextSplitter` since its approach is far more complicated, and it can be difficult to follow the code.

* [High-level overview](#high-level-overview)
* [Splitting algorithm](#splitting-algorithm)
* [Recursive handling of oversized spans](#recursive-handling-of-oversized-spans)
* [Cross-page boundary repair](#cross-page-boundary-repair)
* [Chunk normalization](#chunk-normalization)
* [Semantic overlap](#semantic-overlap)
* [Examples](#examples)

## High-level overview

The `SentenceTextSplitter` is designed to:

1. Produce semantically coherent chunks that align with sentence boundaries.
2. Respect a maximum token count per chunk (hard limit of 500 tokens) plus a soft character length guideline (default 1,000 characters with a 20% overflow tolerance for merges / normalization). Size limit does not apply to figure blocks (chunks containing a `<figure>` may exceed the token limit; figures are never split).
3. Keep structural figure placeholders (`<figure>...</figure>`) atomic: never split internally and always attach them to preceding accumulated text if any exists.
4. Repair mid‑sentence page breaks when possible, while enforcing token + soft character budgets.
5. Avoid empty outputs or unclosed figure tags.
6. Perform a light normalization pass (trim only minimal leading/trailing whitespace that would cause small overflows; do not modify figure chunks).

The splitter includes these components:

* Pre-processing of figures: figure blocks are extracted first and treated as atomic before any span splitting or recursion on plain text.
* An accumulator that appends sentence‑like spans until the next addition would breach character or token limits, then flushes a chunk. Default hard token cap is 500 per chunk. Sentence segmentation is based on sentence-ending punctuation ( `. ! ?` plus CJK equivalents).
* [Recursive subdivision of oversized individual spans](#recursive-handling-of-oversized-spans) based on first looking for a sentence boundary, then a word break, and falling back to a midpoint split with overlap.
* [Cross‑page merge of text chunks](#cross-page-boundary-repair) when combined size fits within the allowed chunk size; otherwise a trailing sentence segment may be shifted forward to the next chunk.
* [A pass that adds semantic overlap](#semantic-overlap) to each chunk by appending a trimmed prefix of the next chunk (10% of max section length) onto the end of the previous chunk. The next chunk itself is left unchanged. Figures are never overlapped or duplicated.

## Splitting algorithm

```mermaid
flowchart TD
    A[For each page] --> B[Extract & order blocks]
    B --> C[Next block]
    C --> D{Figure-only?}
    D -->|No| E[Accumulate spans]
    D -->|Yes| F{Accumulated text exists?}
    F -->|Yes| F1[Attach figure to text]
    F1 --> F2[Emit chunk]
    F -->|No| F3[Emit figure chunk]
    E --> G{Next span would overflow?}
    G -->|Yes| H[Split oversized span recursively]
    G -->|No| I[Continue]
    H --> J[Flush chunk]
    I --> K{Flush needed?}
    K -->|Yes| J
    J --> L{More blocks?}
    F2 --> L
    F3 --> L
    L -->|Yes| C
    L -->|No| M[Cross-page boundary repair]
    M --> N[Done]
```

The diagram uses this terminology to describe the units at different stages:

* **Block**: Either one intact `<figure>...</figure>` element (figure‑only) or a contiguous figure‑free text segment (may contain multiple sentences). Blocks never cross page boundaries.
* **Span**: A sentence‑like slice derived from a text block.
* **Chunk**: An emitted output, which will get indexed individually in Azure AI Search. A chunk may consist of one or more spans, a figure, or text plus an attached figure.

## Recursive handling of oversized spans

When a span is too large, recursive splitting applies. After figure extraction, recursion applies only to plain text spans (there is no separate figure-aware recursion path).

Steps:

1. Measure token count for the span.
2. If within the token cap, emit it as-is (subject to normal accumulation logic).
3. Otherwise, search outward from the midpoint (within the central third of the text) first for a sentence-ending punctuation boundary.
4. If none is found, search the same window for a word-break character (space or supported punctuation) to avoid splitting inside a word.
5. If a boundary is found (sentence or word break), split just after that character (it remains in the first half) and recurse on each half.
6. If no acceptable boundary is found within the search window, split at the midpoint with a symmetric 10% overlap. The overlap portion appears duplicated: once at the end of the first half and again at the start of the second.
7. Recurse until all pieces are within the token cap.

> Note: The 10% overlap is computed on raw character length (`len(text)`), not tokens, so the duplicated region is 2 × floor(0.10 * character_count) characters. Token counts can differ across the two halves.
> Clarification: Recursion is triggered only when the *span itself* exceeds the token cap. If adding a span to the current accumulator would overflow but the span alone fits, the accumulator is flushed—recursion is not used in that case.

```mermaid
flowchart TD
    A[Oversized plain text span] --> B[Count tokens]
    B --> C{Within token cap?}
    C -->|Yes| D[Emit span]
    C -->|No| E[Search for nearby sentence boundary]
    E --> F{Boundary found?}
    F -->|Yes| G[Split at boundary keep punctuation]
    F -->|No| H[Midpoint split with 10 percent overlap]
    G --> I[Recurse on each part]
    H --> I
    I --> J[All parts within cap]
    J --> K[Return parts to accumulator]
```

## Cross-page boundary repair

Page boundaries frequently slice a sentence in half, due to the way PDFs and other document formats handle text layout. The repair phase tries to re‑stitch this so downstream retrieval does not see an artificial break.

There are two strategies, attempted in order:

1. Full merge (ideal path)
2. Trailing sentence fragment carry‑forward

### 1. Full merge

We first try to simply glue the last chunk of Page N to the first chunk of Page N+1. This is only allowed when ALL of these hold:

* Previous chunk does not already end in sentence‑terminating punctuation.
* First new chunk starts with a lowercase letter (heuristic for continuation), is not detected as a heading / list, and does not begin with a `<figure>`.
* The concatenated text fits BOTH: token limit (500) AND soft length budget (<= 1.2 × 1000 chars after normalization).

If all pass, the two chunks are merged into one, with an injected whitespace between them if necessary.

### 2. Trailing sentence fragment carry‑forward

If a full merge would violate limits, we do a more surgical repair: pull only the dangling sentence fragment from the end of the previous chunk and move it forward so it reunites with its continuation at the start of the next page.

Key differences from semantic overlap:

* Carry‑forward MOVES text (no duplication except any recursive split overlap that may occur later). Semantic overlap DUPLICATES a small preview from the next chunk.
* Carry‑forward only activates across a page boundary when a full merge is too large. Semantic overlap is routine and size‑capped.

## Chunk normalization

After chunk assembly and any cross-page merging logic, a normalization step applies:

* Figure-containing chunks are left completely untouched (treated as atomic units).
* Leading spaces are trimmed only if they alone cause the chunk to exceed the soft character budget.
* If a chunk is just a few characters (≤ 3) over the soft limit due solely to trailing whitespace, that trailing whitespace is stripped.
* No aggressive reflow or internal whitespace collapsing is performed; the intent is to preserve original formatting while preventing trivial overflows created by boundary adjustments.

## Semantic overlap

To boost recall, each chunk (except the very last in the stream) tries to borrow a small forward-looking sliver from the start of the following chunk. That sliver is appended to the end of the earlier chunk; the later chunk itself stays pristine so sentence boundaries remain clean for highlighting.

How it works:

* Size: About one tenth of the configured max character length is targeted.
* Source: Always taken from the beginning of the next chunk (never from the tail of the previous one).
* Boundary seeking: The algorithm may extend a little past the initial slice to end on sentence punctuation (preferred) or, failing that, a word break; if neither appears it trims back partial trailing words.
* When applied:
  * Always between adjacent non‑figure chunks on the same page.
  * Across a page boundary only if the prior chunk ends mid‑sentence, the next starts lowercase, and the next line doesn’t look like a heading or figure.
* Safety limits: Skipped or shrunk if adding it would breach token or soft character limits (with a modest overflow allowance); trimmed at natural breaks until it fits.
* Figures: Any chunk containing a `<figure>` is excluded (no giving or receiving overlap).
* De‑duplication: If the earlier chunk already ends with the would‑be prefix, nothing is added.

Difference from the recursive split overlap: recursive overlap is a fallback used only when breaking a single oversized span with no safe boundary— it duplicates a midpoint region in both resulting pieces. Semantic overlap, by contrast, is a one‑directional "look‑ahead" duplication added after chunks are otherwise finalized.

## Examples

Each example shows the raw input first, then the emitted output chunks. For brevity, these examples use a smaller token limit than the actual limit of 500 tokens per chunk.

### Example 1: Simple page

⬅️ **Input:**

```text
Sentence one. Sentence two is slightly longer. Final short one.
```

➡️ **Output (1 chunk):**

```text
Chunk 0:
Sentence one. Sentence two is slightly longer. Final short one.
```

💬 **Explanation:**

All sentences fit within limits so a single chunk is emitted.

### Example 2: Atomic block in middle

⬅️ **Input:**

```text
Heading line
Intro before the figure. <figure><img src="x.png" alt="X"></figure> Text that follows the figure. Another sentence.
```

➡️ **Output (2 chunks):**

```text
Chunk 0:
Heading line
Intro before the figure. <figure><img src="x.png" alt="X"></figure>

Chunk 1:
Text that follows the figure. Another sentence.
```

💬 **Explanation:**

Figure remains atomic and is attached to the preceding text; subsequent text flows into the next chunk.

### Example 3: Oversized single span requiring recursive midpoint overlap split

⬅️ **Input (single very long span without nearby punctuation):**

```text
ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789
```

➡️ **Output (2 chunks with duplicated 24-char overlap):**

```text
Chunk 0:
ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789ABCDEFGHIJKL

Chunk 1:
yz0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789
```

💬 **Explanation:**

Original length = 124; recursive-fallback overlap = int(124 x 0.10) = 12. Duplicated region length = 2 x overlap = 24 characters: `yz0123456789ABCDEFGHIJKL` (end of Chunk 0 and start of Chunk 1) providing continuity when no sentence boundary was near midpoint.

### Example 3b: Oversized span with word-break fallback

⬅️ **Input (long span lacking sentence punctuation but containing spaces):**

```text
alpha beta gamma delta epsilon zeta eta theta iota kappa lambda ... (continues)
```

➡️ **Output (first split falls on a space, not an arbitrary midpoint overlap):**

```text
Chunk 0:
alpha beta gamma delta epsilon zeta eta

Chunk 1:
theta iota kappa lambda ...
```

💬 **Explanation:**
No sentence-ending punctuation lies near the midpoint, but a space (word break) does, so the splitter chooses that boundary instead of generating a duplicated 10% overlap.

### Example 4: Cross-page merge

⬅️ **Page A:**

```text
The procedure continues to operate
```

⬅️ **Page B:**

```text
under heavy load and completes successfully. Follow-up sentence.
```

➡️ **Output:**

```text
Chunk 0:
The procedure continues to operate under heavy load and completes successfully.

Chunk 1:
Follow-up sentence.
```

💬 **Explanation:**

Mid-sentence boundary satisfied merge conditions; remainder forms a second chunk.

### Example 5: Trailing sentence fragment carry‑forward when merge too large

⬅️ **Page A:**

```text
Intro sentence finishes here. This clause is long but near the limit and the following portion would push it over
```

⬅️ **Page B:**

```text
so the trailing fragment carry‑forward moves this trailing portion forward. Remaining context continues here.
```

➡️ **Output:**

```text
Chunk 0:
Intro sentence finishes here.

Chunk 1:
This clause is long but near the limit and the following portion would push it over so the trailing fragment carry‑forward moves this trailing portion forward. Remaining context continues here.
```

💬 **Explanation:**

A full merge would exceed size limits, so the unfinished clause from the end of Page A is shifted to the start of the next chunk.
